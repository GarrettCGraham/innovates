{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass data preprocessing\n",
    "\n",
    "Derived from 20210405_mulitclass_SINGLE_STATION.ipynb.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from progressbar import ProgressBar\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of stations ID #s.\n",
    "data_dir = \"../data/stations/\"\n",
    "station_filenames_list = [\n",
    "    filename for filename in os.listdir(path=data_dir)\n",
    "    if filename!=\".DS_Store\"\n",
    "]\n",
    "\n",
    "# Load just the list of ACCLIMA station IDs.\n",
    "file_path = \"../data/acclima_stations_id_list.txt\"\n",
    "acclima_stations_list = pd.read_csv(file_path, header=None).iloc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load expanded array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34343059, 13)\n",
      "(34343059, 13)\n",
      "(34343058, 13)\n"
     ]
    }
   ],
   "source": [
    "# This operation takes a couple minutes, so only do it if you really need to reload the stuff.\n",
    "df_expanded = pd.read_pickle(\"../data/acclima_soil_water_rleeper_1214.pickle\")\n",
    "print(df_expanded.shape)\n",
    "\n",
    "# The previous line loads column names as values in the first row. Set them\n",
    "# as the actual column names and then delete the first row.\n",
    "df_expanded.columns = df_expanded.iloc[0].values\n",
    "print(df_expanded.shape)\n",
    "\n",
    "df_expanded = df_expanded.iloc[1:, :]\n",
    "print(df_expanded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the expanded 2020-12-14 dataset to ACCLIMA only\n",
    "\n",
    "# This takes about 1 minute to run, so only run when necessary.\n",
    "# Filter the expanded dataset down to just the ACCLIMA stations so that\n",
    "# it's easier to wield in memory.\n",
    "df_acclima = df_expanded.isin({\"WBANNO\":acclima_stations_list})\n",
    "df_acclima = df_expanded.iloc[df_acclima.WBANNO.values]\n",
    "\n",
    "# Delete df_expanded to free up some dang memory.\n",
    "del(df_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the ACCLIMA DF's TAG columns to not be \"TAGS, NaN, NaN, NaN\".\n",
    "df_acclima.columns =\\\n",
    "    df_acclima.columns[:9].tolist() + [\"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset down to a single station and create a data-prep regime that works here for reducing **from** multilabel **to** multiclass\n",
    "\n",
    "First, create a few constants and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS_SET = {\n",
    "    'Acclima-Zero', 'Acclima-Toohigh', 'Acclima-Too high', 'Acclima-NoPrcpResponse', \n",
    "    'Acclima-FrozenRecovery', 'Acclima-Noise', 'Acclima-Failure',\n",
    "    'Acclima-Spike', 'Acclima-DiurnalNoise', 'Acclima-Erratic', \n",
    "    'Acclima-Static'\n",
    "}\n",
    "\n",
    "def clean_tags_dataframe(df_targets):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes a target data frame and replaces the tags with their cleaned-up, space-less versions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy of the dataframe so we don't overwrite the original.\n",
    "    df_targets_cleaned = copy.deepcopy(df_targets)\n",
    "    \n",
    "    # Loop through all the cleaned versions of the tags and replace the original versions,\n",
    "    # which have extra whitespace pre-pended to them, with the cleaned versions.\n",
    "    for tag in TAGS_SET:\n",
    "        df_targets_cleaned.replace(\n",
    "            to_replace=\" \"+tag,\n",
    "            value=tag,\n",
    "            inplace=True,\n",
    "        )\n",
    "    \n",
    "    # Replace \"None\" tags with an empty string.\n",
    "    df_targets_cleaned.replace(\n",
    "        to_replace=[None],\n",
    "        value=[\"\"],\n",
    "        inplace=True,\n",
    "    )\n",
    "    \n",
    "    return df_targets_cleaned\n",
    "    \n",
    "    \n",
    "def rename_tags_in_df(df_targets):\n",
    "    \"\"\"\n",
    "    Replaces 'Acclima-Spike' with 'spike' and noise-related Acclima tags with 'noise'.\n",
    "    Returns a dataframe with renamed tags.\n",
    "    \"\"\"\n",
    "    df_targets_renamed = copy.deepcopy(df_targets)\n",
    "    \n",
    "    # Rename SPIKES.\n",
    "    df_targets_renamed.replace(\n",
    "        to_replace=\"Acclima-Spike\",\n",
    "        value=\"spike\",\n",
    "        inplace=True,\n",
    "    )\n",
    "    # Rename NOISE.\n",
    "    noise_tag_list = [\n",
    "        \"Acclima-Noise\",\n",
    "        \"Acclima-Diurnal Noise\", \n",
    "        \"Acclima-FrozenRecovery\", \n",
    "        \"Acclima-Erratic\",\n",
    "    ]\n",
    "    for noise_tag in noise_tag_list:\n",
    "        df_targets_renamed.replace(\n",
    "            to_replace=noise_tag,\n",
    "            value=\"noise\",\n",
    "            inplace=True,\n",
    "        )\n",
    "    return df_targets_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_dataframe(station_id_num, df_acclima):\n",
    "    # Subset down to the single ACCLIMA station of interest.\n",
    "    df_station = df_acclima[df_acclima.WBANNO.eq(station_id_num)]\n",
    "    return df_station\n",
    "\n",
    "\n",
    "def reduce_station_df_and_convert_dates(df_station):\n",
    "    # Subset down to just the columns of interest.\n",
    "    df_station =\\\n",
    "        df_station[\n",
    "            [\"UTC_START\", \"NAME\", \"VALUE\", \"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\"]\n",
    "        ]\n",
    "    # Convert all datetimes to actual datetime datatypes.\n",
    "    df_station.UTC_START = pd.to_datetime(df_station.UTC_START, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    return df_station\n",
    "\n",
    "\n",
    "def get_station_targets(df_station):\n",
    "\n",
    "    # Get just the station's targets.\n",
    "    df_station_targets = df_station[[\"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\"]]\n",
    "\n",
    "    # Clean up and then rename the targets.\n",
    "    df_station_targets =\\\n",
    "            rename_tags_in_df(\n",
    "                clean_tags_dataframe(\n",
    "                    df_station[[\"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\"]]\n",
    "                )\n",
    "            )\n",
    "    return df_station_targets\n",
    "\n",
    "\n",
    "def get_filtered_targets(df_station_targets):\n",
    "    # Isolate the last three columns of targets. \n",
    "    # If there's a row where they're non-empty, then that's indicative of a multilabel example. \n",
    "    # Our goal here is to eliminate all multilabel examples.\n",
    "    array_multilabel_targets = df_station_targets.iloc[:, 1:].values\n",
    "\n",
    "    # Iterate through the rows of multilabel targets and concatenate all tags into a single string.\n",
    "    # For rows that don't have any multilabel tag, the resulting entry will be an empty string of length 0.\n",
    "    # For multilabel rows, there will be a string with non-zero length.\n",
    "    arr_tags_concatenated = np.array([\"\".join(row) for row in array_multilabel_targets])\n",
    "\n",
    "    # Iterate through the concatenated tags and calculate their lengths. \n",
    "    # These lengths will be stored in the new array defined below.\n",
    "    arr_concattags_lengths = np.array([l for l in map(len, arr_tags_concatenated)])\n",
    "\n",
    "    # Find all zero-length elements of the array. \n",
    "    # These entries are the rows in the original targets dataframe that we want to keep,\n",
    "    # since they are the single-label (ie, non-multilabel) rows.\n",
    "    arr_tags_to_keep = (arr_concattags_lengths == 0)\n",
    "\n",
    "    # Reduce the targets dataframe to the first column.\n",
    "    # This column represents all of the single-label targets.\n",
    "    df_tags_reduced = df_station_targets[arr_tags_to_keep].iloc[:,0]\n",
    "\n",
    "    # Get the final set of targets by filtering out anything that's not a spike, noise, or normal.\n",
    "    df_station_targets_reduced_final = df_tags_reduced[df_tags_reduced.isin([\"\", \"spike\", \"noise\"])]\n",
    "    \n",
    "    return df_station_targets_reduced_final, df_tags_reduced, arr_tags_to_keep\n",
    "    \n",
    "    \n",
    "def get_filtered_features(df_station, df_station_targets_reduced_final):\n",
    "    # Get the final set of station features by using the indices of the remaining targets.\n",
    "    df_station_features = df_station.loc[df_station_targets_reduced_final.index, [\"UTC_START\", \"NAME\", \"VALUE\"]]\n",
    "    return df_station_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare targets and then features for single station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################ 15 ############################\n",
      "Original set of targets:                                          ['' 'spike']\n",
      "Original number of targets:                                       367029\n",
      "\n",
      "Remaining unique labels:                                          ['' 'spike']\n",
      "Original number of targets:                                       367029\n",
      "Number of reduced targets:                                        367029\n",
      "Number of reduced targets plus number of dropped targets:         367029\n",
      "\n",
      "Final set of unique labels:                                       ['' 'spike']\n",
      "Number of final labels:                                           367029\n",
      "Number of final features:                                         367029\n",
      "\n",
      "\n",
      "############################ 16 ############################\n",
      "Original set of targets:                                          ['' 'Acclima-NoPrcpResponse' 'Acclima-Zero' 'spike']\n",
      "Original number of targets:                                       368442\n",
      "\n",
      "Remaining unique labels:                                          ['' 'Acclima-Zero' 'spike']\n",
      "Original number of targets:                                       368442\n",
      "Number of reduced targets:                                        368237\n",
      "Number of reduced targets plus number of dropped targets:         368442\n",
      "\n",
      "Final set of unique labels:                                       ['' 'spike']\n",
      "Number of final labels:                                           368215\n",
      "Number of final features:                                         368215\n",
      "\n",
      "\n",
      "############################ 17 ############################\n",
      "Original set of targets:                                          ['']\n",
      "Original number of targets:                                       181088\n",
      "\n",
      "Remaining unique labels:                                          ['']\n",
      "Original number of targets:                                       181088\n",
      "Number of reduced targets:                                        181088\n",
      "Number of reduced targets plus number of dropped targets:         181088\n",
      "\n",
      "Final set of unique labels:                                       ['']\n",
      "Number of final labels:                                           181088\n",
      "Number of final features:                                         181088\n",
      "\n",
      "\n",
      "############################ 18 ############################\n",
      "Original set of targets:                                          ['']\n",
      "Original number of targets:                                       199617\n",
      "\n",
      "Remaining unique labels:                                          ['']\n",
      "Original number of targets:                                       199617\n",
      "Number of reduced targets:                                        199617\n",
      "Number of reduced targets plus number of dropped targets:         199617\n",
      "\n",
      "Final set of unique labels:                                       ['']\n",
      "Number of final labels:                                           199617\n",
      "Number of final features:                                         199617\n",
      "\n",
      "\n",
      "############################ 19 ############################\n",
      "Original set of targets:                                          ['' 'Acclima-Failure' 'Acclima-Zero']\n",
      "Original number of targets:                                       241670\n",
      "\n",
      "Remaining unique labels:                                          ['']\n",
      "Original number of targets:                                       241670\n",
      "Number of reduced targets:                                        225176\n",
      "Number of reduced targets plus number of dropped targets:         241670\n",
      "\n",
      "Final set of unique labels:                                       ['']\n",
      "Number of final labels:                                           225176\n",
      "Number of final features:                                         225176\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select station to test.\n",
    "# station_list_idx = 14\n",
    "\n",
    "for station_list_idx in range(15, 20):\n",
    "    \n",
    "    station_id_num = acclima_stations_list[station_list_idx]\n",
    "    print(\"############################\", station_list_idx, \"############################\")\n",
    "\n",
    "    # Get just the station of interest.\n",
    "    df_station = get_station_dataframe(station_id_num, df_acclima)\n",
    "\n",
    "    # Cut the station data down to just the columns-of-interest.\n",
    "    # Convert the date-times in the UTC_START column to datetime objects.\n",
    "    df_station = reduce_station_df_and_convert_dates(df_station)\n",
    "\n",
    "    # Isolate the station targets for filtering.\n",
    "    df_station_targets = get_station_targets(df_station)\n",
    "\n",
    "    # Print sanity-check statistics.\n",
    "    print(\"Original set of targets:                                         \", np.unique(df_station_targets.values))\n",
    "    print(\"Original number of targets:                                      \", df_station_targets.shape[0])\n",
    "    \n",
    "    # Filter the targets down to single-label targets-of-interest (ie, just normal, \"spike\" and \"noise\").\n",
    "    df_station_targets_reduced_final, df_tags_reduced, arr_tags_to_keep = get_filtered_targets(df_station_targets)\n",
    "\n",
    "    # Print some sanity statistics.\n",
    "    print()\n",
    "    print(\"Remaining unique labels:                                         \", df_tags_reduced.unique())\n",
    "    print(\"Original number of targets:                                      \", df_station_targets.shape[0])\n",
    "    print(\"Number of reduced targets:                                       \", df_tags_reduced.shape[0])\n",
    "    print(\"Number of reduced targets plus number of dropped targets:        \", (~arr_tags_to_keep).sum() + df_tags_reduced.shape[0])\n",
    "\n",
    "    # Get the final feature-set by filtering to feature-rows that have labels remaining after the labels were filtered.\n",
    "    df_station_features = get_filtered_features(df_station, df_station_targets_reduced_final)\n",
    "\n",
    "    print()\n",
    "    print(\"Final set of unique labels:                                      \", df_station_targets_reduced_final.unique())\n",
    "    print(\"Number of final labels:                                          \", df_station_features.shape[0])\n",
    "    print(\"Number of final features:                                        \",df_station_targets_reduced_final.shape[0])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functionalizing the methods produces the same results as when they were non-functionalized. Whoooo!\n",
    "\n",
    "Moving onto creating pivot tables of features now, with appropriate filtering of missing values.\n",
    "\n",
    "# Pivot and further subset the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select station to test.\n",
    "station_list_idx = 24\n",
    "station_id_num = acclima_stations_list[station_list_idx]\n",
    "\n",
    "# Get just the station of interest.\n",
    "df_station = get_station_dataframe(station_id_num, df_acclima)\n",
    "\n",
    "# Cut the station data down to just the columns-of-interest.\n",
    "# Convert the date-times in the UTC_START column to datetime objects.\n",
    "df_station = reduce_station_df_and_convert_dates(df_station)\n",
    "\n",
    "# Isolate the station targets for filtering.\n",
    "df_station_targets = get_station_targets(df_station)\n",
    "\n",
    "# Filter the targets down to single-label targets-of-interest (ie, just normal, \"spike\" and \"noise\").\n",
    "df_station_targets_reduced_final, df_tags_reduced, arr_tags_to_keep = get_filtered_targets(df_station_targets)\n",
    "\n",
    "# Get the final feature-set by filtering to feature-rows that have labels remaining after the labels were filtered.\n",
    "df_station_features = get_filtered_features(df_station, df_station_targets_reduced_final)\n",
    "\n",
    "############################################################\n",
    "# EVERYTHING IN THE CELL ABOVE THIS POINT ARE THE TARG/FEATURE \n",
    "# PREPROCESSING STEPS THAT I DEVELOPED EARLIER IN THE NB.\n",
    "############################################################\n",
    "\n",
    "df_station_combined = pd.concat([df_station_features, df_station_targets_reduced_final],axis=1)\n",
    "\n",
    "# Identified a new potential problem:\n",
    "# Even though I've eliminated multilable instances for single sensors at \n",
    "# single timepoints, there may be single timepoints where **multiple sensors**\n",
    "# have differing tags. I'll need to drop those intances from this analysis in \n",
    "# order to get to the pure single-label, multiclass case.\n",
    "df_pivoted_targets = df_station_combined.pivot(index=\"UTC_START\", columns=\"NAME\", values=\"TAGS_00\")\n",
    "df_pivoted_targets = df_pivoted_targets.drop([\"p_official\", \"t_official\"], axis=1)\n",
    "\n",
    "# Clean the tags up; ie, convert all NaN to ''.\n",
    "df_pivoted_targets_clean = clean_tags_dataframe(df_pivoted_targets)\n",
    "\n",
    "# Concatenate all the values in each row and check for non-unique labels.\n",
    "# Ie, check for time points where there's normal/spike, etc.\n",
    "arr_tags_concatenated = np.array([\"\".join(row) for row in df_pivoted_targets_clean.values])\n",
    "\n",
    "# Use set operations to the ID unique tags in each row. Then, \n",
    "# set(np.char.array([ [\"\",\"\",\"\"], [\"\",\" Acclima-Too high\", \"\"], [\"a\",\"b\",\"c\"]])[2])\n",
    "unique_tags_by_row = [list(set(row)) for row in df_pivoted_targets_clean.values]\n",
    "\n",
    "# WELLLLL, ACTUALLY, I'm not concerned with multilabel rows that have both normal and then one tag. \n",
    "# I'm concerned with multilabel rows that have \"spike\" and \"noise\".\n",
    "# So, I'll concatenate the unique labels together. Anything over length 5 ( len(\"spike\")=5 and len(\"noise\")=5 ) will be a multilabel instance, since len(\"spikenoise\")=10.  \n",
    "\n",
    "# Find all the multilabel row locations via some string method trickery.\n",
    "multilabel_row_locations = np.array(\n",
    "    [\n",
    "        len(\n",
    "            \"\".join(row)    # Join all the unique tags in each row together; ie, \n",
    "                            # [\"\", \"noise\"] --> \"noise\", while\n",
    "                            # [\"noise\", \"spike\"] --> \"noisespike\"\n",
    "        ) > 5               # Check for anything that has length > 5. This will only occur where\n",
    "                            # \"\".join(row) --> \"spikenoise\" or \"noisespike\".\n",
    "        for row in unique_tags_by_row\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Now I need to drop the multilabel timepoints from the analysis.\n",
    "\n",
    "# Filter the pivoted targets DF of any multilabel row locations (ie, locations that are co-labeled \"spike\" and \"noise\").\n",
    "df_pivoted_targets_singlelabel =\\\n",
    "    df_pivoted_targets[~multilabel_row_locations]\n",
    "\n",
    "# Use the filtered targets dataframe's index to filter the pivoted features dataframe.\n",
    "# That way, we have only feature locations with single-label multiclass features.\n",
    "df_pivoted_features_singlelabel = df_station_features.pivot(\n",
    "    index=\"UTC_START\", columns=\"NAME\", values=\"VALUE\"            # Create a pivoted DF of the features.\n",
    ").loc[\n",
    "    df_pivoted_targets_singlelabel.index                         # Filter the pivoted features DF using the datetimes of the remaining targets.\n",
    "]\n",
    "\n",
    "# It appears that filtering by the single-label DF's index reduces the pivoted feature DF's date range. \n",
    "# So, I need to reverse-filter the single-label DF using the remaining indices from the feature DF.\n",
    "# Ugh. There's so many flipping tricky parts of this problem.\n",
    "\n",
    "# Filter the pivoted single-label targets DF using the remaining feature DF datetime indices.\n",
    "df_pivoted_targets_singlelabel =\\\n",
    "    df_pivoted_targets_singlelabel.loc[\n",
    "        df_pivoted_features_singlelabel.index\n",
    "    ]\n",
    "\n",
    "# Now both the pivoted targets and the pivoted features have the same datetime indices.\n",
    "\n",
    "# Now I need to go back and reformat the remaining targets so that they're a \n",
    "# single-column series, rather than a multi-dimensional dataframe.\n",
    "\n",
    "# Get just the remaining single-label target values.\n",
    "# I'll use these to get down to one label entry per datetime row.\n",
    "df_pivoted_targets_singlelabel = clean_tags_dataframe(df_pivoted_targets_singlelabel)\n",
    "array_pivoted_targets_singlelabel = df_pivoted_targets_singlelabel.values\n",
    "\n",
    "# Use the list(set()) trick to filter down to the unique entries in each row of the targets array.\n",
    "unique_tags_by_row = [list(set(row)) for row in array_pivoted_targets_singlelabel]\n",
    "\n",
    "# Join these unique entries together to form a single entry per target row.\n",
    "# Since each row only has either {\"\"}, {\"\", \"spike\"} or {\"\", \"noise\"}, the result will be a single label per row.\n",
    "multiclass_targets_array = np.char.array(\n",
    "    [\n",
    "        \"\".join(row)                     # Join together the unique single-labels;\n",
    "                                         # ie, [\"\",\"spike\"] --> \"spike\" and [\"\", \"noise\"] --> \"noise\".\n",
    "        for row in unique_tags_by_row\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Recombine the newly-filtered multiclass targets with their original datetime index.\n",
    "df_targets_singlelabel = pd.DataFrame(index=df_pivoted_targets_singlelabel.index, columns=[\"TAGS\"], data=multiclass_targets_array,)\n",
    "\n",
    "# Drop any and all feature rows that have NaN values.\n",
    "df_pivoted_features_singlelabel = df_pivoted_features_singlelabel.dropna(how=\"any\", axis=0)\n",
    "\n",
    "# Filter the targets based on the remaining datetime\n",
    "# indices from the NaN-filtered pivoted features.\n",
    "df_targets_singlelabel =\\\n",
    "    df_targets_singlelabel.loc[\n",
    "        df_pivoted_features_singlelabel.index\n",
    "    ]\n",
    "\n",
    "df_targets_singlelabel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That worked. Now time to functionalize all of the multilabel row-detection methods and then combine them with some other stuff to create a final set of functions to preprocess all the stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_station_combined = pd.concat([df_station_features, df_station_targets_reduced_final],axis=1)\n",
    "\n",
    "# # Identified a new potential problem:\n",
    "# # Even though I've eliminated multilable instances for single sensors at \n",
    "# # single timepoints, there may be single timepoints where **multiple sensors**\n",
    "# # have differing tags. I'll need to drop those intances from this analysis in \n",
    "# # order to get to the pure single-label, multiclass case.\n",
    "# df_pivoted_targets = df_station_combined.pivot(index=\"UTC_START\", columns=\"NAME\", values=\"TAGS_00\")\n",
    "# df_pivoted_targets = df_pivoted_targets.drop([\"p_official\", \"t_official\"], axis=1)\n",
    "\n",
    "# # Clean the tags up; ie, convert all NaN to ''.\n",
    "# df_pivoted_targets_clean = clean_tags_dataframe(df_pivoted_targets)\n",
    "\n",
    "def locate_all_multilabel_rows(df_pivoted_targets_clean):\n",
    "    # Concatenate all the values in each row and check for non-unique labels.\n",
    "    # Ie, check for time points where there's normal/spike, etc.\n",
    "    arr_tags_concatenated = np.array([\"\".join(row) for row in df_pivoted_targets_clean.values])\n",
    "\n",
    "    # Use set operations to the ID unique tags in each row. Then, \n",
    "    # set(np.char.array([ [\"\",\"\",\"\"], [\"\",\" Acclima-Too high\", \"\"], [\"a\",\"b\",\"c\"]])[2])\n",
    "    unique_tags_by_row = [list(set(row)) for row in df_pivoted_targets_clean.values]\n",
    "\n",
    "    # WELLLLL, ACTUALLY, I'm not concerned with multilabel rows that have both normal and then one tag. \n",
    "    # I'm concerned with multilabel rows that have \"spike\" and \"noise\".\n",
    "    # So, I'll concatenate the unique labels together. Anything over length 5 ( len(\"spike\")=5 and len(\"noise\")=5 ) will be a multilabel instance, since len(\"spikenoise\")=10.  \n",
    "\n",
    "    # Find all the multilabel row locations via some string method trickery.\n",
    "    multilabel_row_locations = np.array(\n",
    "        [\n",
    "            len(\n",
    "                \"\".join(row)    # Join all the unique tags in each row together; ie, \n",
    "                                # [\"\", \"noise\"] --> \"noise\", while\n",
    "                                # [\"noise\", \"spike\"] --> \"noisespike\"\n",
    "            ) > 5               # Check for anything that has length > 5. This will only occur where\n",
    "                                # \"\".join(row) --> \"spikenoise\" or \"noisespike\".\n",
    "            for row in unique_tags_by_row\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return multilabel_row_locations\n",
    "\n",
    "# # Now I need to drop the multilabel timepoints from the analysis.\n",
    "# # Filter the pivoted targets DF of any multilabel row locations (ie, locations that are co-labeled \"spike\" and \"noise\").\n",
    "# df_pivoted_targets_singlelabel =\\\n",
    "#     df_pivoted_targets[~multilabel_row_locations]\n",
    "\n",
    "# # Use the filtered targets dataframe's index to filter the pivoted features dataframe.\n",
    "# # That way, we have only feature locations with single-label multiclass features.\n",
    "# df_pivoted_features_singlelabel = df_station_features.pivot(\n",
    "#     index=\"UTC_START\", columns=\"NAME\", values=\"VALUE\"            # Create a pivoted DF of the features.\n",
    "# ).loc[\n",
    "#     df_pivoted_targets_singlelabel.index                         # Filter the pivoted features DF using the datetimes of the remaining targets.\n",
    "# ]\n",
    "\n",
    "# # Filter the pivoted single-label targets DF using the remaining feature DF datetime indices.\n",
    "# df_pivoted_targets_singlelabel =\\\n",
    "#     df_pivoted_targets_singlelabel.loc[\n",
    "#         df_pivoted_features_singlelabel.index\n",
    "#     ]\n",
    "\n",
    "# # Now both the pivoted targets and the pivoted features have the same datetime indices.\n",
    "# # Now I need to go back and reformat the remaining targets so that they're a \n",
    "# # single-column series, rather than a multi-dimensional dataframe.\n",
    "# # Get just the remaining single-label target values.\n",
    "# # I'll use these to get down to one label entry per datetime row.\n",
    "# df_pivoted_targets_singlelabel = clean_tags_dataframe(df_pivoted_targets_singlelabel)\n",
    "\n",
    "def convert_targs_to_single_column(df_pivoted_targets_singlelabel)\n",
    "    array_pivoted_targets_singlelabel = df_pivoted_targets_singlelabel.values\n",
    "\n",
    "    # Use the list(set()) trick to filter down to the unique entries in each row of the targets array.\n",
    "    unique_tags_by_row = [list(set(row)) for row in array_pivoted_targets_singlelabel]\n",
    "\n",
    "    # Join these unique entries together to form a single entry per target row.\n",
    "    # Since each row only has either {\"\"}, {\"\", \"spike\"} or {\"\", \"noise\"}, the result will be a single label per row.\n",
    "    multiclass_targets_array = np.char.array(\n",
    "        [\n",
    "            \"\".join(row)                     # Join together the unique single-labels;\n",
    "                                             # ie, [\"\",\"spike\"] --> \"spike\" and [\"\", \"noise\"] --> \"noise\".\n",
    "            for row in unique_tags_by_row\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Recombine the newly-filtered multiclass targets with their original datetime index.\n",
    "    df_targets_singlelabel = pd.DataFrame(\n",
    "        data=multiclass_targets_array,\n",
    "        columns=[\"TAGS\"], \n",
    "        index=df_pivoted_targets_singlelabel.index, \n",
    "    )\n",
    "\n",
    "    return df_targets_singlelabel\n",
    "\n",
    "# # Drop any and all feature rows that have NaN values.\n",
    "# df_pivoted_features_singlelabel = df_pivoted_features_singlelabel.dropna(how=\"any\", axis=0)\n",
    "\n",
    "# # Filter the targets based on the remaining datetime\n",
    "# # indices from the NaN-filtered pivoted features.\n",
    "# df_targets_singlelabel =\\\n",
    "#     df_targets_singlelabel.loc[\n",
    "#         df_pivoted_features_singlelabel.index\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_all_multilabel_rows(df_pivoted_targets_clean):\n",
    "    # Concatenate all the values in each row and check for non-unique labels.\n",
    "    # Ie, check for time points where there's normal/spike, etc.\n",
    "    arr_tags_concatenated = np.array([\"\".join(row) for row in df_pivoted_targets_clean.values])\n",
    "\n",
    "    # Use set operations to the ID unique tags in each row. Then, \n",
    "    # set(np.char.array([ [\"\",\"\",\"\"], [\"\",\" Acclima-Too high\", \"\"], [\"a\",\"b\",\"c\"]])[2])\n",
    "    unique_tags_by_row = [list(set(row)) for row in df_pivoted_targets_clean.values]\n",
    "\n",
    "    # WELLLLL, ACTUALLY, I'm not concerned with multilabel rows that have both normal and then one tag. \n",
    "    # I'm concerned with multilabel rows that have \"spike\" and \"noise\".\n",
    "    # So, I'll concatenate the unique labels together. Anything over length 5 ( len(\"spike\")=5 and len(\"noise\")=5 ) will be a multilabel instance, since len(\"spikenoise\")=10.  \n",
    "\n",
    "    # Find all the multilabel row locations via some string method trickery.\n",
    "    multilabel_row_locations = np.array(\n",
    "        [\n",
    "            len(\n",
    "                \"\".join(row)    # Join all the unique tags in each row together; ie, \n",
    "                                # [\"\", \"noise\"] --> \"noise\", while\n",
    "                                # [\"noise\", \"spike\"] --> \"noisespike\"\n",
    "            ) > 5               # Check for anything that has length > 5. This will only occur where\n",
    "                                # \"\".join(row) --> \"spikenoise\" or \"noisespike\".\n",
    "            for row in unique_tags_by_row\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return multilabel_row_locations\n",
    "\n",
    "\n",
    "def convert_targs_to_single_column(df_pivoted_targets_singlelabel):\n",
    "    array_pivoted_targets_singlelabel = df_pivoted_targets_singlelabel.values\n",
    "\n",
    "    # Use the list(set()) trick to filter down to the unique entries in each row of the targets array.\n",
    "    unique_tags_by_row = [list(set(row)) for row in array_pivoted_targets_singlelabel]\n",
    "\n",
    "    # Join these unique entries together to form a single entry per target row.\n",
    "    # Since each row only has either {\"\"}, {\"\", \"spike\"} or {\"\", \"noise\"}, the result will be a single label per row.\n",
    "    multiclass_targets_array = np.char.array(\n",
    "        [\n",
    "            \"\".join(row)                     # Join together the unique single-labels;\n",
    "                                             # ie, [\"\",\"spike\"] --> \"spike\" and [\"\", \"noise\"] --> \"noise\".\n",
    "            for row in unique_tags_by_row\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Recombine the newly-filtered multiclass targets with their original datetime index.\n",
    "    df_targets_singlelabel = pd.DataFrame(\n",
    "        data=multiclass_targets_array,\n",
    "        columns=[\"TAGS\"], \n",
    "        index=df_pivoted_targets_singlelabel.index, \n",
    "    )\n",
    "\n",
    "    return df_targets_singlelabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################ 18 ############################\n",
      "Original set of targets:                                          ['']\n",
      "Original number of targets:                                       199617\n",
      "\n",
      "Remaining unique labels:                                          ['']\n",
      "Original number of targets:                                       199617\n",
      "Number of reduced targets:                                        199617\n",
      "Number of reduced targets plus number of dropped targets:         199617\n",
      "\n",
      "Final set of unique labels:                                       ['']\n",
      "Number of final labels:                                           199617\n",
      "Number of final features:                                         199617\n",
      "\n",
      "Final set of unique labels in targets' pd.series object:          []\n",
      "Number of final labels after pivoting and matching:               0\n",
      "Number of final pivoted features:                                 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select station to test.\n",
    "# station_list_idx = 14\n",
    "\n",
    "# for station_list_idx in range(15, 20):\n",
    "# for station_list_idx in [18]:\n",
    "    \n",
    "    station_id_num = acclima_stations_list[station_list_idx]\n",
    "    print(\"############################\", station_list_idx, \"############################\")\n",
    "\n",
    "    # Get just the station of interest.\n",
    "    df_station = get_station_dataframe(station_id_num, df_acclima)\n",
    "\n",
    "    # Cut the station data down to just the columns-of-interest.\n",
    "    # Convert the date-times in the UTC_START column to datetime objects.\n",
    "    df_station = reduce_station_df_and_convert_dates(df_station)\n",
    "\n",
    "    # Isolate the station targets for filtering.\n",
    "    df_station_targets = get_station_targets(df_station)\n",
    "\n",
    "    # Print sanity-check statistics.\n",
    "    print(\"Original set of targets:                                         \", np.unique(df_station_targets.values))\n",
    "    print(\"Original number of targets:                                      \", df_station_targets.shape[0])\n",
    "    \n",
    "    # Filter the targets down to single-label targets-of-interest (ie, just normal, \"spike\" and \"noise\").\n",
    "    df_station_targets_reduced_final, df_tags_reduced, arr_tags_to_keep = get_filtered_targets(df_station_targets)\n",
    "\n",
    "    # Print some sanity statistics.\n",
    "    print()\n",
    "    print(\"Remaining unique labels:                                         \", df_tags_reduced.unique())\n",
    "    print(\"Original number of targets:                                      \", df_station_targets.shape[0])\n",
    "    print(\"Number of reduced targets:                                       \", df_tags_reduced.shape[0])\n",
    "    print(\"Number of reduced targets plus number of dropped targets:        \", (~arr_tags_to_keep).sum() + df_tags_reduced.shape[0])\n",
    "\n",
    "    # Get the final feature-set by filtering to feature-rows that have labels remaining after the labels were filtered.\n",
    "    df_station_features = get_filtered_features(df_station, df_station_targets_reduced_final)\n",
    "\n",
    "    print()\n",
    "    print(\"Final set of unique labels:                                      \", df_station_targets_reduced_final.unique())\n",
    "    print(\"Number of final labels:                                          \", df_station_features.shape[0])\n",
    "    print(\"Number of final features:                                        \",df_station_targets_reduced_final.shape[0])\n",
    "    \n",
    "    # Combine features and targets for station into single DF, then pivot the targets, isolate to just the sensors\n",
    "    # by dropping the precipitation and temperature fields, and then convert any resulting NaN values in the targets\n",
    "    # pivot DF to the normal label ''.\n",
    "    df_station_combined = pd.concat([df_station_features, df_station_targets_reduced_final],axis=1)\n",
    "    df_pivoted_targets = df_station_combined.pivot(index=\"UTC_START\", columns=\"NAME\", values=\"TAGS_00\")\n",
    "    df_pivoted_targets = df_pivoted_targets.drop([\"p_official\", \"t_official\"], axis=1)\n",
    "    df_pivoted_targets_clean =\\\n",
    "        clean_tags_dataframe(df_pivoted_targets) # Clean the tags up; ie, convert all NaN to ''.\n",
    "\n",
    "    # Drop the multilabel timepoints from the analysis. First, filter the pivoted \n",
    "    # targets DF of any multilabel row locations (ie, locations that are co-labeled \"spike\" and \"noise\").\n",
    "    multilabel_row_locations = locate_all_multilabel_rows(df_pivoted_targets_clean)\n",
    "    df_pivoted_targets_singlelabel =\\\n",
    "        df_pivoted_targets[~multilabel_row_locations]\n",
    "\n",
    "    # Use the filtered targets dataframe's index to filter the pivoted features dataframe.\n",
    "    # That way, we have only feature locations with single-label multiclass features.\n",
    "    df_pivoted_features_singlelabel = df_station_features.pivot(\n",
    "        index=\"UTC_START\", columns=\"NAME\", values=\"VALUE\"            # Create a pivoted DF of the features.\n",
    "    ).loc[\n",
    "        df_pivoted_targets_singlelabel.index                         # Filter the pivoted features DF using the datetimes of the remaining targets.\n",
    "    ]\n",
    "\n",
    "    # Filter the pivoted single-label targets DF using the remaining feature DF datetime indices.\n",
    "    df_pivoted_targets_singlelabel =\\\n",
    "        df_pivoted_targets_singlelabel.loc[\n",
    "            df_pivoted_features_singlelabel.index\n",
    "        ]\n",
    "\n",
    "    # Now both the pivoted targets and the pivoted features have the same datetime indices.\n",
    "    # Now I need to go back and reformat the remaining targets so that they're a \n",
    "    # single-column series, rather than a multi-dimensional dataframe.\n",
    "    # Get just the remaining single-label target values.\n",
    "    # I'll use these to get down to one label entry per datetime row.\n",
    "    df_pivoted_targets_singlelabel =\\\n",
    "        clean_tags_dataframe(df_pivoted_targets_singlelabel)\n",
    "    df_targets_singlelabel =\\\n",
    "        convert_targs_to_single_column(df_pivoted_targets_singlelabel)\n",
    "\n",
    "    # Drop any and all feature rows that have NaN values.\n",
    "    df_pivoted_features_singlelabel = df_pivoted_features_singlelabel.dropna(how=\"any\", axis=0)\n",
    "\n",
    "    # Filter the targets based on the remaining datetime\n",
    "    # indices from the NaN-filtered pivoted features.\n",
    "    df_targets_singlelabel =\\\n",
    "        df_targets_singlelabel.loc[\n",
    "            df_pivoted_features_singlelabel.index\n",
    "        ]\n",
    "    \n",
    "    print()\n",
    "    print(\"Final set of unique labels in targets' pd.series object:         \", df_targets_singlelabel[\"TAGS\"].unique())\n",
    "    print(\"Number of final labels after pivoting and matching:              \", df_targets_singlelabel.shape[0])\n",
    "    print(\"Number of final pivoted features:                                \", df_pivoted_features_singlelabel.shape[0])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOM. DONE! WHOOOHOOOOOOO! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################ 18 ############################\n",
      "Original set of targets:                                          ['']\n",
      "Original number of targets:                                       199617\n",
      "\n",
      "Remaining unique labels:                                          ['']\n",
      "Original number of targets:                                       199617\n",
      "Number of reduced targets:                                        199617\n",
      "Number of reduced targets plus number of dropped targets:         199617\n",
      "\n",
      "Final set of unique labels:                                       ['']\n",
      "Number of final labels:                                           199617\n",
      "Number of final features:                                         199617\n"
     ]
    }
   ],
   "source": [
    "# Select station to test.\n",
    "# station_list_idx = 18\n",
    "\n",
    "station_id_num = acclima_stations_list[station_list_idx]\n",
    "print(\"############################\", station_list_idx, \"############################\")\n",
    "\n",
    "# Get just the station of interest.\n",
    "df_station = get_station_dataframe(station_id_num, df_acclima)\n",
    "\n",
    "# Cut the station data down to just the columns-of-interest.\n",
    "# Convert the date-times in the UTC_START column to datetime objects.\n",
    "df_station = reduce_station_df_and_convert_dates(df_station)\n",
    "\n",
    "# Isolate the station targets for filtering.\n",
    "df_station_targets = get_station_targets(df_station)\n",
    "\n",
    "# Print sanity-check statistics.\n",
    "print(\"Original set of targets:                                         \", np.unique(df_station_targets.values))\n",
    "print(\"Original number of targets:                                      \", df_station_targets.shape[0])\n",
    "\n",
    "# Filter the targets down to single-label targets-of-interest (ie, just normal, \"spike\" and \"noise\").\n",
    "df_station_targets_reduced_final, df_tags_reduced, arr_tags_to_keep = get_filtered_targets(df_station_targets)\n",
    "\n",
    "# Print some sanity statistics.\n",
    "print()\n",
    "print(\"Remaining unique labels:                                         \", df_tags_reduced.unique())\n",
    "print(\"Original number of targets:                                      \", df_station_targets.shape[0])\n",
    "print(\"Number of reduced targets:                                       \", df_tags_reduced.shape[0])\n",
    "print(\"Number of reduced targets plus number of dropped targets:        \", (~arr_tags_to_keep).sum() + df_tags_reduced.shape[0])\n",
    "\n",
    "# Get the final feature-set by filtering to feature-rows that have labels remaining after the labels were filtered.\n",
    "df_station_features = get_filtered_features(df_station, df_station_targets_reduced_final)\n",
    "\n",
    "print()\n",
    "print(\"Final set of unique labels:                                      \", df_station_targets_reduced_final.unique())\n",
    "print(\"Number of final labels:                                          \", df_station_features.shape[0])\n",
    "print(\"Number of final features:                                        \",df_station_targets_reduced_final.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>NAME</th>\n",
       "      <th>sw1005</th>\n",
       "      <th>sw1010</th>\n",
       "      <th>sw1020</th>\n",
       "      <th>sw1050</th>\n",
       "      <th>sw1100</th>\n",
       "      <th>sw2005</th>\n",
       "      <th>sw2010</th>\n",
       "      <th>sw2020</th>\n",
       "      <th>sw2050</th>\n",
       "      <th>sw2100</th>\n",
       "      <th>sw3005</th>\n",
       "      <th>sw3010</th>\n",
       "      <th>sw3020</th>\n",
       "      <th>sw3050</th>\n",
       "      <th>sw3100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTC_START</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-01 00:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 01:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 02:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 03:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 04:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 19:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 20:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 21:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 22:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 23:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28460 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "NAME                sw1005 sw1010 sw1020 sw1050 sw1100 sw2005 sw2010 sw2020  \\\n",
       "UTC_START                                                                     \n",
       "2017-05-01 00:00:00                                                           \n",
       "2017-05-01 01:00:00                                                           \n",
       "2017-05-01 02:00:00                                                           \n",
       "2017-05-01 03:00:00                                                           \n",
       "2017-05-01 04:00:00                                                           \n",
       "...                    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2020-07-31 19:00:00                                                           \n",
       "2020-07-31 20:00:00                                                           \n",
       "2020-07-31 21:00:00                                                           \n",
       "2020-07-31 22:00:00                                                           \n",
       "2020-07-31 23:00:00                                                           \n",
       "\n",
       "NAME                sw2050 sw2100 sw3005 sw3010 sw3020 sw3050 sw3100  \n",
       "UTC_START                                                             \n",
       "2017-05-01 00:00:00                                                   \n",
       "2017-05-01 01:00:00                                                   \n",
       "2017-05-01 02:00:00                                                   \n",
       "2017-05-01 03:00:00                                                   \n",
       "2017-05-01 04:00:00                                                   \n",
       "...                    ...    ...    ...    ...    ...    ...    ...  \n",
       "2020-07-31 19:00:00                                                   \n",
       "2020-07-31 20:00:00                                                   \n",
       "2020-07-31 21:00:00                                                   \n",
       "2020-07-31 22:00:00                                                   \n",
       "2020-07-31 23:00:00                                                   \n",
       "\n",
       "[28460 rows x 15 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine features and targets for station into single DF, then pivot the targets, isolate to just the sensors\n",
    "# by dropping the precipitation and temperature fields, and then convert any resulting NaN values in the targets\n",
    "# pivot DF to the normal label ''.\n",
    "df_station_combined = pd.concat([df_station_features, df_station_targets_reduced_final],axis=1)\n",
    "df_pivoted_targets = df_station_combined.pivot(index=\"UTC_START\", columns=\"NAME\", values=\"TAGS_00\")\n",
    "df_pivoted_targets = df_pivoted_targets.drop([\"p_official\", \"t_official\"], axis=1)\n",
    "df_pivoted_targets_clean =\\\n",
    "    clean_tags_dataframe(df_pivoted_targets) # Clean the tags up; ie, convert all NaN to ''."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the multilabel timepoints from the analysis. First, filter the pivoted \n",
    "# targets DF of any multilabel row locations (ie, locations that are co-labeled \"spike\" and \"noise\").\n",
    "multilabel_row_locations = locate_all_multilabel_rows(df_pivoted_targets_clean)\n",
    "df_pivoted_targets_singlelabel =\\\n",
    "    df_pivoted_targets[~multilabel_row_locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>NAME</th>\n",
       "      <th>sw1005</th>\n",
       "      <th>sw1010</th>\n",
       "      <th>sw1020</th>\n",
       "      <th>sw1050</th>\n",
       "      <th>sw1100</th>\n",
       "      <th>sw2005</th>\n",
       "      <th>sw2010</th>\n",
       "      <th>sw2020</th>\n",
       "      <th>sw2050</th>\n",
       "      <th>sw2100</th>\n",
       "      <th>sw3005</th>\n",
       "      <th>sw3010</th>\n",
       "      <th>sw3020</th>\n",
       "      <th>sw3050</th>\n",
       "      <th>sw3100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTC_START</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-11 20:00:00</th>\n",
       "      <td>0.035</td>\n",
       "      <td>0.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.99</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-11 21:00:00</th>\n",
       "      <td>0.035</td>\n",
       "      <td>0.108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-11 22:00:00</th>\n",
       "      <td>0.032</td>\n",
       "      <td>0.109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-11 23:00:00</th>\n",
       "      <td>0.031</td>\n",
       "      <td>0.107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-12 00:00:00</th>\n",
       "      <td>0.032</td>\n",
       "      <td>0.107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 19:00:00</th>\n",
       "      <td>0.075</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 20:00:00</th>\n",
       "      <td>0.077</td>\n",
       "      <td>0.145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 21:00:00</th>\n",
       "      <td>0.076</td>\n",
       "      <td>0.147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 22:00:00</th>\n",
       "      <td>0.075</td>\n",
       "      <td>0.149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 23:00:00</th>\n",
       "      <td>0.074</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23787 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "NAME                sw1005 sw1010 sw1020 sw1050 sw1100 sw2005 sw2010 sw2020  \\\n",
       "UTC_START                                                                     \n",
       "2017-11-11 20:00:00  0.035   0.11    NaN    NaN    NaN  -9.99      0    NaN   \n",
       "2017-11-11 21:00:00  0.035  0.108    NaN    NaN    NaN  0.026  0.039    NaN   \n",
       "2017-11-11 22:00:00  0.032  0.109    NaN    NaN    NaN  0.027  0.039    NaN   \n",
       "2017-11-11 23:00:00  0.031  0.107    NaN    NaN    NaN  0.024  0.042    NaN   \n",
       "2017-11-12 00:00:00  0.032  0.107    NaN    NaN    NaN  0.024   0.04    NaN   \n",
       "...                    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2020-07-31 19:00:00  0.075  0.143    NaN    NaN    NaN  0.091  0.085    NaN   \n",
       "2020-07-31 20:00:00  0.077  0.145    NaN    NaN    NaN  0.093  0.086    NaN   \n",
       "2020-07-31 21:00:00  0.076  0.147    NaN    NaN    NaN  0.091  0.088    NaN   \n",
       "2020-07-31 22:00:00  0.075  0.149    NaN    NaN    NaN  0.093   0.09    NaN   \n",
       "2020-07-31 23:00:00  0.074   0.15    NaN    NaN    NaN  0.092  0.091    NaN   \n",
       "\n",
       "NAME                sw2050 sw2100 sw3005 sw3010 sw3020 sw3050 sw3100  \n",
       "UTC_START                                                             \n",
       "2017-11-11 20:00:00    NaN    NaN      0      0    NaN    NaN    NaN  \n",
       "2017-11-11 21:00:00    NaN    NaN  0.026  0.038    NaN    NaN    NaN  \n",
       "2017-11-11 22:00:00    NaN    NaN  0.028  0.039    NaN    NaN    NaN  \n",
       "2017-11-11 23:00:00    NaN    NaN  0.028  0.039    NaN    NaN    NaN  \n",
       "2017-11-12 00:00:00    NaN    NaN  0.028  0.039    NaN    NaN    NaN  \n",
       "...                    ...    ...    ...    ...    ...    ...    ...  \n",
       "2020-07-31 19:00:00    NaN    NaN  0.078  0.069    NaN    NaN    NaN  \n",
       "2020-07-31 20:00:00    NaN    NaN  0.078  0.069    NaN    NaN    NaN  \n",
       "2020-07-31 21:00:00    NaN    NaN  0.078  0.069    NaN    NaN    NaN  \n",
       "2020-07-31 22:00:00    NaN    NaN   0.08  0.069    NaN    NaN    NaN  \n",
       "2020-07-31 23:00:00    NaN    NaN  0.081  0.069    NaN    NaN    NaN  \n",
       "\n",
       "[23787 rows x 15 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_station_features.loc[\n",
    "    df_station_features.NAME.isin(\n",
    "        [\n",
    "            'sw1005', 'sw1010', 'sw2005', \n",
    "            'sw2010','sw3005', 'sw3010', \n",
    "            'sw1020', 'sw1050', 'sw1100', \n",
    "            'sw2020','sw2050', 'sw2100', \n",
    "            'sw3020', 'sw3050', 'sw3100',\n",
    "        ]\n",
    "    )\n",
    "].pivot(index=\"UTC_START\", columns=\"NAME\", values=\"VALUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>NAME</th>\n",
       "      <th>p_official</th>\n",
       "      <th>sw1005</th>\n",
       "      <th>sw1010</th>\n",
       "      <th>sw1020</th>\n",
       "      <th>sw1050</th>\n",
       "      <th>sw1100</th>\n",
       "      <th>sw2005</th>\n",
       "      <th>sw2010</th>\n",
       "      <th>sw2020</th>\n",
       "      <th>sw2050</th>\n",
       "      <th>sw2100</th>\n",
       "      <th>sw3005</th>\n",
       "      <th>sw3010</th>\n",
       "      <th>sw3020</th>\n",
       "      <th>sw3050</th>\n",
       "      <th>sw3100</th>\n",
       "      <th>t_official</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTC_START</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-01 00:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 01:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 02:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 03:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 04:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 19:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 20:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 21:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 22:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 23:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28460 rows  17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "NAME                p_official sw1005 sw1010 sw1020 sw1050 sw1100 sw2005  \\\n",
       "UTC_START                                                                  \n",
       "2017-05-01 00:00:00          0    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 01:00:00          0    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 02:00:00          0    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 03:00:00          0    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 04:00:00          0    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "...                        ...    ...    ...    ...    ...    ...    ...   \n",
       "2020-07-31 19:00:00          0  0.075  0.143    NaN    NaN    NaN  0.091   \n",
       "2020-07-31 20:00:00          0  0.077  0.145    NaN    NaN    NaN  0.093   \n",
       "2020-07-31 21:00:00          0  0.076  0.147    NaN    NaN    NaN  0.091   \n",
       "2020-07-31 22:00:00          0  0.075  0.149    NaN    NaN    NaN  0.093   \n",
       "2020-07-31 23:00:00          0  0.074   0.15    NaN    NaN    NaN  0.092   \n",
       "\n",
       "NAME                sw2010 sw2020 sw2050 sw2100 sw3005 sw3010 sw3020 sw3050  \\\n",
       "UTC_START                                                                     \n",
       "2017-05-01 00:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 01:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 02:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 03:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 04:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "...                    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2020-07-31 19:00:00  0.085    NaN    NaN    NaN  0.078  0.069    NaN    NaN   \n",
       "2020-07-31 20:00:00  0.086    NaN    NaN    NaN  0.078  0.069    NaN    NaN   \n",
       "2020-07-31 21:00:00  0.088    NaN    NaN    NaN  0.078  0.069    NaN    NaN   \n",
       "2020-07-31 22:00:00   0.09    NaN    NaN    NaN   0.08  0.069    NaN    NaN   \n",
       "2020-07-31 23:00:00  0.091    NaN    NaN    NaN  0.081  0.069    NaN    NaN   \n",
       "\n",
       "NAME                sw3100 t_official  \n",
       "UTC_START                              \n",
       "2017-05-01 00:00:00    NaN     17.898  \n",
       "2017-05-01 01:00:00    NaN      16.65  \n",
       "2017-05-01 02:00:00    NaN     12.031  \n",
       "2017-05-01 03:00:00    NaN     10.618  \n",
       "2017-05-01 04:00:00    NaN      9.832  \n",
       "...                    ...        ...  \n",
       "2020-07-31 19:00:00    NaN     35.316  \n",
       "2020-07-31 20:00:00    NaN     36.386  \n",
       "2020-07-31 21:00:00    NaN     36.395  \n",
       "2020-07-31 22:00:00    NaN     36.463  \n",
       "2020-07-31 23:00:00    NaN     35.235  \n",
       "\n",
       "[28460 rows x 17 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the filtered targets dataframe's index to filter the pivoted features dataframe.\n",
    "# That way, we have only feature locations with single-label multiclass features.\n",
    "df_pivoted_features_singlelabel = df_station_features.pivot(\n",
    "    index=\"UTC_START\", columns=\"NAME\", values=\"VALUE\"            # Create a pivoted DF of the features.\n",
    ").loc[\n",
    "    df_pivoted_targets_singlelabel.index                         # Filter the pivoted features DF using the datetimes of the remaining targets.\n",
    "]\n",
    "df_pivoted_features_singlelabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>NAME</th>\n",
       "      <th>sw1005</th>\n",
       "      <th>sw1010</th>\n",
       "      <th>sw1020</th>\n",
       "      <th>sw1050</th>\n",
       "      <th>sw1100</th>\n",
       "      <th>sw2005</th>\n",
       "      <th>sw2010</th>\n",
       "      <th>sw2020</th>\n",
       "      <th>sw2050</th>\n",
       "      <th>sw2100</th>\n",
       "      <th>sw3005</th>\n",
       "      <th>sw3010</th>\n",
       "      <th>sw3020</th>\n",
       "      <th>sw3050</th>\n",
       "      <th>sw3100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTC_START</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-01 00:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 01:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 02:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 03:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 04:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "NAME                sw1005 sw1010 sw1020 sw1050 sw1100 sw2005 sw2010 sw2020  \\\n",
       "UTC_START                                                                     \n",
       "2017-05-01 00:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 01:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 02:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 03:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 04:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "NAME                sw2050 sw2100 sw3005 sw3010 sw3020 sw3050 sw3100  \n",
       "UTC_START                                                             \n",
       "2017-05-01 00:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "2017-05-01 01:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "2017-05-01 02:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "2017-05-01 03:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "2017-05-01 04:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the pivoted single-label targets DF using the remaining feature DF datetime indices.\n",
    "df_pivoted_targets_singlelabel =\\\n",
    "    df_pivoted_targets_singlelabel.loc[\n",
    "        df_pivoted_features_singlelabel.index\n",
    "    ]\n",
    "df_pivoted_targets_singlelabel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now both the pivoted targets and the pivoted features have the same datetime indices.\n",
    "# Now I need to go back and reformat the remaining targets so that they're a \n",
    "# single-column series, rather than a multi-dimensional dataframe.\n",
    "# Get just the remaining single-label target values.\n",
    "# I'll use these to get down to one label entry per datetime row.\n",
    "df_pivoted_targets_singlelabel =\\\n",
    "    clean_tags_dataframe(df_pivoted_targets_singlelabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>NAME</th>\n",
       "      <th>sw1005</th>\n",
       "      <th>sw1010</th>\n",
       "      <th>sw1020</th>\n",
       "      <th>sw1050</th>\n",
       "      <th>sw1100</th>\n",
       "      <th>sw2005</th>\n",
       "      <th>sw2010</th>\n",
       "      <th>sw2020</th>\n",
       "      <th>sw2050</th>\n",
       "      <th>sw2100</th>\n",
       "      <th>sw3005</th>\n",
       "      <th>sw3010</th>\n",
       "      <th>sw3020</th>\n",
       "      <th>sw3050</th>\n",
       "      <th>sw3100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTC_START</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-01 00:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 01:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 02:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 03:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 04:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 19:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 20:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 21:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 22:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 23:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28460 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "NAME                sw1005 sw1010 sw1020 sw1050 sw1100 sw2005 sw2010 sw2020  \\\n",
       "UTC_START                                                                     \n",
       "2017-05-01 00:00:00                                                           \n",
       "2017-05-01 01:00:00                                                           \n",
       "2017-05-01 02:00:00                                                           \n",
       "2017-05-01 03:00:00                                                           \n",
       "2017-05-01 04:00:00                                                           \n",
       "...                    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2020-07-31 19:00:00                                                           \n",
       "2020-07-31 20:00:00                                                           \n",
       "2020-07-31 21:00:00                                                           \n",
       "2020-07-31 22:00:00                                                           \n",
       "2020-07-31 23:00:00                                                           \n",
       "\n",
       "NAME                sw2050 sw2100 sw3005 sw3010 sw3020 sw3050 sw3100  \n",
       "UTC_START                                                             \n",
       "2017-05-01 00:00:00                                                   \n",
       "2017-05-01 01:00:00                                                   \n",
       "2017-05-01 02:00:00                                                   \n",
       "2017-05-01 03:00:00                                                   \n",
       "2017-05-01 04:00:00                                                   \n",
       "...                    ...    ...    ...    ...    ...    ...    ...  \n",
       "2020-07-31 19:00:00                                                   \n",
       "2020-07-31 20:00:00                                                   \n",
       "2020-07-31 21:00:00                                                   \n",
       "2020-07-31 22:00:00                                                   \n",
       "2020-07-31 23:00:00                                                   \n",
       "\n",
       "[28460 rows x 15 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pivoted_targets_singlelabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TAGS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTC_START</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-01 00:00:00</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 01:00:00</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 02:00:00</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 03:00:00</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 04:00:00</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 19:00:00</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 20:00:00</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 21:00:00</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 22:00:00</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 23:00:00</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28460 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    TAGS\n",
       "UTC_START               \n",
       "2017-05-01 00:00:00     \n",
       "2017-05-01 01:00:00     \n",
       "2017-05-01 02:00:00     \n",
       "2017-05-01 03:00:00     \n",
       "2017-05-01 04:00:00     \n",
       "...                  ...\n",
       "2020-07-31 19:00:00     \n",
       "2020-07-31 20:00:00     \n",
       "2020-07-31 21:00:00     \n",
       "2020-07-31 22:00:00     \n",
       "2020-07-31 23:00:00     \n",
       "\n",
       "[28460 rows x 1 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targets_singlelabel =\\\n",
    "    convert_targs_to_single_column(df_pivoted_targets_singlelabel)\n",
    "\n",
    "df_targets_singlelabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final set of unique labels in targets' pd.series object:          []\n",
      "Number of final labels after pivoting and matching:               0\n",
      "Number of final pivoted features:                                 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop any and all feature rows that have NaN values.\n",
    "df_pivoted_features_singlelabel = df_pivoted_features_singlelabel.dropna(how=\"any\", axis=0)\n",
    "\n",
    "# Filter the targets based on the remaining datetime\n",
    "# indices from the NaN-filtered pivoted features.\n",
    "df_targets_singlelabel =\\\n",
    "    df_targets_singlelabel.loc[\n",
    "        df_pivoted_features_singlelabel.index\n",
    "    ]\n",
    "\n",
    "print()\n",
    "print(\"Final set of unique labels in targets' pd.series object:         \", df_targets_singlelabel[\"TAGS\"].unique())\n",
    "print(\"Number of final labels after pivoting and matching:              \", df_targets_singlelabel.shape[0])\n",
    "print(\"Number of final pivoted features:                                \", df_pivoted_features_singlelabel.shape[0])\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

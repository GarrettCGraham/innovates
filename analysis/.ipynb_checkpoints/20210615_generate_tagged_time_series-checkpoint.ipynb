{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production of tagged time series for analysis\n",
    "\n",
    "The time series will be predicted upon by the trained classifier and then cached for future data viz and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the requisite libraries and submodules.\n",
    "import copy\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import time\n",
    "import xgboost as xgb\n",
    "\n",
    "from datetime import datetime\n",
    "from dynomics import models\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Define file locations.\n",
    "DATA_DIR = \"../data/multiclass/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the station IDs. Some of the Accl\n",
    "station_filenames_list = [\n",
    "    filename for filename in os.listdir(path=DATA_DIR)\n",
    "    if (filename[-7:]==\".pickle\") and (\"53155\" not in filename)\n",
    "]\n",
    "\n",
    "# List of \n",
    "stations_id_list = list(\n",
    "    set(\n",
    "        fname.split(\"_\")[2].split(\".\")[0] for fname in station_filenames_list\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load features and targets into features and targets dictionaries, indexed\n",
    "# by station ID number.\n",
    "template_dict = {\n",
    "    station_id : None\n",
    "    for station_id in stations_id_list\n",
    "    if station_id != \"53155\"     # Station 53155's sensor data had data missing for at least one sensor\n",
    "                               # at literally every time point, so my data preprocessing eliminated it. \n",
    "}\n",
    "features_dict,targets_dict = copy.deepcopy(template_dict), copy.deepcopy(template_dict)\n",
    "del(template_dict)\n",
    "\n",
    "for fname in station_filenames_list:\n",
    "    station_id = fname.split(\"_\")[2].split(\".\")[0]\n",
    "    if station_id=='53155':\n",
    "        continue\n",
    "    elif \"features\" in fname:\n",
    "        features_dict[station_id] = pd.read_pickle(\n",
    "            DATA_DIR+fname,\n",
    "        )\n",
    "    elif \"targets\" in fname:\n",
    "        targets_dict[station_id] = pd.read_pickle(\n",
    "            DATA_DIR+fname,\n",
    "        )\n",
    "\n",
    "final_stations_list = [\n",
    "    station_id for station_id in features_dict.keys()\n",
    "]\n",
    "\n",
    "# Got the following station ID info from this URL:\n",
    "# https://mesonet.agron.iastate.edu/sites/site.php?station=23906&network=USCRN\n",
    "port_aransas_stat_id = \"23906\"\n",
    "feats_df_port_aransas = features_dict[port_aransas_stat_id]\n",
    "targs_df_port_aransas = targets_dict[port_aransas_stat_id]\n",
    "\n",
    "# Got the following date-exclusion idea from here:\n",
    "# https://stackoverflow.com/questions/55680603/pandas-filter-on-datetimeindex-by-excluding-date-range\n",
    "exclusion_dates = pd.date_range(start=\"2019-08-01\", end=\"2019-09-01\")\n",
    "\n",
    "features_dict[port_aransas_stat_id] = feats_df_port_aransas.loc[~feats_df_port_aransas.index.isin(exclusion_dates)]\n",
    "\n",
    "targets_dict[port_aransas_stat_id] = targs_df_port_aransas.loc[~targs_df_port_aransas.index.isin(exclusion_dates)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ML functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mkdirs(*paths):\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "    return\n",
    "\n",
    "\n",
    "def fix_conf_mat_labels(conf_mat):\n",
    "    # Format metal names so as to remove the remnants of tuple-formatting,\n",
    "    # which is an artifact of the MySQL database and its concentration \n",
    "    # specifications.\n",
    "    new_confusion_matrix_labels = [\n",
    "        label.split(\"'\")[1] \n",
    "        if \"'\" in label else label\n",
    "        for label in conf_mat.index.values\n",
    "    ]\n",
    "    conf_mat.index = new_confusion_matrix_labels\n",
    "    conf_mat.columns = new_confusion_matrix_labels\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "        cm_df,\n",
    "        title=None,\n",
    "        cmap=plt.cm.Blues,\n",
    "        fontsize=12,\n",
    "        fontcolor=None,\n",
    "        num_round=4,\n",
    "        plot_top=0.88,\n",
    "        cbar_ticks=None,\n",
    "        cbar_min_divisor=2,\n",
    "        figsize=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create and return a matplotlib figure representing a confusion matrix.\n",
    "\n",
    "    Input:\n",
    "        cm_df : pandas.DataFrame\n",
    "            a pandas dataframe representing a confusion matrix\n",
    "        title : str\n",
    "            a plot title\n",
    "        cmap : color map\n",
    "            some pyplot colormap to use in plotting\n",
    "        fontsize : int\n",
    "            how large the text in each posititon of the matrix should be\n",
    "        fontcolor : str\n",
    "            the color that the text in each position of the matrix\n",
    "    Return: pyplot.figure\n",
    "        a figure object representing the plot\n",
    "\"\"\"\n",
    "\n",
    "    # Set figure title.\n",
    "    if title is None:\n",
    "        title = 'Confusion matrix'\n",
    "\n",
    "    # Set figure fontcolor.\n",
    "    if fontcolor is None:\n",
    "        fontcolor = \"black\"\n",
    "    \n",
    "    if figsize is None:\n",
    "        figsize = (14, 10)\n",
    "\n",
    "    conf_mat = cm_df.values\n",
    "    conf_mat_nozeros = cm_df.copy()\n",
    "    #     conf_mat_nozeros['Sum'] = 0\n",
    "    #     conf_mat_nozeros.loc['Sum'] = 0\n",
    "    conf_mat_nozeros = conf_mat_nozeros.values\n",
    "\n",
    "    # Get class names.\n",
    "    classes = cm_df.index\n",
    "\n",
    "    # Set color bar ticks and format their labels.\n",
    "    if cbar_ticks is None:\n",
    "        cbar_ticks = [0.001, 0.01, 0.1, 0.2, 0.4, 0.8, 1.0]\n",
    "    cbar_tick_labels = [str(label) for label in cbar_ticks]\n",
    "\n",
    "    # Set color bar minimum and maximum.\n",
    "    cbar_min = np.min(\n",
    "        [i for i in cm_df.values.ravel() if i > 0]) / cbar_min_divisor\n",
    "    cbar_max = np.max([i for i in cm_df.values.ravel() if i < 1])\n",
    "\n",
    "    # Eliminate actual zeros from plotting data.\n",
    "    for i, row in enumerate(conf_mat):\n",
    "        for j, col in enumerate(row):\n",
    "            if col < cbar_min:\n",
    "                conf_mat_nozeros[i, j] = cbar_min\n",
    "\n",
    "    # Initialize figure and axes objects and plot colored cells.\n",
    "    fig, ax = plt.subplots(\n",
    "        1,\n",
    "        1,\n",
    "        figsize=figsize\n",
    "    )\n",
    "    cax = ax.imshow(\n",
    "        conf_mat_nozeros,\n",
    "        interpolation='nearest',\n",
    "        cmap=cmap,\n",
    "        norm=LogNorm(vmin=cbar_min, vmax=cbar_max)\n",
    "    )\n",
    "\n",
    "    # Add color bar, figure title, labels, and axis ticks.\n",
    "    cbar = fig.colorbar(cax, ax=ax, ticks=cbar_ticks)\n",
    "    cbar.ax.set_yticklabels(cbar_tick_labels)\n",
    "    fig.suptitle(\n",
    "        title,\n",
    "        **{'x': 0.53, 'y': 0.97}\n",
    "    )\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ticks = list(range(len(classes)))\n",
    "    plt.xticks(ticks, classes, rotation=45)\n",
    "    plt.yticks(ticks, classes)\n",
    "    ax.tick_params(axis=u'both', which=u'both', length=0)\n",
    "\n",
    "    # Add numerical values to the matrix's cells.\n",
    "    for i in range(conf_mat.shape[0]):\n",
    "        for j in range(conf_mat.shape[1]):\n",
    "            ax.text(\n",
    "                j,\n",
    "                i,\n",
    "                # conf_mat[i, j],\n",
    "                '{results:.{digits}f}'.format(\n",
    "                    results=conf_mat[i, j],\n",
    "                    digits=num_round\n",
    "                    ),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=fontcolor,\n",
    "                fontsize=fontsize\n",
    "            )\n",
    "\n",
    "    # Format final image for saving.\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=plot_top)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# def save_kwargs(kwargs_dict, kwargs_dict_name, data_save_dir):\n",
    "#     with open(data_save_dir+kwargs_dict_name+\".txt\", \"w\") as output_file:\n",
    "#         output_file.write(json.dumps(kwargs_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate anomaly data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_feats_dict = {\n",
    "    stat_id : None\n",
    "    for stat_id in final_stations_list\n",
    "    if features_dict[stat_id].shape[1]==15\n",
    "}\n",
    "for stat_id in final_stations_list:\n",
    "    \n",
    "    df_to_scale = copy.deepcopy(features_dict[stat_id]).drop(columns=[\"p_official\", \"t_official\"])\n",
    "    \n",
    "    if df_to_scale.shape[1]==15:\n",
    "#         df_to_scale[(df_to_scale == -99999)] = pd.NA\n",
    "#         df_to_scale = df_to_scale.interpolate(method=\"time\", axis=0)\n",
    "#         df_to_scale = df_to_scale.fillna(method=\"bfill\", axis=0)\n",
    "        scaler = StandardScaler(with_mean=True)\n",
    "        feats_scaled_array = scaler.fit_transform(df_to_scale)\n",
    "        feats_scaled_df = pd.DataFrame(data=feats_scaled_array, index=df_to_scale.index, columns=df_to_scale.columns)\n",
    "#         if np.isnan(feats_scaled_array).sum() == 0:\n",
    "        scaled_feats_dict[stat_id] = copy.deepcopy(feats_scaled_df)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "final_station_ids_list = [stat_id for stat_id in scaled_feats_dict.keys()]        \n",
    "\n",
    "features_dfs_list = [scaled_feats_dict[station_id] for station_id in final_station_ids_list]\n",
    "targets_dfs_list = [targets_dict[station_id] for station_id in final_station_ids_list]\n",
    "\n",
    "scaled_feats_df = pd.concat(features_dfs_list, axis=0)\n",
    "targs_df = pd.concat(targets_dfs_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(627937, 16)\n"
     ]
    }
   ],
   "source": [
    "combined_scaledfeats_targs_df = pd.concat([scaled_feats_df,targs_df], axis=1)\n",
    "\n",
    "print(combined_scaledfeats_targs_df.shape)\n",
    "\n",
    "combined_scaledfeats_targs_NONAN_df = combined_scaledfeats_targs_df.dropna(how=\"any\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-isolate the targets and then combine them into a 1-D pd.series. \n",
    "# Whenever a row has at least one anomaly, then we will collapse that row's \n",
    "# values to a single value of 1. Rows with no anomalies shall be labeled as 0.\n",
    "ml_targs_df = combined_scaledfeats_targs_NONAN_df.iloc[:, 15]\n",
    "\n",
    "# Re-isolate the ML-ready features.\n",
    "ml_feats_df = combined_scaledfeats_targs_NONAN_df.iloc[:, :15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the percentage of the ML-ready data that are anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.32% of the data are anomalies.\n",
      "normal: 97.6808 %\n",
      "noise: 1.7951 %\n",
      "spike: 0.5241 %\n"
     ]
    }
   ],
   "source": [
    "percentage_anomalies = np.round(100 * (ml_targs_df != \"\").sum()/ml_targs_df.shape[0], 2 )\n",
    "\n",
    "print(f\"{percentage_anomalies}% of the data are anomalies.\")\n",
    "\n",
    "for label in ml_targs_df.unique():\n",
    "    num_labels = (ml_targs_df == label).sum()\n",
    "    if label==\"\":\n",
    "        label=\"normal\"\n",
    "    print(label+\":\", np.round(100*num_labels/ml_targs_df.shape[0],4),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various ML parameters, plus load the optimized classer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define caching directories.\n",
    "master_dir = \"20210506_XGB_MC_random_search/\"\n",
    "plot_dir = \"plots/\" + master_dir\n",
    "data_save_dir = \"data/\" + master_dir\n",
    "check_mkdirs(plot_dir, data_save_dir)\n",
    "\n",
    "classer_init_kwargs = {\n",
    "    \"n_jobs\":-1,\n",
    "    \"objective\":\"multi:softmax\",\n",
    "}\n",
    "\n",
    "fitting_kwargs = {\n",
    "#     \"early_stopping_rounds\":50,\n",
    "    \"eval_metric\":\"merror\",\n",
    "    \"verbose\":False,\n",
    "}\n",
    "\n",
    "param_test_00 = {\n",
    "    \"n_estimators\": [int(np.round(n, 0)) for n in np.geomspace(20, 1000, num=6)], \n",
    "    \"learning_rate\":[0.1, 0.4, 0.8],\n",
    "    'max_depth':range(7, 16,2),\n",
    "    'min_child_weight':range(5, 13, 2),\n",
    "    'gamma':[i/10.0 for i in range(0, 6)]\n",
    "}\n",
    "\n",
    "features = combined_scaledfeats_targs_NONAN_df.iloc[:,0:15]\n",
    "\n",
    "targets = combined_scaledfeats_targs_NONAN_df[\"TAGS\"].to_frame()\n",
    "\n",
    "# ML RANDOMIZED SEARCH WAS PERFORMED AT THIS STEP.\n",
    "# I erased the dang thing, though, so I wouldn't accidentally re-train the classer.\n",
    "\n",
    "gridsearch_00 = joblib.load(\"data/20210506_XGB_MC_random_search/gridsearch_00.pickle\")\n",
    "\n",
    "print(gridsearch_00.best_params_)\n",
    "\n",
    "print(gridsearch_00.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate confusion matrices using best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = StratifiedShuffleSplit(n_splits=10, random_state=7)\n",
    "# cv_splits = [split_indices for split_indices in cv.split(features.values, targets.values)]\n",
    "\n",
    "# feature_frame = features\n",
    "# target_frame = targets\n",
    "# classer = gridsearch_00.best_estimator_\n",
    "\n",
    "# # Create variables to store conf matrix values during cross validation\n",
    "# unique_labels = target_frame.iloc[:, 0].unique()\n",
    "# num_labels = len(unique_labels)\n",
    "# test_mat = np.zeros((num_labels, num_labels))\n",
    "# train_mat = np.zeros((num_labels, num_labels))\n",
    "# # perform cross validation\n",
    "# for split_idx, (train, test) in enumerate(cv_splits[:1]):\n",
    "#     print(\"##################################################\")\n",
    "#     print(\"         ###### Processing \"+str(split_idx+1)+\" of \"+str(len(cv_splits))+\" ######\")\n",
    "#     print(\"            \" + time.ctime())\n",
    "#     print(\"##################################################\")\n",
    "#     # get train and test splits\n",
    "#     train_X = feature_frame.values[train]\n",
    "#     train_y = target_frame.values.ravel()[train]\n",
    "#     test_X = feature_frame.values[test]\n",
    "#     test_y = target_frame.values.ravel()[test]\n",
    "\n",
    "#     # fit training data\n",
    "#     classer.fit(\n",
    "#         train_X, train_y, \n",
    "#         eval_set=[(test_X, test_y)], \n",
    "#         verbose=False,\n",
    "#     )\n",
    "\n",
    "#     # Predict for training data and update confusion matrix\n",
    "#     train_p = classer.predict(train_X)\n",
    "#     test_p = classer.predict(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/20210615_labeled_timeseries/test/test_p.pickle']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Cache these dirs for testing.\n",
    "\n",
    "# check_mkdirs(data_save_dir+\"test/\")\n",
    "\n",
    "# joblib.dump(train_p, data_save_dir+\"test/train_p.pickle\")\n",
    "# joblib.dump(test_p, data_save_dir+\"test/test_p.pickle\")\n",
    "# joblib.dump(cv_splits, data_save_dir+\"test/cv_splits.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stitch predicted points back to their original time series locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '', '', ..., '', '', ''], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([492972, 302149, 183822, ..., 502872, 552569,  55121]),\n",
       "  array([443968,  39708,  95277, ...,   3572,  95457, 282265])),\n",
       " (array([285804,  88694,  15516, ..., 334268, 404635, 166587]),\n",
       "  array([289066, 254933, 575210, ...,  60203, 513035,  35766])),\n",
       " (array([283116, 477960, 298293, ..., 597419, 148152, 385399]),\n",
       "  array([477174, 620332, 466580, ..., 485094, 498651,  15531])),\n",
       " (array([464725, 331141, 205850, ...,  24605,  55490, 166796]),\n",
       "  array([598401, 258567, 583063, ..., 386132, 587560, 308606])),\n",
       " (array([592595, 372768, 455708, ..., 220128, 163736, 294188]),\n",
       "  array([602731, 549166,  44363, ...,  88800,  31495, 490300])),\n",
       " (array([ 57005, 312065, 207233, ..., 599628, 514811,  50218]),\n",
       "  array([ 16559, 168525, 210891, ...,  89114, 128799, 310693])),\n",
       " (array([446412, 441495, 414131, ...,   7463, 515073, 216067]),\n",
       "  array([249419, 377915, 353784, ..., 515345, 159413, 120673])),\n",
       " (array([142181, 408212, 540835, ..., 474145, 208040, 132648]),\n",
       "  array([519113, 191134, 170031, ..., 594393, 612775, 353086])),\n",
       " (array([138639, 560003, 305296, ..., 315617, 369528,  27514]),\n",
       "  array([268673, 401912, 262887, ..., 494943,  86346, 193498])),\n",
       " (array([619532, 395000, 192333, ..., 453679, 382565, 472805]),\n",
       "  array([511760, 467443, 119305, ..., 153363, 120513, 243439]))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv_splits splits on combined_scaledfeats_targs_NONAN_df.\n",
    "# combined_scaledfeats_targs_NONAN_df was created by stitching together\n",
    "# features_dfs_list = [scaled_feats_dict[station_id] for station_id in final_station_ids_list]\n",
    "# and then converting it into \n",
    "# scaled_feats_df = pd.concat(features_dfs_list, axis=0)\n",
    "# and then again into \n",
    "# combined_scaledfeats_targs_df = pd.concat([scaled_feats_df,targs_df], axis=1).\n",
    "# Phew. That's a lot of hoops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>NAME</th>\n",
       "      <th>sw1005</th>\n",
       "      <th>sw1010</th>\n",
       "      <th>sw1020</th>\n",
       "      <th>sw1050</th>\n",
       "      <th>sw1100</th>\n",
       "      <th>sw2005</th>\n",
       "      <th>sw2010</th>\n",
       "      <th>sw2020</th>\n",
       "      <th>sw2050</th>\n",
       "      <th>sw2100</th>\n",
       "      <th>sw3005</th>\n",
       "      <th>sw3010</th>\n",
       "      <th>sw3020</th>\n",
       "      <th>sw3050</th>\n",
       "      <th>sw3100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTC_START</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-11 23:00:00</th>\n",
       "      <td>-0.339417</td>\n",
       "      <td>-0.157723</td>\n",
       "      <td>-0.106653</td>\n",
       "      <td>-0.523950</td>\n",
       "      <td>-0.685879</td>\n",
       "      <td>-1.255675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.054003</td>\n",
       "      <td>-0.210345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104618</td>\n",
       "      <td>0.120708</td>\n",
       "      <td>-1.728136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.977957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 00:00:00</th>\n",
       "      <td>-0.339417</td>\n",
       "      <td>-0.162261</td>\n",
       "      <td>-0.111396</td>\n",
       "      <td>-0.527781</td>\n",
       "      <td>-0.685879</td>\n",
       "      <td>-1.255675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.054003</td>\n",
       "      <td>-0.210345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104618</td>\n",
       "      <td>0.120708</td>\n",
       "      <td>-1.728136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.977957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 01:00:00</th>\n",
       "      <td>-0.339417</td>\n",
       "      <td>-0.162261</td>\n",
       "      <td>-0.111396</td>\n",
       "      <td>-0.543104</td>\n",
       "      <td>-0.685879</td>\n",
       "      <td>-1.255675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.054003</td>\n",
       "      <td>-0.210345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104618</td>\n",
       "      <td>0.120708</td>\n",
       "      <td>-1.739403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.977957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 02:00:00</th>\n",
       "      <td>-0.343474</td>\n",
       "      <td>-0.162261</td>\n",
       "      <td>-0.106653</td>\n",
       "      <td>-0.543104</td>\n",
       "      <td>-0.685879</td>\n",
       "      <td>-1.255675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.054003</td>\n",
       "      <td>-0.210345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104618</td>\n",
       "      <td>0.120708</td>\n",
       "      <td>-1.750670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.977957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 03:00:00</th>\n",
       "      <td>-0.343474</td>\n",
       "      <td>-0.162261</td>\n",
       "      <td>-0.106653</td>\n",
       "      <td>-0.543104</td>\n",
       "      <td>-0.679400</td>\n",
       "      <td>-1.255675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.054003</td>\n",
       "      <td>-0.210345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104618</td>\n",
       "      <td>0.120708</td>\n",
       "      <td>-1.750670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.977957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 19:00:00</th>\n",
       "      <td>-1.019930</td>\n",
       "      <td>-0.437291</td>\n",
       "      <td>-0.051922</td>\n",
       "      <td>-1.472280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.696185</td>\n",
       "      <td>-1.463738</td>\n",
       "      <td>-0.935376</td>\n",
       "      <td>-1.132814</td>\n",
       "      <td>-1.185998</td>\n",
       "      <td>-0.639980</td>\n",
       "      <td>-0.026778</td>\n",
       "      <td>-1.145286</td>\n",
       "      <td>-1.435158</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 20:00:00</th>\n",
       "      <td>-1.019930</td>\n",
       "      <td>-0.437291</td>\n",
       "      <td>-0.012781</td>\n",
       "      <td>-1.472280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.708920</td>\n",
       "      <td>-1.463738</td>\n",
       "      <td>-0.962421</td>\n",
       "      <td>-1.150544</td>\n",
       "      <td>-1.185998</td>\n",
       "      <td>-0.659533</td>\n",
       "      <td>-0.033970</td>\n",
       "      <td>-1.175937</td>\n",
       "      <td>-1.435158</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 21:00:00</th>\n",
       "      <td>-1.040264</td>\n",
       "      <td>-0.415954</td>\n",
       "      <td>-0.061707</td>\n",
       "      <td>-1.472280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.721654</td>\n",
       "      <td>-1.478579</td>\n",
       "      <td>-0.962421</td>\n",
       "      <td>-1.186004</td>\n",
       "      <td>-1.185998</td>\n",
       "      <td>-0.698637</td>\n",
       "      <td>-0.048354</td>\n",
       "      <td>-1.191263</td>\n",
       "      <td>-1.472960</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 22:00:00</th>\n",
       "      <td>-1.050431</td>\n",
       "      <td>-0.430179</td>\n",
       "      <td>0.045930</td>\n",
       "      <td>-1.472280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.734388</td>\n",
       "      <td>-1.478579</td>\n",
       "      <td>-0.962421</td>\n",
       "      <td>-1.132814</td>\n",
       "      <td>-1.185998</td>\n",
       "      <td>-0.708413</td>\n",
       "      <td>-0.048354</td>\n",
       "      <td>-1.191263</td>\n",
       "      <td>-1.454059</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 23:00:00</th>\n",
       "      <td>-1.050431</td>\n",
       "      <td>-0.423066</td>\n",
       "      <td>-0.081277</td>\n",
       "      <td>-1.488210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.747123</td>\n",
       "      <td>-1.463738</td>\n",
       "      <td>-0.975943</td>\n",
       "      <td>-1.132814</td>\n",
       "      <td>-1.185998</td>\n",
       "      <td>-0.708413</td>\n",
       "      <td>-0.062738</td>\n",
       "      <td>-1.206589</td>\n",
       "      <td>-1.454059</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>627937 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "NAME                   sw1005    sw1010    sw1020    sw1050    sw1100  \\\n",
       "UTC_START                                                               \n",
       "2019-12-11 23:00:00 -0.339417 -0.157723 -0.106653 -0.523950 -0.685879   \n",
       "2019-12-12 00:00:00 -0.339417 -0.162261 -0.111396 -0.527781 -0.685879   \n",
       "2019-12-12 01:00:00 -0.339417 -0.162261 -0.111396 -0.543104 -0.685879   \n",
       "2019-12-12 02:00:00 -0.343474 -0.162261 -0.106653 -0.543104 -0.685879   \n",
       "2019-12-12 03:00:00 -0.343474 -0.162261 -0.106653 -0.543104 -0.679400   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2020-07-31 19:00:00 -1.019930 -0.437291 -0.051922 -1.472280  0.000000   \n",
       "2020-07-31 20:00:00 -1.019930 -0.437291 -0.012781 -1.472280  0.000000   \n",
       "2020-07-31 21:00:00 -1.040264 -0.415954 -0.061707 -1.472280  0.000000   \n",
       "2020-07-31 22:00:00 -1.050431 -0.430179  0.045930 -1.472280  0.000000   \n",
       "2020-07-31 23:00:00 -1.050431 -0.423066 -0.081277 -1.488210  0.000000   \n",
       "\n",
       "NAME                   sw2005    sw2010    sw2020    sw2050    sw2100  \\\n",
       "UTC_START                                                               \n",
       "2019-12-11 23:00:00 -1.255675  0.000000 -0.054003 -0.210345  0.000000   \n",
       "2019-12-12 00:00:00 -1.255675  0.000000 -0.054003 -0.210345  0.000000   \n",
       "2019-12-12 01:00:00 -1.255675  0.000000 -0.054003 -0.210345  0.000000   \n",
       "2019-12-12 02:00:00 -1.255675  0.000000 -0.054003 -0.210345  0.000000   \n",
       "2019-12-12 03:00:00 -1.255675  0.000000 -0.054003 -0.210345  0.000000   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2020-07-31 19:00:00 -0.696185 -1.463738 -0.935376 -1.132814 -1.185998   \n",
       "2020-07-31 20:00:00 -0.708920 -1.463738 -0.962421 -1.150544 -1.185998   \n",
       "2020-07-31 21:00:00 -0.721654 -1.478579 -0.962421 -1.186004 -1.185998   \n",
       "2020-07-31 22:00:00 -0.734388 -1.478579 -0.962421 -1.132814 -1.185998   \n",
       "2020-07-31 23:00:00 -0.747123 -1.463738 -0.975943 -1.132814 -1.185998   \n",
       "\n",
       "NAME                   sw3005    sw3010    sw3020    sw3050    sw3100  \n",
       "UTC_START                                                              \n",
       "2019-12-11 23:00:00  0.104618  0.120708 -1.728136  0.000000 -1.977957  \n",
       "2019-12-12 00:00:00  0.104618  0.120708 -1.728136  0.000000 -1.977957  \n",
       "2019-12-12 01:00:00  0.104618  0.120708 -1.739403  0.000000 -1.977957  \n",
       "2019-12-12 02:00:00  0.104618  0.120708 -1.750670  0.000000 -1.977957  \n",
       "2019-12-12 03:00:00  0.104618  0.120708 -1.750670  0.000000 -1.977957  \n",
       "...                       ...       ...       ...       ...       ...  \n",
       "2020-07-31 19:00:00 -0.639980 -0.026778 -1.145286 -1.435158  0.000000  \n",
       "2020-07-31 20:00:00 -0.659533 -0.033970 -1.175937 -1.435158  0.000000  \n",
       "2020-07-31 21:00:00 -0.698637 -0.048354 -1.191263 -1.472960  0.000000  \n",
       "2020-07-31 22:00:00 -0.708413 -0.048354 -1.191263 -1.454059  0.000000  \n",
       "2020-07-31 23:00:00 -0.708413 -0.062738 -1.206589 -1.454059  0.000000  \n",
       "\n",
       "[627937 rows x 15 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_feats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = 0\n",
    "for station_id in final_station_ids_list:\n",
    "    total_length+=scaled_feats_dict[station_id].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

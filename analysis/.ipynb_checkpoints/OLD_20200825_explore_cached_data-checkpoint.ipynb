{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names_list = [\"station_id\", 'date', 'name', 'value', 'range_flag', 'door_flag', 'frozen_flag', 'manual_flag']\n",
    "\n",
    "# # Ingest raw data for single soil station.\n",
    "# df = pd.read_csv(\n",
    "#     data_dir+\"acclima_soil_water_rleeper_FULL.csv\",\n",
    "#     header=\"infer\",\n",
    "#     index_col=False\n",
    "# )\n",
    "\n",
    "# df = df.drop(columns=[\"TAGS\"])\n",
    "# df.columns = column_names_list\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_station_ids = [\n",
    "    int(stat_id)\n",
    "    if stat_id!=\"UN\" else stat_id\n",
    "    for stat_id in df.station_id.values\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.station_id = new_station_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of all unique station IDs.\n",
    "station_id_array = df.station_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garrettgraham/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Isolate the current station's rows.\n",
    "df_station = df[df.station_id == station_id_array[0]]\n",
    "\n",
    "# Convert all values in \"date\" column to datetime objects.\n",
    "df_station[\"date\"] = pd.to_datetime(df_station[\"date\"], format='%Y-%m-%d %H:%M:%S',)\n",
    "\n",
    "# Pivot the station's data and target values into ML-compatible arrays. \n",
    "df_station_data =\\\n",
    "    df_station.pivot(index=\"date\", columns=\"name\", values=\"value\")\n",
    "\n",
    "# Create the class labels DF for the station.\n",
    "df_station_targets =\\\n",
    "    df_station.pivot(\n",
    "        index=\"date\", columns=\"name\", values=\"manual_flag\"\n",
    "    )\n",
    "\n",
    "# Create a precipitation and temperature DF.\n",
    "df_station_pt_data = df_station_data[[\"p_official\", \"t_official\"]]\n",
    "\n",
    "# Drop irrelevant columns from the DFs.\n",
    "df_station_data = df_station_data.drop(\n",
    "        columns=[\"p_official\", \"t_official\"]\n",
    "    )\n",
    "df_station_targets = df_station_targets.drop(\n",
    "        columns=[\"p_official\", \"t_official\"]\n",
    "    )\n",
    "\n",
    "# Get rid of rows of missing data and then drop those rows from the targets dataframe.\n",
    "df_station_data = df_station_data.dropna(how=\"all\")\n",
    "df_station_pt_data = df_station_pt_data.loc[df_station_data.index]\n",
    "df_station_targets = df_station_targets.loc[df_station_data.index]\n",
    "\n",
    "# Add the precip and temp data back onto the station data.\n",
    "df_station_data[\"p_official\"] = df_station_pt_data[\"p_official\"]\n",
    "df_station_data[\"t_official\"] = df_station_pt_data[\"t_official\"]\n",
    "\n",
    "# # Save the station's data and labels as a CSV\n",
    "df_station_data.to_csv(\"../data/stations/data_\"+str(station_id_array[0])+\".csv\")\n",
    "df_station_targets.to_csv(\"../data/stations/targets_\"+str(station_id_array[0])+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station_data[\"p_official\"] = df_station_pt_data[\"p_official\"]\n",
    "df_station_data[\"t_official\"] = df_station_pt_data[\"t_official\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset and cache all station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garrettgraham/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "pbar = ProgressBar()\n",
    "\n",
    "for station_id in pbar(station_id_array):\n",
    "    \n",
    "    # Isolate the current station's rows.\n",
    "    df_station = df[df.station_id == station_id]\n",
    "\n",
    "    # Convert all values in \"date\" column to datetime objects.\n",
    "    df_station[\"date\"] = pd.to_datetime(df_station[\"date\"], format='%Y-%m-%d %H:%M:%S',)\n",
    "\n",
    "    # Pivot the station's data and target values into ML-compatible arrays. \n",
    "    df_station_data =\\\n",
    "        df_station.pivot(index=\"date\", columns=\"name\", values=\"value\")\n",
    "\n",
    "    # Create the class labels DF for the station.\n",
    "    df_station_targets =\\\n",
    "        df_station.pivot(\n",
    "            index=\"date\", columns=\"name\", values=\"manual_flag\"\n",
    "        )\n",
    "\n",
    "    # Create a precipitation and temperature DF.\n",
    "    df_station_pt_data = df_station_data[[\"p_official\", \"t_official\"]]\n",
    "\n",
    "    # Drop irrelevant columns from the DFs.\n",
    "    df_station_data = df_station_data.drop(\n",
    "            columns=[\"p_official\", \"t_official\"]\n",
    "        )\n",
    "    df_station_targets = df_station_targets.drop(\n",
    "            columns=[\"p_official\", \"t_official\"]\n",
    "        )\n",
    "\n",
    "    # Get rid of rows of missing data and then drop those rows from the targets dataframe.\n",
    "    df_station_data = df_station_data.dropna(how=\"all\")\n",
    "    df_station_pt_data = df_station_pt_data.loc[df_station_data.index]\n",
    "    df_station_targets = df_station_targets.loc[df_station_data.index]\n",
    "\n",
    "    # Add the precip and temp data back onto the station data.\n",
    "    df_station_data[\"p_official\"] = df_station_pt_data[\"p_official\"]\n",
    "    df_station_data[\"t_official\"] = df_station_pt_data[\"t_official\"]\n",
    "\n",
    "    # # Save the station's data and labels as a CSV\n",
    "    df_station_data.to_csv(\"../data/stations/data_\"+str(station_id)+\".csv\")\n",
    "    df_station_targets.to_csv(\"../data/stations/targets_\"+str(station_id)+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that data and targets were properly cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sw1005</th>\n",
       "      <th>sw1010</th>\n",
       "      <th>sw1020</th>\n",
       "      <th>sw1050</th>\n",
       "      <th>sw1100</th>\n",
       "      <th>sw2005</th>\n",
       "      <th>sw2010</th>\n",
       "      <th>sw2020</th>\n",
       "      <th>sw2050</th>\n",
       "      <th>sw2100</th>\n",
       "      <th>sw3005</th>\n",
       "      <th>sw3010</th>\n",
       "      <th>sw3020</th>\n",
       "      <th>sw3050</th>\n",
       "      <th>sw3100</th>\n",
       "      <th>p_official</th>\n",
       "      <th>t_official</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01 15:00:00</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-6.412</td>\n",
       "      <td>-6.824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-01 16:00:00</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.115</td>\n",
       "      <td>22.290</td>\n",
       "      <td>21.900</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>0.149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-01 17:00:00</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-99999.000</td>\n",
       "      <td>-99999.000</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-01 18:00:00</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-99999.000</td>\n",
       "      <td>-99999.000</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-01 19:00:00</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-99999.000</td>\n",
       "      <td>-99999.000</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  sw1005  sw1010  sw1020  sw1050  sw1100  sw2005  \\\n",
       "0  2018-03-01 15:00:00   0.100   0.110   0.136   0.122   0.151   0.103   \n",
       "1  2018-03-01 16:00:00   0.111   0.124   0.154   0.138   0.172   0.116   \n",
       "2  2018-03-01 17:00:00   0.114   0.123   0.155   0.138   0.172   0.116   \n",
       "3  2018-03-01 18:00:00   0.114   0.125   0.156   0.138   0.172   0.118   \n",
       "4  2018-03-01 19:00:00   0.115   0.126   0.159   0.138   0.172   0.119   \n",
       "\n",
       "   sw2010     sw2020     sw2050   sw2100   sw3005  sw3010   sw3020   sw3050  \\\n",
       "0   0.103     -6.412     -6.824      0.0      0.0   0.082      0.0      0.0   \n",
       "1   0.115     22.290     21.900 -99999.0 -99999.0   0.094 -99999.0 -99999.0   \n",
       "2   0.116 -99999.000 -99999.000 -99999.0 -99999.0   0.091 -99999.0 -99999.0   \n",
       "3   0.116 -99999.000 -99999.000 -99999.0 -99999.0   0.090 -99999.0 -99999.0   \n",
       "4   0.116 -99999.000 -99999.000 -99999.0 -99999.0   0.091 -99999.0 -99999.0   \n",
       "\n",
       "   sw3100  p_official  t_official  \n",
       "0   0.132         NaN         NaN  \n",
       "1   0.149         NaN         NaN  \n",
       "2   0.149         0.0      27.284  \n",
       "3   0.149         0.0      28.615  \n",
       "4   0.149         0.0      29.304  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"../data/stations/\"\n",
    "test_station_id = 12987\n",
    "test_data = pd.read_csv(data_dir+f\"data_{test_station_id}.csv\")\n",
    "test_targets = pd.read_csv(data_dir+f\"targets_{test_station_id}.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sw1005</th>\n",
       "      <th>sw1010</th>\n",
       "      <th>sw1020</th>\n",
       "      <th>sw1050</th>\n",
       "      <th>sw1100</th>\n",
       "      <th>sw2005</th>\n",
       "      <th>sw2010</th>\n",
       "      <th>sw2020</th>\n",
       "      <th>sw2050</th>\n",
       "      <th>sw2100</th>\n",
       "      <th>sw3005</th>\n",
       "      <th>sw3010</th>\n",
       "      <th>sw3020</th>\n",
       "      <th>sw3050</th>\n",
       "      <th>sw3100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01 15:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-01 16:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-01 17:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-01 18:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-01 19:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  sw1005  sw1010  sw1020  sw1050  sw1100  sw2005  \\\n",
       "0  2018-03-01 15:00:00     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1  2018-03-01 16:00:00     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2  2018-03-01 17:00:00     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3  2018-03-01 18:00:00     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4  2018-03-01 19:00:00     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   sw2010  sw2020  sw2050  sw2100  sw3005  sw3010  sw3020  sw3050  sw3100  \n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3728\n",
      "3758\n",
      "4126\n",
      "4141\n",
      "4236\n",
      "4990\n",
      "4994\n",
      "13301\n",
      "23906\n",
      "26563\n",
      "53152\n",
      "53155\n",
      "53877\n",
      "53968\n",
      "54808\n",
      "54811\n",
      "54854\n",
      "63826\n",
      "63828\n",
      "63829\n",
      "63831\n",
      "63850\n",
      "63855\n",
      "63858\n",
      "63869\n",
      "64756\n",
      "92827\n",
      "93243\n",
      "94077\n",
      "94995\n",
      "UN\n"
     ]
    }
   ],
   "source": [
    "# Check whether anomalies were actually recorded for at least some stations.\n",
    "data_dir = \"../data/stations/\"\n",
    "for test_station_id in station_id_array:\n",
    "    test_data = pd.read_csv(data_dir+f\"data_{test_station_id}.csv\")\n",
    "    test_targets = pd.read_csv(data_dir+f\"targets_{test_station_id}.csv\")\n",
    "\n",
    "    tt = test_targets.drop(columns=[\"date\"])\n",
    "    anom_sum = (tt > 0).sum().sum()\n",
    "    if anom_sum > 0:\n",
    "        print(test_station_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's all looking pretty reasonable! Time to do some data exploration on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

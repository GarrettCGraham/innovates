{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass data preprocessing\n",
    "\n",
    "Derived from 20210405_mulitclass_SINGLE_STATION.ipynb.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from progressbar import ProgressBar\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of stations ID #s.\n",
    "data_dir = \"../data/stations/\"\n",
    "station_filenames_list = [\n",
    "    filename for filename in os.listdir(path=data_dir)\n",
    "    if filename!=\".DS_Store\"\n",
    "]\n",
    "\n",
    "# Load just the list of ACCLIMA station IDs.\n",
    "file_path = \"../data/acclima_stations_id_list.txt\"\n",
    "acclima_stations_list = pd.read_csv(file_path, header=None).iloc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load expanded array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34343059, 13)\n",
      "(34343059, 13)\n",
      "(34343058, 13)\n"
     ]
    }
   ],
   "source": [
    "# This operation takes a couple minutes, so only do it if you really need to reload the stuff.\n",
    "df_expanded = pd.read_pickle(\"../data/acclima_soil_water_rleeper_1214.pickle\")\n",
    "print(df_expanded.shape)\n",
    "\n",
    "# The previous line loads column names as values in the first row. Set them\n",
    "# as the actual column names and then delete the first row.\n",
    "df_expanded.columns = df_expanded.iloc[0].values\n",
    "print(df_expanded.shape)\n",
    "\n",
    "df_expanded = df_expanded.iloc[1:, :]\n",
    "print(df_expanded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the expanded 2020-12-14 dataset to ACCLIMA only\n",
    "\n",
    "# This takes about 1 minute to run, so only run when necessary.\n",
    "# Filter the expanded dataset down to just the ACCLIMA stations so that\n",
    "# it's easier to wield in memory.\n",
    "df_acclima = df_expanded.isin({\"WBANNO\":acclima_stations_list})\n",
    "df_acclima = df_expanded.iloc[df_acclima.WBANNO.values]\n",
    "\n",
    "# Delete df_expanded to free up some dang memory.\n",
    "del(df_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the ACCLIMA DF's TAG columns to not be \"TAGS, NaN, NaN, NaN\".\n",
    "df_acclima.columns =\\\n",
    "    df_acclima.columns[:9].tolist() + [\"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset down to a single station and create a data-prep regime that works here for reducing **from** multilabel **to** multiclass\n",
    "\n",
    "First, create a few constants and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS_SET = {\n",
    "    'Acclima-Zero', 'Acclima-Toohigh', 'Acclima-Too high', 'Acclima-NoPrcpResponse', \n",
    "    'Acclima-FrozenRecovery', 'Acclima-Noise', 'Acclima-Failure',\n",
    "    'Acclima-Spike', 'Acclima-DiurnalNoise', 'Acclima-Erratic', \n",
    "    'Acclima-Static'\n",
    "}\n",
    "\n",
    "def clean_tags_dataframe(df_targets):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes a target data frame and replaces the tags with their cleaned-up, space-less versions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy of the dataframe so we don't overwrite the original.\n",
    "    df_targets_cleaned = copy.deepcopy(df_targets)\n",
    "    \n",
    "    # Loop through all the cleaned versions of the tags and replace the original versions,\n",
    "    # which have extra whitespace pre-pended to them, with the cleaned versions.\n",
    "    for tag in TAGS_SET:\n",
    "        df_targets_cleaned.replace(\n",
    "            to_replace=\" \"+tag,\n",
    "            value=tag,\n",
    "            inplace=True,\n",
    "        )\n",
    "    \n",
    "    # Replace \"None\" tags with an empty string.\n",
    "    df_targets_cleaned.replace(\n",
    "        to_replace=[None],\n",
    "        value=[\"\"],\n",
    "        inplace=True,\n",
    "    )\n",
    "    \n",
    "    return df_targets_cleaned\n",
    "    \n",
    "    \n",
    "def rename_tags_in_df(df_targets):\n",
    "    \"\"\"\n",
    "    Replaces 'Acclima-Spike' with 'spike' and noise-related Acclima tags with 'noise'.\n",
    "    Returns a dataframe with renamed tags.\n",
    "    \"\"\"\n",
    "    df_targets_renamed = copy.deepcopy(df_targets)\n",
    "    \n",
    "    # Rename SPIKES.\n",
    "    df_targets_renamed.replace(\n",
    "        to_replace=\"Acclima-Spike\",\n",
    "        value=\"spike\",\n",
    "        inplace=True,\n",
    "    )\n",
    "    # Rename NOISE.\n",
    "    noise_tag_list = [\n",
    "        \"Acclima-Noise\",\n",
    "        \"Acclima-Diurnal Noise\", \n",
    "        \"Acclima-FrozenRecovery\", \n",
    "        \"Acclima-Erratic\",\n",
    "    ]\n",
    "    for noise_tag in noise_tag_list:\n",
    "        df_targets_renamed.replace(\n",
    "            to_replace=noise_tag,\n",
    "            value=\"noise\",\n",
    "            inplace=True,\n",
    "        )\n",
    "    return df_targets_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_dataframe(station_id_num, df_acclima):\n",
    "    # Subset down to the single ACCLIMA station of interest.\n",
    "    df_station = df_acclima[df_acclima.WBANNO.eq(station_id_num)]\n",
    "    return df_station\n",
    "\n",
    "\n",
    "def reduce_station_df_and_convert_dates(df_station):\n",
    "    # Subset down to just the columns of interest.\n",
    "    df_station =\\\n",
    "        df_station[\n",
    "            [\"UTC_START\", \"NAME\", \"VALUE\", \"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\"]\n",
    "        ]\n",
    "    # Convert all datetimes to actual datetime datatypes.\n",
    "    df_station.UTC_START = pd.to_datetime(df_station.UTC_START, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    return df_station\n",
    "\n",
    "\n",
    "def get_station_targets(df_station):\n",
    "\n",
    "    # Get just the station's targets.\n",
    "    df_station_targets = df_station[[\"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\"]]\n",
    "\n",
    "    # Clean up and then rename the targets.\n",
    "    df_station_targets =\\\n",
    "            rename_tags_in_df(\n",
    "                clean_tags_dataframe(\n",
    "                    df_station[[\"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\"]]\n",
    "                )\n",
    "            )\n",
    "    return df_station_targets\n",
    "\n",
    "\n",
    "def get_filtered_targets(df_station_targets):\n",
    "    # Isolate the last three columns of targets. \n",
    "    # If there's a row where they're non-empty, then that's indicative of a multilabel example. \n",
    "    # Our goal here is to eliminate all multilabel examples.\n",
    "    array_multilabel_targets = df_station_targets.iloc[:, 1:].values\n",
    "\n",
    "    # Iterate through the rows of multilabel targets and concatenate all tags into a single string.\n",
    "    # For rows that don't have any multilabel tag, the resulting entry will be an empty string of length 0.\n",
    "    # For multilabel rows, there will be a string with non-zero length.\n",
    "    arr_tags_concatenated = np.array([\"\".join(row) for row in array_multilabel_targets])\n",
    "\n",
    "    # Iterate through the concatenated tags and calculate their lengths. \n",
    "    # These lengths will be stored in the new array defined below.\n",
    "    arr_concattags_lengths = np.array([l for l in map(len, arr_tags_concatenated)])\n",
    "\n",
    "    # Find all zero-length elements of the array. \n",
    "    # These entries are the rows in the original targets dataframe that we want to keep,\n",
    "    # since they are the single-label (ie, non-multilabel) rows.\n",
    "    arr_tags_to_keep = (arr_concattags_lengths == 0)\n",
    "\n",
    "    # Reduce the targets dataframe to the first column.\n",
    "    # This column represents all of the single-label targets.\n",
    "    df_tags_reduced = df_station_targets[arr_tags_to_keep].iloc[:,0]\n",
    "\n",
    "    # Get the final set of targets by filtering out anything that's not a spike, noise, or normal.\n",
    "    df_station_targets_reduced_final = df_tags_reduced[df_tags_reduced.isin([\"\", \"spike\", \"noise\"])]\n",
    "    \n",
    "    return df_station_targets_reduced_final, df_tags_reduced, arr_tags_to_keep\n",
    "    \n",
    "    \n",
    "def get_filtered_features(df_station, df_station_targets_reduced_final):\n",
    "    # Get the final set of station features by using the indices of the remaining targets.\n",
    "    df_station_features = df_station.loc[df_station_targets_reduced_final.index, [\"UTC_START\", \"NAME\", \"VALUE\"]]\n",
    "    return df_station_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare targets and then features for single station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################ 15 ############################\n",
      "Original set of targets:                                          ['' 'spike']\n",
      "Original number of targets:                                       367029\n",
      "\n",
      "Remaining unique labels:                                          ['' 'spike']\n",
      "Original number of targets:                                       367029\n",
      "Number of reduced targets:                                        367029\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'arr_tags_to_keep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-57da2aaebc86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original number of targets:                                      \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_station_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of reduced targets:                                       \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_tags_reduced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of reduced targets plus number of dropped targets:        \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0marr_tags_to_keep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf_tags_reduced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Get the final feature-set by filtering to feature-rows that have labels remaining after the labels were filtered.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr_tags_to_keep' is not defined"
     ]
    }
   ],
   "source": [
    "# Select station to test.\n",
    "# station_list_idx = 14\n",
    "\n",
    "for station_list_idx in range(15, 20):\n",
    "    \n",
    "    station_id_num = acclima_stations_list[station_list_idx]\n",
    "    print(\"############################\", station_list_idx, \"############################\")\n",
    "\n",
    "    # Get just the station of interest.\n",
    "    df_station = get_station_dataframe(station_id_num, df_acclima)\n",
    "\n",
    "    # Cut the station data down to just the columns-of-interest.\n",
    "    # Convert the date-times in the UTC_START column to datetime objects.\n",
    "    df_station = reduce_station_df_and_convert_dates(df_station)\n",
    "\n",
    "    # Isolate the station targets for filtering.\n",
    "    df_station_targets = get_station_targets(df_station)\n",
    "\n",
    "    # Print sanity-check statistics.\n",
    "    print(\"Original set of targets:                                         \", np.unique(df_station_targets.values))\n",
    "    print(\"Original number of targets:                                      \", df_station_targets.shape[0])\n",
    "    \n",
    "    # Filter the targets down to single-label targets-of-interest (ie, just normal, \"spike\" and \"noise\").\n",
    "    df_station_targets_reduced_final, df_tags_reduced, arr_tags_to_keep = get_filtered_targets(df_station_targets)\n",
    "\n",
    "    # Print some sanity statistics.\n",
    "    print()\n",
    "    print(\"Remaining unique labels:                                         \", df_tags_reduced.unique())\n",
    "    print(\"Original number of targets:                                      \", df_station_targets.shape[0])\n",
    "    print(\"Number of reduced targets:                                       \", df_tags_reduced.shape[0])\n",
    "    print(\"Number of reduced targets plus number of dropped targets:        \", (~arr_tags_to_keep).sum() + df_tags_reduced.shape[0])\n",
    "\n",
    "    # Get the final feature-set by filtering to feature-rows that have labels remaining after the labels were filtered.\n",
    "    df_station_features = get_filtered_features(df_station, df_station_targets_reduced_final)\n",
    "\n",
    "    print()\n",
    "    print(\"Final set of unique labels:                                      \", df_station_targets_reduced_final.unique())\n",
    "    print(\"Number of final labels:                                          \", df_station_features.shape[0])\n",
    "    print(\"Number of final features:                                        \",df_station_targets_reduced_final.shape[0])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functionalizing the methods produces the same results as when they were non-functionalized. Whoooo!\n",
    "\n",
    "Moving onto creating pivot tables of features now, with appropriate filtering of missing values.\n",
    "\n",
    "# Pivot and further subset the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select station to test.\n",
    "station_list_idx = 24\n",
    "station_id_num = acclima_stations_list[station_list_idx]\n",
    "\n",
    "# Get just the station of interest.\n",
    "df_station = get_station_dataframe(station_id_num, df_acclima)\n",
    "\n",
    "# Cut the station data down to just the columns-of-interest.\n",
    "# Convert the date-times in the UTC_START column to datetime objects.\n",
    "df_station = reduce_station_df_and_convert_dates(df_station)\n",
    "\n",
    "# Isolate the station targets for filtering.\n",
    "df_station_targets = get_station_targets(df_station)\n",
    "\n",
    "# Filter the targets down to single-label targets-of-interest (ie, just normal, \"spike\" and \"noise\").\n",
    "df_station_targets_reduced_final, df_tags_reduced = get_filtered_targets(df_station_targets)\n",
    "\n",
    "# Get the final feature-set by filtering to feature-rows that have labels remaining after the labels were filtered.\n",
    "df_station_features = get_filtered_features(df_station, df_station_targets_reduced_final)\n",
    "\n",
    "df_station_combined = pd.concat([df_station_features, df_station_targets_reduced_final],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identified a new potential problem:\n",
    "# Even though I've eliminated multilable instances for single sensors at \n",
    "# single timepoints, there may be single timepoints where **multiple sensors**\n",
    "# have differing tags. I'll need to drop those intances from this analysis in \n",
    "# order to get to the pure single-label, multiclass case.\n",
    "\n",
    "df_pivoted_targets = df_station_combined.pivot(index=\"UTC_START\", columns=\"NAME\", values=\"TAGS_00\")\n",
    "df_pivoted_targets = df_pivoted_targets.drop([\"p_official\", \"t_official\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28491\n",
      "28491\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>NAME</th>\n",
       "      <th>sw1005</th>\n",
       "      <th>sw1010</th>\n",
       "      <th>sw1020</th>\n",
       "      <th>sw1050</th>\n",
       "      <th>sw1100</th>\n",
       "      <th>sw2005</th>\n",
       "      <th>sw2010</th>\n",
       "      <th>sw2020</th>\n",
       "      <th>sw2050</th>\n",
       "      <th>sw2100</th>\n",
       "      <th>sw3005</th>\n",
       "      <th>sw3010</th>\n",
       "      <th>sw3020</th>\n",
       "      <th>sw3050</th>\n",
       "      <th>sw3100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTC_START</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-01 00:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 01:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 02:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 03:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 04:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "NAME                sw1005 sw1010 sw1020 sw1050 sw1100 sw2005 sw2010 sw2020  \\\n",
       "UTC_START                                                                     \n",
       "2017-05-01 00:00:00                                                           \n",
       "2017-05-01 01:00:00                                                           \n",
       "2017-05-01 02:00:00                                                           \n",
       "2017-05-01 03:00:00                                                           \n",
       "2017-05-01 04:00:00                                                           \n",
       "\n",
       "NAME                sw2050 sw2100 sw3005 sw3010 sw3020 sw3050 sw3100  \n",
       "UTC_START                                                             \n",
       "2017-05-01 00:00:00                                                   \n",
       "2017-05-01 01:00:00                                                   \n",
       "2017-05-01 02:00:00                                                   \n",
       "2017-05-01 03:00:00                                                   \n",
       "2017-05-01 04:00:00                                                   "
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Figure out how many rows have a missing value. It seems like there may be a lot. Ooof.\n",
    "# print(\n",
    "#     (df_pivoted_targets.isna().values.sum(axis=1) > 0).sum()\n",
    "# )\n",
    "\n",
    "# Clean the tags up; ie, convert all NaN to ''.\n",
    "df_pivoted_targets_clean = clean_tags_dataframe(df_pivoted_targets)\n",
    "\n",
    "print(df_pivoted_targets.shape[0])\n",
    "print(df_pivoted_targets_clean.shape[0])\n",
    "\n",
    "df_pivoted_targets_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' '' '' ... 'noise' 'noise' 'noise']\n",
      "['' 'noise' 'spikespikespike' 'spikespikespikespikespike']\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all the values in each row and check for non-unique labels.\n",
    "# Ie, check for time points where there's normal/spike, etc.\n",
    "arr_tags_concatenated = np.array([\"\".join(row) for row in df_pivoted_targets_clean.values])\n",
    "\n",
    "print(arr_tags_concatenated)\n",
    "print(np.unique(arr_tags_concatenated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['', '', '', ..., '', '', ''],\n",
       "       ['', '', '', ..., '', '', ''],\n",
       "       ['', '', '', ..., '', '', ''],\n",
       "       ...,\n",
       "       ['', '', '', ..., '', '', ''],\n",
       "       ['', '', '', ..., '', '', ''],\n",
       "       ['', '', '', ..., '', '', '']], dtype=object)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play = df_pivoted_targets_clean.values\n",
    "play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[''], [''], [''], [''], [''], [''], [''], [''], [''], ['']]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use set operations to the ID unique tags in each row. Then, \n",
    "# set(np.char.array([ [\"\",\"\",\"\"], [\"\",\" Acclima-Too high\", \"\"], [\"a\",\"b\",\"c\"]])[2])\n",
    "unique_tags_by_row = [list(set(row)) for row in play]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "28491\n"
     ]
    }
   ],
   "source": [
    "# Check where there's more than one unique tag per row.\n",
    "# These ought to be the timepoints with more than one unique label.\n",
    "# These should be dropped from the analysis.\n",
    "print(\n",
    "    np.array(\n",
    "        [len(row) > 1 for row in unique_tags_by_row]\n",
    "    ).sum()\n",
    ")\n",
    "print(\n",
    "    len(unique_tags_by_row)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WELLLLL, ACTUALLY, I'm not concerned with multilabel rows that have both normal and then one tag. \n",
    "# I'm concerned with multilabel rows that have \"spike\" and \"noise\".\n",
    "# So, I'll concatenate the unique labels together. Anything over length 5 ( len(\"spike\")=5 and len(\"noise\")=5 ) will be a multilabel instance, since len(\"spikenoise\")=10.  \n",
    "\n",
    "# Find all the multilabel row locations via some string method trickery.\n",
    "multilabel_row_locations = np.array(\n",
    "    [\n",
    "        len(\n",
    "            \"\".join(row)    # Join all the unique tags in each row together; ie, \n",
    "                            # [\"\", \"noise\"] --> \"noise\", while\n",
    "                            # [\"noise\", \"spike\"] --> \"noisespike\"\n",
    "        ) > 5               # Check for anything that has length > 5. This will only occur where\n",
    "                            # \"\".join(row) --> \"spikenoise\" or \"noisespike\".\n",
    "        for row in unique_tags_by_row\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Now I need to drop the multilabel timepoints from the analysis.\n",
    "\n",
    "# Filter the pivoted targets DF of any multilabel row locations (ie, locations that are co-labeled \"spike\" and \"noise\").\n",
    "df_pivoted_targets_singlelabel =\\\n",
    "    df_pivoted_targets[~multilabel_row_locations]\n",
    "\n",
    "# Use the filtered targets dataframe's index to filter the pivoted features dataframe.\n",
    "# That way, we have only feature locations with single-label multiclass features.\n",
    "df_pivoted_features_singlelabel = df_station_features.pivot(\n",
    "    index=\"UTC_START\", columns=\"NAME\", values=\"VALUE\"            # Create a pivoted DF of the features.\n",
    ").loc[\n",
    "    df_pivoted_targets_singlelabel.index                         # Filter the pivoted features DF using the datetimes of the remaining targets.\n",
    "]\n",
    "\n",
    "# It appears that filtering by the single-label DF's index reduces the pivoted feature DF's date range. \n",
    "# So, I need to reverse-filter the single-label DF using the remaining indices from the feature DF.\n",
    "# Ugh. There's so many flipping tricky parts of this problem.\n",
    "\n",
    "# Filter the pivoted single-label targets DF using the remaining feature DF datetime indices.\n",
    "df_pivoted_targets_singlelabel =\\\n",
    "    df_pivoted_targets_singlelabel.loc[\n",
    "        df_pivoted_features_singlelabel.index\n",
    "    ]\n",
    "\n",
    "# Now both the pivoted targets and the pivoted features have the same datetime indices.\n",
    "\n",
    "# Now I need to go back and reformat the remaining targets so that they're a \n",
    "# single-column series, rather than a multi-dimensional dataframe.\n",
    "\n",
    "# Get just the remaining single-label target values.\n",
    "# I'll use these to get down to one label entry per datetime row.\n",
    "df_pivoted_targets_singlelabel = clean_tags_dataframe(df_pivoted_targets_singlelabel)\n",
    "array_pivoted_targets_singlelabel = df_pivoted_targets_singlelabel.values\n",
    "\n",
    "# Use the list(set()) trick to filter down to the unique entries in each row of the targets array.\n",
    "unique_tags_by_row = [list(set(row)) for row in array_pivoted_targets_singlelabel]\n",
    "\n",
    "# Join these unique entries together to form a single entry per target row.\n",
    "# Since each row only has either {\"\"}, {\"\", \"spike\"} or {\"\", \"noise\"}, the result will be a single label per row.\n",
    "multiclass_targets_array = np.char.array(\n",
    "    [\n",
    "        \"\".join(row)                     # Join together the unique single-labels;\n",
    "                                         # ie, [\"\",\"spike\"] --> \"spike\" and [\"\", \"noise\"] --> \"noise\".\n",
    "        for row in unique_tags_by_row\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Recombine the newly-filtered multiclass targets with their original datetime index.\n",
    "df_targets_singlelabel = pd.DataFrame(index=df_pivoted_targets_singlelabel.index, columns=[\"TAGS\"], data=multiclass_targets_array,)\n",
    "\n",
    "# Drop any and all feature rows that have NaN values.\n",
    "df_pivoted_features_singlelabel = df_pivoted_features_singlelabel.dropna(how=\"any\", axis=0)\n",
    "\n",
    "# Filter the targets based on the remaining datetime\n",
    "# indices from the NaN-filtered pivoted features.\n",
    "df_targets_singlelabel =\\\n",
    "    df_targets_singlelabel.loc[\n",
    "        df_pivoted_features_singlelabel.index\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOM. DONE! WHOOOHOOOOOOO! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass SANITY CHECK\n",
    "\n",
    "This notebook is for me to figure out where the heck I'm going wrong with breaking down the multiclass stuff labeling.\n",
    "\n",
    "The problem was discovered when I tried to do a 1-to-1 comparison of the binary labels to the multiclass labels. If a multiclass label was present, it ought to have been matched by a '1' in the binary label set. This was not the case.\n",
    "\n",
    "I circled back and parsed apart the binary label treatment and am fairly confident that I didn't make an error there.\n",
    "\n",
    "Thus, the error must reside with how I preprocessed the multiclass labels. \n",
    "\n",
    "Hopefully, I can identify the source of the errors in this notebook. \n",
    "\n",
    "This notebook was derived from _20201229_multiclass.ipynb_.\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# FINISHED!\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from progressbar import ProgressBar\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of stations ID #s.\n",
    "data_dir = \"../data/stations/\"\n",
    "station_filenames_list = [\n",
    "    filename for filename in os.listdir(path=data_dir)\n",
    "    if filename!=\".DS_Store\"\n",
    "]\n",
    "\n",
    "# Load just the list of ACCLIMA station IDs.\n",
    "file_path = \"../data/acclima_stations_id_list.txt\"\n",
    "acclima_stations_list = pd.read_csv(file_path, header=None).iloc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load expanded array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34343059, 13)\n",
      "(34343059, 13)\n",
      "(34343058, 13)\n"
     ]
    }
   ],
   "source": [
    "# This operation takes a couple minutes, so only do it if you really need to reload the stuff.\n",
    "df_expanded = pd.read_pickle(\"../data/acclima_soil_water_rleeper_1214.pickle\")\n",
    "print(df_expanded.shape)\n",
    "\n",
    "# The previous line loads column names as values in the first row. Set them\n",
    "# as the actual column names and then delete the first row.\n",
    "df_expanded.columns = df_expanded.iloc[0].values\n",
    "print(df_expanded.shape)\n",
    "\n",
    "df_expanded = df_expanded.iloc[1:, :]\n",
    "print(df_expanded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks out thus far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset the expanded 2020-12-14 dataset to ACCLIMA only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes about 1 minute to run, so only run when necessary.\n",
    "# Filter the expanded dataset down to just the ACCLIMA stations so that\n",
    "# it's easier to wield in memory.\n",
    "df_acclima = df_expanded.isin({\"WBANNO\":acclima_stations_list})\n",
    "df_acclima = df_expanded.iloc[df_acclima.WBANNO.values]\n",
    "\n",
    "# Delete df_expanded to free up some dang memory.\n",
    "del(df_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14641744, 13)\n"
     ]
    }
   ],
   "source": [
    "print(df_acclima.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19701315\n"
     ]
    }
   ],
   "source": [
    "print(34343059 - 14641744)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, checks out. Successfully subsetted to ACCLIMA sensor stations only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify all unique anomaly tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the 'TAGS' columns of the expanded dataframe and get an array of all\n",
    "# the unique tags in each column. Some of these tag strings will be unique only b/c\n",
    "# they contain whitespace, which will be dealt with later.\n",
    "unq_tags_list = list()\n",
    "for col_idx in range(9, df_acclima.shape[1]):\n",
    "    unq_tags_list.append(\n",
    "        df_acclima.iloc[1:, col_idx].unique()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WBANNO</th>\n",
       "      <th>UTC_START</th>\n",
       "      <th>NAME</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>ACCLIMA</th>\n",
       "      <th>RANGE_FLAG</th>\n",
       "      <th>DOOR_FLAG</th>\n",
       "      <th>FROZEN_FLAG</th>\n",
       "      <th>MANUAL_FLAG</th>\n",
       "      <th>TAGS</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740067</th>\n",
       "      <td>03054</td>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740068</th>\n",
       "      <td>03054</td>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>16.491</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740069</th>\n",
       "      <td>03054</td>\n",
       "      <td>2017-05-01 01:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740070</th>\n",
       "      <td>03054</td>\n",
       "      <td>2017-05-01 01:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>12.662</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740071</th>\n",
       "      <td>03054</td>\n",
       "      <td>2017-05-01 02:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       WBANNO            UTC_START        NAME   VALUE ACCLIMA RANGE_FLAG  \\\n",
       "740067  03054  2017-05-01 00:00:00  p_official       0                  0   \n",
       "740068  03054  2017-05-01 00:00:00  t_official  16.491                  0   \n",
       "740069  03054  2017-05-01 01:00:00  p_official       0                  0   \n",
       "740070  03054  2017-05-01 01:00:00  t_official  12.662                  0   \n",
       "740071  03054  2017-05-01 02:00:00  p_official       0                  0   \n",
       "\n",
       "       DOOR_FLAG FROZEN_FLAG MANUAL_FLAG TAGS   NaN   NaN   NaN  \n",
       "740067         0           0           0       None  None  None  \n",
       "740068         0           0           0       None  None  None  \n",
       "740069         0           0           0       None  None  None  \n",
       "740070         0           0           0       None  None  None  \n",
       "740071         0           0           0       None  None  None  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acclima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['', 'Acclima-Too high', 'Acclima-Spike', 'Acclima-Zero',\n",
      "       'Acclima-Failure', 'Acclima-Erratic', 'Acclima-Noise',\n",
      "       'Acclima-NoPrcpResponse', 'Acclima-Diurnal Noise',\n",
      "       'Acclima-FrozenRecovery'], dtype=object), array([None, ' Acclima-Zero', ' Acclima-Too high', ' Acclima-Spike',\n",
      "       ' Acclima-Static', ' Acclima-Erratic', ' Acclima-FrozenRecovery'],\n",
      "      dtype=object), array([None, ' Acclima-Zero'], dtype=object), array([None, ' Acclima-Zero'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "print(unq_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' 'Acclima-Too high' 'Acclima-Spike' 'Acclima-Zero' 'Acclima-Failure'\n",
      " 'Acclima-Erratic' 'Acclima-Noise' 'Acclima-NoPrcpResponse'\n",
      " 'Acclima-Diurnal Noise' 'Acclima-FrozenRecovery' None ' Acclima-Zero'\n",
      " ' Acclima-Too high' ' Acclima-Spike' ' Acclima-Static' ' Acclima-Erratic'\n",
      " ' Acclima-FrozenRecovery' None ' Acclima-Zero' None ' Acclima-Zero']\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all arrays of unique tags to a 1-D numpy.array.\n",
    "unq_tags_array = np.concatenate([array for array in unq_tags_list])\n",
    "print(unq_tags_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Acclima-Zero', 'Acclima-FrozenRecovery', 'Acclima-Static', 'Acclima-NoPrcpResponse', 'Acclima-DiurnalNoise', 'Acclima-Spike', 'Acclima-Toohigh', 'Acclima-Noise', 'Acclima-Failure', 'Acclima-Erratic'}\n"
     ]
    }
   ],
   "source": [
    "# Convert list to a set to eliminate redundant tags.\n",
    "unq_tags_cleaned = set( # Converting to a set eliminates redundant tag values.\n",
    "    [\n",
    "        tag.replace(\" \", \"\") # Get rid of whitespace in otherwise identical tag strings.\n",
    "        for tag in unq_tags_array.tolist()  # Iterate through the 'unique' tags.\n",
    "        if tag not in [None, \"\"] # Keep the tag only if it's not any of those values.\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(unq_tags_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still checking out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert unique tags into multiclass targets dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TAGS</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740067</th>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740068</th>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740069</th>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740070</th>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740071</th>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TAGS   NaN   NaN   NaN\n",
       "740067       None  None  None\n",
       "740068       None  None  None\n",
       "740069       None  None  None\n",
       "740070       None  None  None\n",
       "740071       None  None  None"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isolate just the columns containing the categorical multiclass targets.\n",
    "df_targets_acclima = df_acclima.iloc[:, 9:]\n",
    "\n",
    "df_targets_acclima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manually iterate through the redundant targets and replace them, \n",
    "# # thereby cleaning up the dataframe.\n",
    "df_targets_cleaned = df_targets_acclima.copy()\n",
    "\n",
    "# df_targets_cleaned = df_targets_acclima.replace(\n",
    "#     to_replace=\" Acclima-Erratic\",\n",
    "#     value=\"Acclima-Erratic\",\n",
    "#     inplace=False,\n",
    "# )\n",
    "# df_targets_cleaned.replace(\n",
    "#     to_replace=\" Acclima-FrozenRecovery\",\n",
    "#     value=\"Acclima-FrozenRecovery\",\n",
    "#     inplace=True,\n",
    "# )\n",
    "# df_targets_cleaned.replace(\n",
    "#     to_replace=\" Acclima-Spike\",\n",
    "#     value=\"Acclima-Spike\",\n",
    "#     inplace=True,\n",
    "# )\n",
    "# df_targets_cleaned.replace(\n",
    "#     to_replace=\" Acclima-Static\",\n",
    "#     value=\"Acclima-Static\",\n",
    "#     inplace=True,\n",
    "# )\n",
    "# df_targets_cleaned.replace(\n",
    "#     to_replace=\" Acclima-Too high\",\n",
    "#     value=\"Acclima-Too high\",\n",
    "#     inplace=True,\n",
    "# )\n",
    "# df_targets_cleaned.replace(\n",
    "#     to_replace=\" Acclima-Zero\",\n",
    "#     value=\"Acclima-Zero\",\n",
    "#     inplace=True,\n",
    "# )\n",
    "\n",
    "# df_targets_cleaned.replace(\n",
    "#     to_replace=[\"\"],\n",
    "#     value=[None],\n",
    "#     inplace=True,\n",
    "# )\n",
    "\n",
    "# df_targets_cleaned.replace(\n",
    "#     to_replace=[None],\n",
    "#     value=[\"\"],\n",
    "#     inplace=True,\n",
    "# )\n",
    "\n",
    "# Rename the column values to be integers. Otherwise, it's just TAGS\n",
    "# and then NaN ad nauseum.\n",
    "df_targets_cleaned.columns = [i for i in range(df_targets_cleaned.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a sanity check to ensure that we really did clean up whitespace-redundant\n",
    "# tags. Iterate through all values and add unique tags to a list.\n",
    "tags_check_list = list()\n",
    "for col_idx in range(0, df_targets_cleaned.shape[1]):\n",
    "    tags_check_list.append(df_targets_cleaned.iloc[:, col_idx].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all arrays of unique tags to a 1-D numpy.array.\n",
    "tags_check_array = np.concatenate([array for array in tags_check_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'', 'Acclima-Failure', 'Acclima-Diurnal Noise', 'Acclima-FrozenRecovery', 'Acclima-Erratic', 'Acclima-Spike', 'Acclima-Static', 'Acclima-Zero', 'Acclima-Too high', 'Acclima-Noise', 'Acclima-NoPrcpResponse'}\n"
     ]
    }
   ],
   "source": [
    "# Turn that array into a list and then into a set in order to check for unique tags.\n",
    "print(set(tags_check_array.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14641744, 4)\n",
      "(14641744, 13)\n"
     ]
    }
   ],
   "source": [
    "# # Formerly useful for molding the targets dataframe into a numpy.array that was\n",
    "# # useful for scikit-learn's MultiLabelBinarizer class, but that's no longer necessary\n",
    "# # with some of the reworking that I did in the preceding cells.\n",
    "# # targets_cleaned_array = np.concatenate(\n",
    "#     [\n",
    "#         np.reshape(\n",
    "#             df_targets_cleaned[idx].values,\n",
    "#             (df_targets_cleaned.shape[0], 1)\n",
    "#         )\n",
    "#         for idx in range(df_targets_cleaned.shape[1])\n",
    "#     ],\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# Do a sanity check that all dimensions have been preserved as expected\n",
    "# (ie, that we didn't lose any rows of station data along the way).\n",
    "print(df_targets_cleaned.values.shape)\n",
    "print(df_acclima.shape)\n",
    "# print(targets_cleaned_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode the multiclass labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' 'Acclima-Diurnal Noise' 'Acclima-Erratic' 'Acclima-Failure'\n",
      " 'Acclima-FrozenRecovery' 'Acclima-NoPrcpResponse' 'Acclima-Noise'\n",
      " 'Acclima-Spike' 'Acclima-Static' 'Acclima-Too high' 'Acclima-Zero']\n"
     ]
    }
   ],
   "source": [
    "# Turn all categorical class labels into binary (yes/no) labels.\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "targets_multilabel_binarized = multilabel_binarizer.fit_transform(df_targets_cleaned.values, )\n",
    "print(multilabel_binarizer.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the '' label with \"Normal\" and then remove the \"Acclima-\" prefix\n",
    "# from the remaining class names.\n",
    "binarized_class_names_list =\\\n",
    "    [\"Normal\"] + [tag.split(\"-\")[1] for tag in multilabel_binarizer.classes_[1:].tolist()]\n",
    "\n",
    "# Load the binary-encoded class labels into a pandas.DataFrame so that I can\n",
    "# use pandas' methods on it.\n",
    "df_targs_binarized = pd.DataFrame(\n",
    "    data=targets_multilabel_binarized,\n",
    "    columns=binarized_class_names_list,\n",
    "#     index=df_trimmed.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erroneous normal labels attached to all anomaly instances with 3 or less anomaly labels\n",
    "\n",
    "This is b/c the label binarizer above interprets None/'' as the \"Normal\" class (ie, the data that we want). \n",
    "\n",
    "Since no single data point has more than 4 anomaly labels at once, _the df_targets_cleaned_ dataframe has 4 columns for the different labels.\n",
    "\n",
    "In _df_targets_cleaned_, normal data points have blank rows. When converted to a numpy.array, those normal data points have rows of blank strings; ie, _['', '', '', '']_.\n",
    "\n",
    "The multilabel binarizer converts these rows to having the tag \"Normal\". \n",
    "\n",
    "However, any anomalous data that have less than 4 anomaly labels attached to them will also have blank strings in their rows. Consider the following three examples: _['Acclima-Spike', '', '', '']_, _['Acclima-NoPrcpResponse', 'Acclima-Too high', '', '']_, and _['Acclima-Erratic', 'Acclima-Spike', 'Acclima-Erratic', '']_ (I just made these three up; I don't know if they're actually represented in the data, but they're only to illustrate what's happening with the multilabel binarizer).\n",
    "\n",
    "What happens in the case of anomalous data is that sklearn's MultilabelBinarizer interprets these as having a \"Normal\" label attached to them. So, in the _df_targs_binarized_ dataframe, they all get labeled as both \"Normal\" and anomalous. \n",
    "\n",
    "I demonstrate this in the next few cells, as a sanity check that I haven't mangled the data via my preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of labels per row via summing the binarized labels row-wise;\n",
    "# ie, if a row has three anomaly labels, it ought to just have a row sum of 3.\n",
    "# Butttttt, we know this isn't the case b/c \n",
    "num_binarized_labels = df_targs_binarized.values.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Normal</th>\n",
       "      <th>Diurnal Noise</th>\n",
       "      <th>Erratic</th>\n",
       "      <th>Failure</th>\n",
       "      <th>FrozenRecovery</th>\n",
       "      <th>NoPrcpResponse</th>\n",
       "      <th>Noise</th>\n",
       "      <th>Spike</th>\n",
       "      <th>Static</th>\n",
       "      <th>Too high</th>\n",
       "      <th>Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Normal, Diurnal Noise, Erratic, Failure, FrozenRecovery, NoPrcpResponse, Noise, Spike, Static, Too high, Zero]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targs_binarized.iloc[num_binarized_labels > 4].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the previous dataframe is empty. That implies that the binarizer didn't tack a \"Normal\" label on to data points with 4 anomaly labels. This is in keeping with our hypothesis of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal            14383585\n",
      "Diurnal Noise            0\n",
      "Erratic                  0\n",
      "Failure                  0\n",
      "FrozenRecovery           0\n",
      "NoPrcpResponse           0\n",
      "Noise                    0\n",
      "Spike                    0\n",
      "Static                   0\n",
      "Too high                 0\n",
      "Zero                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find all the rows with a single value (ie, their row sum equals 1; ie, they only have one label).\n",
    "# B/c the multilabelbinarizer interprets '' as \"Normal\", only the data points with blank labels\n",
    "# should be listed. That would imply that any anomalous data point had a \"Normal\" label attached to it.\n",
    "idx_list_single_valued_rows = df_targs_binarized.iloc[num_binarized_labels == 1].index\n",
    "print(df_targs_binarized.loc[idx_list_single_valued_rows].sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, there are no anomalous data points with only a single tag, which we know to be an error. An early inspection of the data before binarization shows there to be many examples of single anomaly labels. \n",
    "\n",
    "I'll proceed to correct the problem in the following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set all \"Normal\" values to 0 if other tags are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "# Takes a minute or two to run, so only rerun when needed. Find all locations \n",
    "# where \"Normal\" is indicated, but do have other flags present. This is going\n",
    "# to be pretty much all of the anomalous data points b/c of the way in which \n",
    "# I had to preprocess the flags.\n",
    "erroneous_normal_tags = df_targs_binarized[df_targs_binarized.Normal == 1]\n",
    "erroneous_normal_tags = erroneous_normal_tags[erroneous_normal_tags.sum(axis=1) > 1]\n",
    "\n",
    "# Print the number of tagged data points without a \"Normal\" label. Based on how I did the pre-processing,\n",
    "# this ought to be nearly none of the rows, except the ones with three-or-four-label Acclima anomaly flags. \n",
    "print(df_targs_binarized.shape[0] - df_targs_binarized[df_targs_binarized.Normal == 1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26773018</th>\n",
       "      <td>Acclima-Erratic</td>\n",
       "      <td>Acclima-Spike</td>\n",
       "      <td>Acclima-Zero</td>\n",
       "      <td>Acclima-Zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26773034</th>\n",
       "      <td>Acclima-Erratic</td>\n",
       "      <td>Acclima-Spike</td>\n",
       "      <td>Acclima-Zero</td>\n",
       "      <td>Acclima-Zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26773035</th>\n",
       "      <td>Acclima-Erratic</td>\n",
       "      <td>Acclima-Spike</td>\n",
       "      <td>Acclima-Zero</td>\n",
       "      <td>Acclima-Zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26773051</th>\n",
       "      <td>Acclima-Erratic</td>\n",
       "      <td>Acclima-Spike</td>\n",
       "      <td>Acclima-Zero</td>\n",
       "      <td>Acclima-Zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26773052</th>\n",
       "      <td>Acclima-Erratic</td>\n",
       "      <td>Acclima-Spike</td>\n",
       "      <td>Acclima-Zero</td>\n",
       "      <td>Acclima-Zero</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0              1             2             3\n",
       "26773018  Acclima-Erratic  Acclima-Spike  Acclima-Zero  Acclima-Zero\n",
       "26773034  Acclima-Erratic  Acclima-Spike  Acclima-Zero  Acclima-Zero\n",
       "26773035  Acclima-Erratic  Acclima-Spike  Acclima-Zero  Acclima-Zero\n",
       "26773051  Acclima-Erratic  Acclima-Spike  Acclima-Zero  Acclima-Zero\n",
       "26773052  Acclima-Erratic  Acclima-Spike  Acclima-Zero  Acclima-Zero"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what's different about these 37 rows by manually inspecting the data.\n",
    "idx_of_rows_without_Normal_labels = df_targs_binarized[df_targs_binarized.Normal != 1].index\n",
    "df_targets_cleaned.iloc[idx_of_rows_without_Normal_labels].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what we would expect! All of the original data points have 4 labels apiece. \n",
    "\n",
    "Since some of those labels are redundant, we end up with some of the binarized versions of those rows only having 3 binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's correct this problem by manually setting all of the \"Normal\" labels in\n",
    "# these rows to equal 0.\n",
    "idx_erroneous_normal_tags = erroneous_normal_tags.index\n",
    "df_targs_binarized.iloc[idx_erroneous_normal_tags, 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Normal</th>\n",
       "      <th>Diurnal Noise</th>\n",
       "      <th>Erratic</th>\n",
       "      <th>Failure</th>\n",
       "      <th>FrozenRecovery</th>\n",
       "      <th>NoPrcpResponse</th>\n",
       "      <th>Noise</th>\n",
       "      <th>Spike</th>\n",
       "      <th>Static</th>\n",
       "      <th>Too high</th>\n",
       "      <th>Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Normal, Diurnal Noise, Erratic, Failure, FrozenRecovery, NoPrcpResponse, Noise, Spike, Static, Too high, Zero]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's run a sanity check by checking that (1) all rows with \"Normal\" labels now have\n",
    "# have only one tag per row and (2) that all rows have at least one label. We'll check\n",
    "# the first condition in this cell.\n",
    "sanity_check_erroneous_normal_tags = df_targs_binarized[(df_targs_binarized.Normal == 1)]\n",
    "sanity_check_erroneous_normal_tags = sanity_check_erroneous_normal_tags[(sanity_check_erroneous_normal_tags.sum(axis=1) > 1)]\n",
    "sanity_check_erroneous_normal_tags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _sanity_check_erroneous_normal_tags_ dataframe is empty! Heck yeah! \n",
    "\n",
    "Now to check the second condition: that all rows have at least one tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Normal</th>\n",
       "      <th>Diurnal Noise</th>\n",
       "      <th>Erratic</th>\n",
       "      <th>Failure</th>\n",
       "      <th>FrozenRecovery</th>\n",
       "      <th>NoPrcpResponse</th>\n",
       "      <th>Noise</th>\n",
       "      <th>Spike</th>\n",
       "      <th>Static</th>\n",
       "      <th>Too high</th>\n",
       "      <th>Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Normal, Diurnal Noise, Erratic, Failure, FrozenRecovery, NoPrcpResponse, Noise, Spike, Static, Too high, Zero]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targs_binarized[df_targs_binarized.sum(axis=1) == 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! The dataframe is empty. So, all of the \"Normal\" data don't have another label associated with them and all other data points have at least one anomaly label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# WORKING!\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalize preprocessing and then cache the data and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garrettgraham/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (2,11,12,13,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/garrettgraham/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# df_acclima_feats_targs_binarized = pd.concat(\n",
    "#     [df_acclima.reset_index(), df_targs_binarized],\n",
    "#     axis=1,\n",
    "# )\n",
    "\n",
    "filepath_csv =\\\n",
    "    \"../data/multiclass/df_acclima_feats_targs_binarized_20210216.csv\"\n",
    "\n",
    "# # This takes a COUPLE MINUTES TO RUN. So, only re-run if necessary!!\n",
    "# df_acclima_feats_targs_binarized.to_csv(filepath_csv)\n",
    "\n",
    "df_acclima_feats_targs_binarized = pd.read_csv(filepath_csv, index_col=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>WBANNO</th>\n",
       "      <th>UTC_START</th>\n",
       "      <th>NAME</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>ACCLIMA</th>\n",
       "      <th>RANGE_FLAG</th>\n",
       "      <th>DOOR_FLAG</th>\n",
       "      <th>FROZEN_FLAG</th>\n",
       "      <th>MANUAL_FLAG</th>\n",
       "      <th>...</th>\n",
       "      <th>Diurnal Noise</th>\n",
       "      <th>Erratic</th>\n",
       "      <th>Failure</th>\n",
       "      <th>FrozenRecovery</th>\n",
       "      <th>NoPrcpResponse</th>\n",
       "      <th>Noise</th>\n",
       "      <th>Spike</th>\n",
       "      <th>Static</th>\n",
       "      <th>Too high</th>\n",
       "      <th>Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>740067</td>\n",
       "      <td>3054</td>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>740068</td>\n",
       "      <td>3054</td>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>16.491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>740069</td>\n",
       "      <td>3054</td>\n",
       "      <td>2017-05-01 01:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>740070</td>\n",
       "      <td>3054</td>\n",
       "      <td>2017-05-01 01:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>12.662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>740071</td>\n",
       "      <td>3054</td>\n",
       "      <td>2017-05-01 02:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index WBANNO            UTC_START        NAME   VALUE  ACCLIMA  \\\n",
       "0  740067   3054  2017-05-01 00:00:00  p_official   0.000      NaN   \n",
       "1  740068   3054  2017-05-01 00:00:00  t_official  16.491      NaN   \n",
       "2  740069   3054  2017-05-01 01:00:00  p_official   0.000      NaN   \n",
       "3  740070   3054  2017-05-01 01:00:00  t_official  12.662      NaN   \n",
       "4  740071   3054  2017-05-01 02:00:00  p_official   0.000      NaN   \n",
       "\n",
       "   RANGE_FLAG  DOOR_FLAG  FROZEN_FLAG  MANUAL_FLAG  ... Diurnal Noise Erratic  \\\n",
       "0           0          0            0            0  ...             0       0   \n",
       "1           0          0            0            0  ...             0       0   \n",
       "2           0          0            0            0  ...             0       0   \n",
       "3           0          0            0            0  ...             0       0   \n",
       "4           0          0            0            0  ...             0       0   \n",
       "\n",
       "  Failure FrozenRecovery  NoPrcpResponse  Noise  Spike  Static  Too high  Zero  \n",
       "0       0              0               0      0      0       0         0     0  \n",
       "1       0              0               0      0      0       0         0     0  \n",
       "2       0              0               0      0      0       0         0     0  \n",
       "3       0              0               0      0      0       0         0     0  \n",
       "4       0              0               0      0      0       0         0     0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acclima_feats_targs_binarized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Normal</th>\n",
       "      <th>Diurnal Noise</th>\n",
       "      <th>Erratic</th>\n",
       "      <th>Failure</th>\n",
       "      <th>FrozenRecovery</th>\n",
       "      <th>NoPrcpResponse</th>\n",
       "      <th>Noise</th>\n",
       "      <th>Spike</th>\n",
       "      <th>Static</th>\n",
       "      <th>Too high</th>\n",
       "      <th>Zero</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTC_START</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-03 19:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-03 20:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-03 21:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-03 22:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-03 23:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16 09:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16 10:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16 11:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16 12:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16 13:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16410 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Normal  Diurnal Noise  Erratic  Failure  FrozenRecovery  \\\n",
       "UTC_START                                                                      \n",
       "2017-05-03 19:00:00       1              0        0        0               0   \n",
       "2017-05-03 20:00:00       1              0        0        0               0   \n",
       "2017-05-03 21:00:00       1              0        0        0               0   \n",
       "2017-05-03 22:00:00       1              0        0        0               0   \n",
       "2017-05-03 23:00:00       1              0        0        0               0   \n",
       "...                     ...            ...      ...      ...             ...   \n",
       "2020-03-16 09:00:00       1              0        0        0               0   \n",
       "2020-03-16 10:00:00       1              0        0        0               0   \n",
       "2020-03-16 11:00:00       1              0        0        0               0   \n",
       "2020-03-16 12:00:00       1              0        0        0               0   \n",
       "2020-03-16 13:00:00       1              0        0        0               0   \n",
       "\n",
       "                     NoPrcpResponse  Noise  Spike  Static  Too high  Zero  \n",
       "UTC_START                                                                  \n",
       "2017-05-03 19:00:00               0      0      0       0         0     0  \n",
       "2017-05-03 20:00:00               0      0      0       0         0     0  \n",
       "2017-05-03 21:00:00               0      0      0       0         0     0  \n",
       "2017-05-03 22:00:00               0      0      0       0         0     0  \n",
       "2017-05-03 23:00:00               0      0      0       0         0     0  \n",
       "...                             ...    ...    ...     ...       ...   ...  \n",
       "2020-03-16 09:00:00               0      0      0       0         0     0  \n",
       "2020-03-16 10:00:00               0      0      0       0         0     0  \n",
       "2020-03-16 11:00:00               0      0      0       0         0     0  \n",
       "2020-03-16 12:00:00               0      0      0       0         0     0  \n",
       "2020-03-16 13:00:00               0      0      0       0         0     0  \n",
       "\n",
       "[16410 rows x 11 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_station_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check: see how many anomalies there are per station.\n",
    "\n",
    "These should be on the same order as the numbers from the binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for station_id in acclima_stations_list:\n",
    "#     print(\"Station\", station_id)\n",
    "    \n",
    "#     # Subset the giant features/targets dataframe down to just that station.\n",
    "#     df_station =\\\n",
    "#         df_acclima_feats_targs_binarized[\n",
    "#             df_acclima_feats_targs_binarized.WBANNO == int(station_id)\n",
    "#         ]\n",
    "#     print(df_station.iloc[:, 14:].sum(axis=0))\n",
    "#     print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, preprocess the features \n",
    "\n",
    "### Prototype the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_station.columns[14:])\n",
    "# # >> Index(['Normal', 'Diurnal Noise', 'Erratic', 'Failure', 'FrozenRecovery',\n",
    "# #        'NoPrcpResponse', 'Noise', 'Spike', 'Static', 'Too high', 'Zero'],\n",
    "# #       dtype='object')\n",
    "\n",
    "columns_to_keep_for_targs =\\\n",
    "    [\n",
    "        'UTC_START', 'Normal', 'Diurnal Noise', 'Erratic',\n",
    "        'Failure', 'FrozenRecovery', 'NoPrcpResponse', 'Noise',\n",
    "        'Spike', 'Static', 'Too high', 'Zero'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ACCLIMA station ID #.\n",
    "# station_id = acclima_stations_list[0]\n",
    "station_id = int(\"04127\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the giant features/targets dataframe down to just that station.\n",
    "df_station = df_acclima_feats_targs_binarized[df_acclima_feats_targs_binarized.WBANNO == station_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garrettgraham/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# Convert all values in \"date\" column to datetime objects.\n",
    "df_station.loc[:, \"UTC_START\"] = pd.to_datetime(df_station.loc[:, \"UTC_START\"], format='%Y-%m-%d %H:%M:%S',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the station's data and target values into ML-compatible arrays. \n",
    "df_station_data =\\\n",
    "    df_station.pivot(index=\"UTC_START\", columns=\"NAME\", values=\"VALUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station_times_unique =\\\n",
    "    df_station.UTC_START.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28493, 17)\n",
      "(28493,)\n"
     ]
    }
   ],
   "source": [
    "print(df_station_data.shape)\n",
    "print(df_station_times_unique.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_targets = pd.DataFrame(index=df_station_times_unique, columns=columns_to_keep_for_targs[1:])\n",
    "df_unique_targets.index.name = \"UTC_START\"\n",
    "\n",
    "for time_of_interest in df_station_times_unique:\n",
    "    time_OI_targets = df_station[df_station.UTC_START == time_of_interest].iloc[:, 14:]\n",
    "    df_unique_targets.loc[time_of_interest] = time_OI_targets.values.any(axis=0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28493, 11)\n"
     ]
    }
   ],
   "source": [
    "df_unique_targets = df_unique_targets.astype(int)\n",
    "print(df_unique_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop the station data/targets caching process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep_for_targs =\\\n",
    "    [\n",
    "        'Normal', 'Diurnal Noise', 'Erratic',\n",
    "        'Failure', 'FrozenRecovery', 'NoPrcpResponse', 'Noise',\n",
    "        'Spike', 'Static', 'Too high', 'Zero'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% |                                                                        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station UN\n",
      "(16410, 17)\n",
      "(22293,)\n",
      "(16410, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/multiclass/stations/\"\n",
    "\n",
    "pbar = ProgressBar()\n",
    "\n",
    "# Get the ACCLIMA station ID #.\n",
    "for station_id in pbar(acclima_stations_list[-1:]):\n",
    "    \n",
    "    print(\"Station\", station_id)\n",
    "    \n",
    "    if station_id != \"UN\":\n",
    "        station_id = int(station_id)\n",
    "    \n",
    "    # Subset the giant features/targets dataframe down to just that station.\n",
    "    df_station = df_acclima_feats_targs_binarized[df_acclima_feats_targs_binarized.WBANNO == station_id]\n",
    "\n",
    "    # Convert all values in \"date\" column to datetime objects.\n",
    "    df_station.loc[:, \"UTC_START\"] = pd.to_datetime(df_station.loc[:, \"UTC_START\"], format='%Y-%m-%d %H:%M:%S',)\n",
    "\n",
    "    # Pivot the station's data and target values into ML-compatible arrays. \n",
    "    df_station_data =\\\n",
    "        df_station.pivot(index=\"UTC_START\", columns=\"NAME\", values=\"VALUE\")\n",
    "\n",
    "    df_station_times_unique =\\\n",
    "        df_station.UTC_START.unique()\n",
    "\n",
    "    df_unique_targets = pd.DataFrame(index=df_station_times_unique, columns=columns_to_keep_for_targs[1:])\n",
    "    df_unique_targets.index.name = \"UTC_START\"\n",
    "\n",
    "    for time_of_interest in df_station_times_unique:\n",
    "        time_OI_targets = df_station[df_station.UTC_START == time_of_interest].iloc[:, 14:]\n",
    "        df_unique_targets.loc[time_of_interest] = time_OI_targets.values.any(axis=0).astype(int)\n",
    "\n",
    "    df_station_targets = df_unique_targets.astype(int)\n",
    "\n",
    "    df_station_data = df_station_data.dropna(how=\"any\")\n",
    "    df_station_targets = df_station_targets.loc[df_station_data.index]\n",
    "    \n",
    "    print(df_station_data.shape)\n",
    "    print(df_station_times_unique.shape)\n",
    "    print(df_station_targets.shape)\n",
    "    \n",
    "    # Save the station's data and labels as a CSV\n",
    "    df_station_data.to_csv(data_dir+\"data_\"+str(station_id)+\".csv\")\n",
    "    df_station_targets.to_csv(data_dir+\"targets_\"+str(station_id)+\".csv\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload the cached features/targets and compare to the cached targets for the binary cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features and targets into features and targets dictionaries, indexed\n",
    "# by station ID number.\n",
    "# temp_station_id_list = [\n",
    "#     int(station_id) \n",
    "#     if station_id != \"UN\" else station_id\n",
    "#     for station_id in acclima_stations_list\n",
    "# ]\n",
    "mclass_targets_dict = {\n",
    "    station_id : None\n",
    "    for station_id in acclima_stations_list \n",
    "}\n",
    "\n",
    "for station_id in acclima_stations_list:\n",
    "    \n",
    "    temp_targs_df = pd.read_csv(\n",
    "        data_dir+\"targets_\"+station_id+\".csv\",\n",
    "        index_col=\"UTC_START\"\n",
    "    )\n",
    "    temp_targs_df.index = pd.to_datetime(temp_targs_df.index, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    mclass_targets_dict[station_id] = temp_targs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_targets_dict = {\n",
    "    station_id : None\n",
    "    for station_id in acclima_stations_list \n",
    "}\n",
    "\n",
    "binary_data_dir = \"../data/stations/\"\n",
    "\n",
    "for station_id in acclima_stations_list:\n",
    "    \n",
    "    temp_targs_df = pd.read_csv(\n",
    "        binary_data_dir+\"targets_\"+station_id+\".csv\",\n",
    "        index_col=\"date\"\n",
    "    )\n",
    "    temp_targs_df.index = pd.to_datetime(temp_targs_df.index, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    binary_targets_dict[station_id] = temp_targs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare multiclass targs to binary targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sw1005</th>\n",
       "      <th>sw1010</th>\n",
       "      <th>sw1020</th>\n",
       "      <th>sw1050</th>\n",
       "      <th>sw1100</th>\n",
       "      <th>sw2005</th>\n",
       "      <th>sw2010</th>\n",
       "      <th>sw2020</th>\n",
       "      <th>sw2050</th>\n",
       "      <th>sw2100</th>\n",
       "      <th>sw3005</th>\n",
       "      <th>sw3010</th>\n",
       "      <th>sw3020</th>\n",
       "      <th>sw3050</th>\n",
       "      <th>sw3100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-03-05 19:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-05 20:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-05 21:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-05 22:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-05 23:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 19:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 20:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 21:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 22:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 23:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21084 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sw1005  sw1010  sw1020  sw1050  sw1100  sw2005  sw2010  \\\n",
       "date                                                                          \n",
       "2018-03-05 19:00:00       0       0       0       0       0       0       0   \n",
       "2018-03-05 20:00:00       0       0       0       0       0       0       0   \n",
       "2018-03-05 21:00:00       0       0       0       0       0       0       0   \n",
       "2018-03-05 22:00:00       0       0       0       0       0       0       0   \n",
       "2018-03-05 23:00:00       0       0       0       0       0       0       0   \n",
       "...                     ...     ...     ...     ...     ...     ...     ...   \n",
       "2020-07-31 19:00:00       0       0       0       0       0       0       0   \n",
       "2020-07-31 20:00:00       0       0       0       0       0       0       0   \n",
       "2020-07-31 21:00:00       0       0       0       0       0       0       0   \n",
       "2020-07-31 22:00:00       0       0       0       0       0       0       0   \n",
       "2020-07-31 23:00:00       0       0       0       0       0       0       0   \n",
       "\n",
       "                     sw2020  sw2050  sw2100  sw3005  sw3010  sw3020  sw3050  \\\n",
       "date                                                                          \n",
       "2018-03-05 19:00:00       0       0       0       0       0       0       0   \n",
       "2018-03-05 20:00:00       0       0       0       0       0       0       0   \n",
       "2018-03-05 21:00:00       0       0       0       0       0       0       0   \n",
       "2018-03-05 22:00:00       0       0       0       0       0       0       0   \n",
       "2018-03-05 23:00:00       0       0       0       0       0       0       0   \n",
       "...                     ...     ...     ...     ...     ...     ...     ...   \n",
       "2020-07-31 19:00:00       0       0       0       0       0       0       0   \n",
       "2020-07-31 20:00:00       0       0       0       0       0       0       0   \n",
       "2020-07-31 21:00:00       0       0       0       0       0       0       0   \n",
       "2020-07-31 22:00:00       0       0       0       0       0       0       0   \n",
       "2020-07-31 23:00:00       0       0       0       0       0       0       0   \n",
       "\n",
       "                     sw3100  \n",
       "date                         \n",
       "2018-03-05 19:00:00       0  \n",
       "2018-03-05 20:00:00       0  \n",
       "2018-03-05 21:00:00       0  \n",
       "2018-03-05 22:00:00       0  \n",
       "2018-03-05 23:00:00       0  \n",
       "...                     ...  \n",
       "2020-07-31 19:00:00       0  \n",
       "2020-07-31 20:00:00       0  \n",
       "2020-07-31 21:00:00       0  \n",
       "2020-07-31 22:00:00       0  \n",
       "2020-07-31 23:00:00       0  \n",
       "\n",
       "[21084 rows x 15 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_targets_dict[\"03054\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21054, 11)\n",
      "(21084, 15)\n"
     ]
    }
   ],
   "source": [
    "df_mclass_targs = mclass_targets_dict[\"03054\"] \n",
    "df_bin_targs = binary_targets_dict[\"03054\"]\n",
    "print(df_mclass_targs.shape)\n",
    "print(df_bin_targs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_index = df_mclass_targs.index.intersection(df_bin_targs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21054, 11)\n",
      "(21054, 15)\n"
     ]
    }
   ],
   "source": [
    "sub_df_mclass_targs = df_mclass_targs.loc[common_index]\n",
    "sub_df_bin_targs = df_bin_targs.loc[common_index]\n",
    "\n",
    "print(sub_df_mclass_targs.shape)\n",
    "print(sub_df_bin_targs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_mclass_targs_ANOMS = sub_df_mclass_targs.drop(\"Normal\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "mclass_1D = sub_df_mclass_targs_ANOMS.any(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "bclass_1D = sub_df_bin_targs.any(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mclass_1D.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bclass_1D.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_to_scale.shape[1]==15:\n",
    "    df_to_scale[(df_to_scale == -99999)] = pd.NA\n",
    "    df_to_scale = df_to_scale.interpolate(method=\"time\", axis=0)\n",
    "    df_to_scale = df_to_scale.fillna(method=\"bfill\", axis=0)\n",
    "    scaler = StandardScaler(with_mean=True)\n",
    "    feats_scaled_array = scaler.fit_transform(df_to_scale)\n",
    "    feats_scaled_df = pd.DataFrame(data=feats_scaled_array, index=df_to_scale.index, columns=df_to_scale.columns)\n",
    "#         if np.isnan(feats_scaled_array).sum() == 0:\n",
    "    scaled_feats_dict[stat_id] = copy.deepcopy(feats_scaled_df)\n",
    "\n",
    "else:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-b3717e66dc75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstat_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0macclima_stations_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf_to_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstat_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"p_official\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"t_official\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdf_to_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_dict' is not defined"
     ]
    }
   ],
   "source": [
    "final_station_ids_list = [stat_id for stat_id in scaled_feats_dict.keys()]        \n",
    "\n",
    "features_dfs_list = [scaled_feats_dict[station_id] for station_id in final_station_ids_list]\n",
    "targets_dfs_list = [targets_dict[station_id] for station_id in final_station_ids_list]\n",
    "\n",
    "scaled_feats_df = pd.concat(features_dfs_list, axis=0)\n",
    "targs_df = pd.concat(targets_dfs_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-b3717e66dc75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstat_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0macclima_stations_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf_to_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstat_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"p_official\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"t_official\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdf_to_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_dict' is not defined"
     ]
    }
   ],
   "source": [
    "for stat_id in acclima_stations_list:\n",
    "    \n",
    "    df_to_scale = copy.deepcopy(features_dict[stat_id]).drop(columns=[\"p_official\", \"t_official\"])\n",
    "\n",
    "    if df_to_scale.shape[1]==15:\n",
    "        df_to_scale[(df_to_scale == -99999)] = pd.NA\n",
    "        df_to_scale = df_to_scale.interpolate(method=\"time\", axis=0)\n",
    "        df_to_scale = df_to_scale.fillna(method=\"bfill\", axis=0)\n",
    "        scaler = StandardScaler(with_mean=True)\n",
    "        feats_scaled_array = scaler.fit_transform(df_to_scale)\n",
    "        feats_scaled_df = pd.DataFrame(data=feats_scaled_array, index=df_to_scale.index, columns=df_to_scale.columns)\n",
    "#         if np.isnan(feats_scaled_array).sum() == 0:\n",
    "        scaled_feats_dict[stat_id] = copy.deepcopy(feats_scaled_df)\n",
    "    \n",
    "    else:\n",
    "        continue\n",
    "\n",
    "final_station_ids_list = [stat_id for stat_id in scaled_feats_dict.keys()]        \n",
    "\n",
    "features_dfs_list = [scaled_feats_dict[station_id] for station_id in final_station_ids_list]\n",
    "targets_dfs_list = [targets_dict[station_id] for station_id in final_station_ids_list]\n",
    "\n",
    "scaled_feats_df = pd.concat(features_dfs_list, axis=0)\n",
    "targs_df = pd.concat(targets_dfs_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WBANNO</th>\n",
       "      <th>UTC_START</th>\n",
       "      <th>NAME</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>ACCLIMA</th>\n",
       "      <th>RANGE_FLAG</th>\n",
       "      <th>DOOR_FLAG</th>\n",
       "      <th>FROZEN_FLAG</th>\n",
       "      <th>MANUAL_FLAG</th>\n",
       "      <th>TAGS</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740067</th>\n",
       "      <td>03054</td>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740068</th>\n",
       "      <td>03054</td>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>16.491</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740069</th>\n",
       "      <td>03054</td>\n",
       "      <td>2017-05-01 01:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740070</th>\n",
       "      <td>03054</td>\n",
       "      <td>2017-05-01 01:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>12.662</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740071</th>\n",
       "      <td>03054</td>\n",
       "      <td>2017-05-01 02:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       WBANNO            UTC_START        NAME   VALUE ACCLIMA RANGE_FLAG  \\\n",
       "740067  03054  2017-05-01 00:00:00  p_official       0                  0   \n",
       "740068  03054  2017-05-01 00:00:00  t_official  16.491                  0   \n",
       "740069  03054  2017-05-01 01:00:00  p_official       0                  0   \n",
       "740070  03054  2017-05-01 01:00:00  t_official  12.662                  0   \n",
       "740071  03054  2017-05-01 02:00:00  p_official       0                  0   \n",
       "\n",
       "       DOOR_FLAG FROZEN_FLAG MANUAL_FLAG TAGS   NaN   NaN   NaN  \n",
       "740067         0           0           0       None  None  None  \n",
       "740068         0           0           0       None  None  None  \n",
       "740069         0           0           0       None  None  None  \n",
       "740070         0           0           0       None  None  None  \n",
       "740071         0           0           0       None  None  None  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acclima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_trimmed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-56283fc1544d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_trimmed_with_multilabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_trimmed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_targs_binarized\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_trimmed_with_multilabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_trimmed' is not defined"
     ]
    }
   ],
   "source": [
    "df_trimmed_with_multilabels = pd.concat([df_trimmed, df_targs_binarized], axis=1)\n",
    "df_trimmed_with_multilabels.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label targets caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE @ 9:52 AM 2021-01-06.\n",
    "# I'M JUST THROWING THIS HERE UNTIL I KNOW WHAT TO DO WITH IT.\n",
    "\n",
    "# Recreate the trimmed dataframe from 20201214_new_data_set.ipynb. We'll need its\n",
    "# index in order to correctly subset the one-hot encoded multilabels dataframe correctly.\n",
    "column_names_list =\\\n",
    "    [\"station_id\", \"date\", \"name\", \"value\",]\n",
    "df_trimmed = df_expanded.iloc[1:,:4]\n",
    "df_trimmed.columns = column_names_list\n",
    "station_id_array = df_trimmed.station_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single station for prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the current station's rows.\n",
    "df_station = df_trimmed_with_multilabels[df_trimmed_with_multilabels.station_id == station_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "      <th>Normal</th>\n",
       "      <th>Diurnal Noise</th>\n",
       "      <th>Erratic</th>\n",
       "      <th>Failure</th>\n",
       "      <th>FrozenRecovery</th>\n",
       "      <th>NoPrcpResponse</th>\n",
       "      <th>Noise</th>\n",
       "      <th>Spike</th>\n",
       "      <th>Static</th>\n",
       "      <th>Too high</th>\n",
       "      <th>Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33416812</th>\n",
       "      <td>94995</td>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33416813</th>\n",
       "      <td>94995</td>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>5.558</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33416814</th>\n",
       "      <td>94995</td>\n",
       "      <td>2017-05-01 01:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33416815</th>\n",
       "      <td>94995</td>\n",
       "      <td>2017-05-01 01:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>5.337</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33416816</th>\n",
       "      <td>94995</td>\n",
       "      <td>2017-05-01 02:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33723535</th>\n",
       "      <td>94995</td>\n",
       "      <td>2020-07-31 23:00:00</td>\n",
       "      <td>sw3010</td>\n",
       "      <td>0.406</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33723536</th>\n",
       "      <td>94995</td>\n",
       "      <td>2020-07-31 23:00:00</td>\n",
       "      <td>sw3020</td>\n",
       "      <td>0.022</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33723537</th>\n",
       "      <td>94995</td>\n",
       "      <td>2020-07-31 23:00:00</td>\n",
       "      <td>sw3050</td>\n",
       "      <td>0.077</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33723538</th>\n",
       "      <td>94995</td>\n",
       "      <td>2020-07-31 23:00:00</td>\n",
       "      <td>sw3100</td>\n",
       "      <td>0.356</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33723539</th>\n",
       "      <td>94995</td>\n",
       "      <td>2020-07-31 23:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>25.916</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306728 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         station_id                 date        name   value  Normal  \\\n",
       "33416812      94995  2017-05-01 00:00:00  p_official       0       1   \n",
       "33416813      94995  2017-05-01 00:00:00  t_official   5.558       1   \n",
       "33416814      94995  2017-05-01 01:00:00  p_official       0       1   \n",
       "33416815      94995  2017-05-01 01:00:00  t_official   5.337       1   \n",
       "33416816      94995  2017-05-01 02:00:00  p_official       0       1   \n",
       "...             ...                  ...         ...     ...     ...   \n",
       "33723535      94995  2020-07-31 23:00:00      sw3010   0.406       1   \n",
       "33723536      94995  2020-07-31 23:00:00      sw3020   0.022       1   \n",
       "33723537      94995  2020-07-31 23:00:00      sw3050   0.077       1   \n",
       "33723538      94995  2020-07-31 23:00:00      sw3100   0.356       1   \n",
       "33723539      94995  2020-07-31 23:00:00  t_official  25.916       1   \n",
       "\n",
       "          Diurnal Noise  Erratic  Failure  FrozenRecovery  NoPrcpResponse  \\\n",
       "33416812              0        0        0               0               0   \n",
       "33416813              0        0        0               0               0   \n",
       "33416814              0        0        0               0               0   \n",
       "33416815              0        0        0               0               0   \n",
       "33416816              0        0        0               0               0   \n",
       "...                 ...      ...      ...             ...             ...   \n",
       "33723535              0        0        0               0               0   \n",
       "33723536              0        0        0               0               0   \n",
       "33723537              0        0        0               0               0   \n",
       "33723538              0        0        0               0               0   \n",
       "33723539              0        0        0               0               0   \n",
       "\n",
       "          Noise  Spike  Static  Too high  Zero  \n",
       "33416812      0      0       0         0     0  \n",
       "33416813      0      0       0         0     0  \n",
       "33416814      0      0       0         0     0  \n",
       "33416815      0      0       0         0     0  \n",
       "33416816      0      0       0         0     0  \n",
       "...         ...    ...     ...       ...   ...  \n",
       "33723535      0      0       0         0     0  \n",
       "33723536      0      0       0         0     0  \n",
       "33723537      0      0       0         0     0  \n",
       "33723538      0      0       0         0     0  \n",
       "33723539      0      0       0         0     0  \n",
       "\n",
       "[306728 rows x 15 columns]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garrettgraham/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Convert all values in \"date\" column to datetime objects.\n",
    "df_station[\"date\"] = pd.to_datetime(df_station[\"date\"], format='%Y-%m-%d %H:%M:%S',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the station's data and target values into ML-compatible arrays. \n",
    "df_station_data =\\\n",
    "    df_station.pivot(index=\"date\", columns=\"name\", values=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "      <th>Normal</th>\n",
       "      <th>Diurnal Noise</th>\n",
       "      <th>Erratic</th>\n",
       "      <th>Failure</th>\n",
       "      <th>FrozenRecovery</th>\n",
       "      <th>NoPrcpResponse</th>\n",
       "      <th>Noise</th>\n",
       "      <th>Spike</th>\n",
       "      <th>Static</th>\n",
       "      <th>Too high</th>\n",
       "      <th>Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33416812</th>\n",
       "      <td>94995</td>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33416813</th>\n",
       "      <td>94995</td>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>5.558</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33416814</th>\n",
       "      <td>94995</td>\n",
       "      <td>2017-05-01 01:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33416815</th>\n",
       "      <td>94995</td>\n",
       "      <td>2017-05-01 01:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>5.337</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33416816</th>\n",
       "      <td>94995</td>\n",
       "      <td>2017-05-01 02:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33723535</th>\n",
       "      <td>94995</td>\n",
       "      <td>2020-07-31 23:00:00</td>\n",
       "      <td>sw3010</td>\n",
       "      <td>0.406</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33723536</th>\n",
       "      <td>94995</td>\n",
       "      <td>2020-07-31 23:00:00</td>\n",
       "      <td>sw3020</td>\n",
       "      <td>0.022</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33723537</th>\n",
       "      <td>94995</td>\n",
       "      <td>2020-07-31 23:00:00</td>\n",
       "      <td>sw3050</td>\n",
       "      <td>0.077</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33723538</th>\n",
       "      <td>94995</td>\n",
       "      <td>2020-07-31 23:00:00</td>\n",
       "      <td>sw3100</td>\n",
       "      <td>0.356</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33723539</th>\n",
       "      <td>94995</td>\n",
       "      <td>2020-07-31 23:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>25.916</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306728 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         station_id                date        name   value  Normal  \\\n",
       "33416812      94995 2017-05-01 00:00:00  p_official       0       1   \n",
       "33416813      94995 2017-05-01 00:00:00  t_official   5.558       1   \n",
       "33416814      94995 2017-05-01 01:00:00  p_official       0       1   \n",
       "33416815      94995 2017-05-01 01:00:00  t_official   5.337       1   \n",
       "33416816      94995 2017-05-01 02:00:00  p_official       0       1   \n",
       "...             ...                 ...         ...     ...     ...   \n",
       "33723535      94995 2020-07-31 23:00:00      sw3010   0.406       1   \n",
       "33723536      94995 2020-07-31 23:00:00      sw3020   0.022       1   \n",
       "33723537      94995 2020-07-31 23:00:00      sw3050   0.077       1   \n",
       "33723538      94995 2020-07-31 23:00:00      sw3100   0.356       1   \n",
       "33723539      94995 2020-07-31 23:00:00  t_official  25.916       1   \n",
       "\n",
       "          Diurnal Noise  Erratic  Failure  FrozenRecovery  NoPrcpResponse  \\\n",
       "33416812              0        0        0               0               0   \n",
       "33416813              0        0        0               0               0   \n",
       "33416814              0        0        0               0               0   \n",
       "33416815              0        0        0               0               0   \n",
       "33416816              0        0        0               0               0   \n",
       "...                 ...      ...      ...             ...             ...   \n",
       "33723535              0        0        0               0               0   \n",
       "33723536              0        0        0               0               0   \n",
       "33723537              0        0        0               0               0   \n",
       "33723538              0        0        0               0               0   \n",
       "33723539              0        0        0               0               0   \n",
       "\n",
       "          Noise  Spike  Static  Too high  Zero  \n",
       "33416812      0      0       0         0     0  \n",
       "33416813      0      0       0         0     0  \n",
       "33416814      0      0       0         0     0  \n",
       "33416815      0      0       0         0     0  \n",
       "33416816      0      0       0         0     0  \n",
       "...         ...    ...     ...       ...   ...  \n",
       "33723535      0      0       0         0     0  \n",
       "33723536      0      0       0         0     0  \n",
       "33723537      0      0       0         0     0  \n",
       "33723538      0      0       0         0     0  \n",
       "33723539      0      0       0         0     0  \n",
       "\n",
       "[306728 rows x 15 columns]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>p_official</th>\n",
       "      <th>sw1005</th>\n",
       "      <th>sw1010</th>\n",
       "      <th>sw1020</th>\n",
       "      <th>sw1050</th>\n",
       "      <th>sw1100</th>\n",
       "      <th>sw2005</th>\n",
       "      <th>sw2010</th>\n",
       "      <th>sw2020</th>\n",
       "      <th>sw2050</th>\n",
       "      <th>sw2100</th>\n",
       "      <th>sw3005</th>\n",
       "      <th>sw3010</th>\n",
       "      <th>sw3020</th>\n",
       "      <th>sw3050</th>\n",
       "      <th>sw3100</th>\n",
       "      <th>t_official</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-01 00:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 01:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 02:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 03:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 04:00:00</th>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 19:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.355</td>\n",
       "      <td>26.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 20:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.356</td>\n",
       "      <td>26.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 21:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.355</td>\n",
       "      <td>27.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 22:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.356</td>\n",
       "      <td>26.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 23:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.356</td>\n",
       "      <td>25.916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28256 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "name                p_official sw1005 sw1010 sw1020 sw1050 sw1100 sw2005  \\\n",
       "date                                                                       \n",
       "2017-05-01 00:00:00          0    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 01:00:00          0    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 02:00:00          0    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 03:00:00          0    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 04:00:00        0.5    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "...                        ...    ...    ...    ...    ...    ...    ...   \n",
       "2020-07-31 19:00:00          0  0.373  0.416  0.415      0  0.585  0.371   \n",
       "2020-07-31 20:00:00          0  0.378  0.414  0.416      0  0.618  0.369   \n",
       "2020-07-31 21:00:00          0  0.371  0.423  0.409      0    0.6   0.37   \n",
       "2020-07-31 22:00:00          0   0.37  0.419   0.42      0  0.628  0.373   \n",
       "2020-07-31 23:00:00          0  0.371  0.421  0.412      0  0.614  0.368   \n",
       "\n",
       "name                sw2010 sw2020 sw2050 sw2100 sw3005 sw3010 sw3020 sw3050  \\\n",
       "date                                                                          \n",
       "2017-05-01 00:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 01:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 02:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 03:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 04:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "...                    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2020-07-31 19:00:00      0  0.423      0      0  0.389  0.415  0.021  0.075   \n",
       "2020-07-31 20:00:00      0  0.425      0      0   0.39  0.412  0.021  0.076   \n",
       "2020-07-31 21:00:00      0  0.426      0      0  0.377  0.409  0.021  0.077   \n",
       "2020-07-31 22:00:00      0  0.442      0      0  0.374  0.413  0.021  0.077   \n",
       "2020-07-31 23:00:00      0  0.433      0      0  0.374  0.406  0.022  0.077   \n",
       "\n",
       "name                sw3100 t_official  \n",
       "date                                   \n",
       "2017-05-01 00:00:00    NaN      5.558  \n",
       "2017-05-01 01:00:00    NaN      5.337  \n",
       "2017-05-01 02:00:00    NaN      5.193  \n",
       "2017-05-01 03:00:00    NaN      4.913  \n",
       "2017-05-01 04:00:00    NaN      4.669  \n",
       "...                    ...        ...  \n",
       "2020-07-31 19:00:00  0.355     26.625  \n",
       "2020-07-31 20:00:00  0.356     26.912  \n",
       "2020-07-31 21:00:00  0.355     27.068  \n",
       "2020-07-31 22:00:00  0.356     26.745  \n",
       "2020-07-31 23:00:00  0.356     25.916  \n",
       "\n",
       "[28256 rows x 17 columns]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_station_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# TO DO!\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features and targets CSVs into features and targets dictionaries, indexed\n",
    "# by station ID number.  Again, this is ACCLIMA only.\n",
    "\n",
    "# First, make a template dictionary to store both the features and the targets by station ID #.\n",
    "template_dict = {\n",
    "    test_station_id : None\n",
    "    for test_station_id in acclima_stations_list\n",
    "}\n",
    "features_dict, targets_dict = copy.deepcopy(template_dict), copy.deepcopy(template_dict)\n",
    "\n",
    "# Delete the template dictionary so that it's not just lying around.\n",
    "del(template_dict)\n",
    "\n",
    "# Iterate through the ACCLIMA station IDs list.\n",
    "for fname in station_filenames_list:\n",
    "    \n",
    "    # Isolate just the ID number from the station name string.\n",
    "    station_id = fname.split(\"_\")[1].split(\".\")[0]\n",
    "    \n",
    "    # Skip this station if it's not an ACCLIMA station.\n",
    "    if station_id not in acclima_stations_list:\n",
    "        continue\n",
    "    \n",
    "    # If it's a data or targets file, read it into a pandas.DataFrame (DF) as a CSV, modify the \n",
    "    # DF index to be a datetime index, and then store that DF in either the features or the targets dictionary.\n",
    "    elif \"data\" in fname:\n",
    "        temp_feats_df = pd.read_csv(\n",
    "            data_dir+fname,\n",
    "            index_col=\"date\"\n",
    "        )\n",
    "        temp_feats_df.index = pd.to_datetime(temp_feats_df.index, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        features_dict[station_id] = temp_feats_df\n",
    "    elif \"targets\" in fname:\n",
    "        temp_targs_df = pd.read_csv(\n",
    "            data_dir+fname,\n",
    "            index_col=\"date\"\n",
    "        )\n",
    "        temp_targs_df.index = pd.to_datetime(temp_targs_df.index, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        targets_dict[station_id] = temp_targs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Port Aransas has mislabeled dates in August 2019. I'm going to exclude the mislabeled data\n",
    "# for that period.\n",
    "\n",
    "# Got the following station ID info from this URL:\n",
    "# https://mesonet.agron.iastate.edu/sites/site.php?station=23906&network=USCRN\n",
    "port_aransas_stat_id = \"23906\"\n",
    "feats_df_port_aransas = features_dict[port_aransas_stat_id]\n",
    "targs_df_port_aransas = targets_dict[port_aransas_stat_id]\n",
    "\n",
    "# Got the following date-exclusion idea from here:\n",
    "# https://stackoverflow.com/questions/55680603/pandas-filter-on-datetimeindex-by-excluding-date-range\n",
    "exclusion_dates = pd.date_range(start=\"2019-08-01\", end=\"2019-09-01\")\n",
    "\n",
    "features_dict[port_aransas_stat_id] = feats_df_port_aransas.loc[~feats_df_port_aransas.index.isin(exclusion_dates)]\n",
    "\n",
    "targets_dict[port_aransas_stat_id] = targs_df_port_aransas.loc[~targs_df_port_aransas.index.isin(exclusion_dates)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_feats_dict = {\n",
    "    stat_id : None\n",
    "    for stat_id in acclima_stations_list\n",
    "    if features_dict[stat_id].shape[1]==15\n",
    "}\n",
    "for stat_id in acclima_stations_list:\n",
    "    \n",
    "    df_to_scale = copy.deepcopy(features_dict[stat_id]).drop(columns=[\"p_official\", \"t_official\"])\n",
    "    \n",
    "    if df_to_scale.shape[1]==15:\n",
    "        df_to_scale[(df_to_scale == -99999)] = pd.NA\n",
    "        df_to_scale = df_to_scale.interpolate(method=\"time\", axis=0)\n",
    "        df_to_scale = df_to_scale.fillna(method=\"bfill\", axis=0)\n",
    "        scaler = StandardScaler(with_mean=True)\n",
    "        feats_scaled_array = scaler.fit_transform(df_to_scale)\n",
    "#         if np.isnan(feats_scaled_array).sum() == 0:\n",
    "        scaled_feats_dict[stat_id] = copy.deepcopy(feats_scaled_array)\n",
    "    \n",
    "    else:\n",
    "        continue\n",
    "\n",
    "t = [stat_tuple[1] for stat_tuple in scaled_feats_dict.items()]\n",
    "scaled_feats_array = np.concatenate(t, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the class labels DF for the station.\n",
    "df_station_targets =\\\n",
    "    df_station.pivot(\n",
    "        index=\"date\", columns=\"name\", values=\"manual_flag\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a precipitation and temperature DF.\n",
    "df_station_pt_data = df_station_data[[\"p_official\", \"t_official\"]]\n",
    "\n",
    "# Drop irrelevant columns from the DFs.\n",
    "df_station_data = df_station_data.drop(\n",
    "        columns=[\"p_official\", \"t_official\"]\n",
    "    )\n",
    "df_station_targets = df_station_targets.drop(\n",
    "        columns=[\"p_official\", \"t_official\"]\n",
    "    )\n",
    "\n",
    "# Get rid of rows of missing data and then drop those rows from the targets dataframe.\n",
    "df_station_data = df_station_data.dropna(how=\"all\")\n",
    "df_station_pt_data = df_station_pt_data.loc[df_station_data.index]\n",
    "df_station_targets = df_station_targets.loc[df_station_data.index]\n",
    "\n",
    "# # Add the precip and temp data back onto the station data.\n",
    "# df_station_data[\"p_official\"] = df_station_pt_data[\"p_official\"]\n",
    "# df_station_data[\"t_official\"] = df_station_pt_data[\"t_official\"]\n",
    "\n",
    "# # # Save the station's data and labels as a CSV\n",
    "# df_station_data.to_csv(\"../data/stations/data_\"+str(station_id)+\".csv\")\n",
    "# df_station_targets.to_csv(\"../data/stations/targets_\"+str(station_id)+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = ProgressBar()\n",
    "\n",
    "for station_id in pbar(station_id_array):\n",
    "    \n",
    "    # Isolate the current station's rows.\n",
    "    df_station = df_trimmed[df_trimmed.station_id == station_id]\n",
    "\n",
    "    # Convert all values in \"date\" column to datetime objects.\n",
    "    df_station[\"date\"] = pd.to_datetime(df_station[\"date\"], format='%Y-%m-%d %H:%M:%S',)\n",
    "\n",
    "    # Pivot the station's data and target values into ML-compatible arrays. \n",
    "    df_station_data =\\\n",
    "        df_station.pivot(index=\"date\", columns=\"name\", values=\"value\")\n",
    "\n",
    "    # Create the class labels DF for the station.\n",
    "    df_station_targets =\\\n",
    "        df_station.pivot(\n",
    "            index=\"date\", columns=\"name\", values=\"manual_flag\"\n",
    "        )\n",
    "\n",
    "    # Create a precipitation and temperature DF.\n",
    "    df_station_pt_data = df_station_data[[\"p_official\", \"t_official\"]]\n",
    "\n",
    "    # Drop irrelevant columns from the DFs.\n",
    "    df_station_data = df_station_data.drop(\n",
    "            columns=[\"p_official\", \"t_official\"]\n",
    "        )\n",
    "    df_station_targets = df_station_targets.drop(\n",
    "            columns=[\"p_official\", \"t_official\"]\n",
    "        )\n",
    "\n",
    "    # Get rid of rows of missing data and then drop those rows from the targets dataframe.\n",
    "    df_station_data = df_station_data.dropna(how=\"all\")\n",
    "    df_station_pt_data = df_station_pt_data.loc[df_station_data.index]\n",
    "    df_station_targets = df_station_targets.loc[df_station_data.index]\n",
    "\n",
    "    # Add the precip and temp data back onto the station data.\n",
    "    df_station_data[\"p_official\"] = df_station_pt_data[\"p_official\"]\n",
    "    df_station_data[\"t_official\"] = df_station_pt_data[\"t_official\"]\n",
    "\n",
    "    # # Save the station's data and labels as a CSV\n",
    "    df_station_data.to_csv(\"../data/stations/data_\"+str(station_id)+\".csv\")\n",
    "    df_station_targets.to_csv(\"../data/stations/targets_\"+str(station_id)+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate anomaly data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stat_id in acclima_stations_list:\n",
    "    \n",
    "    df_to_scale = copy.deepcopy(features_dict[stat_id]).drop(columns=[\"p_official\", \"t_official\"])\n",
    "    \n",
    "    if df_to_scale.shape[1]==15:\n",
    "        df_to_scale[(df_to_scale == -99999)] = pd.NA\n",
    "        df_to_scale = df_to_scale.interpolate(method=\"time\", axis=0)\n",
    "        df_to_scale = df_to_scale.fillna(method=\"bfill\", axis=0)\n",
    "        scaler = StandardScaler(with_mean=True)\n",
    "        feats_scaled_array = scaler.fit_transform(df_to_scale)\n",
    "        feats_scaled_df = pd.DataFrame(data=feats_scaled_array, index=df_to_scale.index, columns=df_to_scale.columns)\n",
    "#         if np.isnan(feats_scaled_array).sum() == 0:\n",
    "        scaled_feats_dict[stat_id] = copy.deepcopy(feats_scaled_df)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "final_station_ids_list = [stat_id for stat_id in scaled_feats_dict.keys()]        \n",
    "\n",
    "features_dfs_list = [scaled_feats_dict[station_id] for station_id in final_station_ids_list]\n",
    "targets_dfs_list = [targets_dict[station_id] for station_id in final_station_ids_list]\n",
    "\n",
    "scaled_feats_df = pd.concat(features_dfs_list, axis=0)\n",
    "targs_df = pd.concat(targets_dfs_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_scaledfeats_targs_df = pd.concat([scaled_feats_df,targs_df], axis=1)\n",
    "\n",
    "# This takes a COUPLE MINUTES TO RUN. So, only re-run if necessary!!\n",
    "# # combined_scaledfeats_targs_df.to_csv(\"../data/combined_scaledfeats_targs_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_scaledfeats_targs_NONAN_df = combined_scaledfeats_targs_df.dropna(how=\"any\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-isolate the targets and then combine them into a 1-D pd.series. \n",
    "# Whenever a row has at least one anomaly, then we will collapse that row's \n",
    "# values to a single value of 1. Rows with no anomalies shall be labeled as 0.\n",
    "ml_targs_df = combined_scaledfeats_targs_NONAN_df.iloc[:, -15:]\n",
    "ml_1D_targs_series = ml_targs_df.sum(axis=1)\n",
    "ml_1D_targs_series[ml_1D_targs_series > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-isolate the ML-ready features.\n",
    "ml_feats_df = combined_scaledfeats_targs_NONAN_df.iloc[:, :15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the percentage of the ML-ready data that are anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.62% of the data are anomalies.\n"
     ]
    }
   ],
   "source": [
    "percentage_anomalies =\\\n",
    "    np.round(100*ml_1D_targs_series.sum()/ml_1D_targs_series.shape[0], 2)\n",
    "print(f\"{percentage_anomalies}% of the data are anomalies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt some ML! Yewwwwww!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "X = ml_feats_df.values\n",
    "y = ml_1D_targs_series.values\n",
    "\n",
    "# Initialize a random forest (RF) classifier object with the best hyperparameter \n",
    "# sets from the grid search.\n",
    "rf_classifier = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Initialize a stratified 5-fold cross-validation object.  Make certain that it\n",
    "# randomly shuffles the data. AND, since it's randomly shuffling the data, make\n",
    "# certain that its random state is set to 7 (an arbitrary choice: it simply\n",
    "# needs to have a constant random state seed).\n",
    "stratified_kfold_generator = StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=7\n",
    ")\n",
    "\n",
    "# Iterate through each of the splits and train/test both classifiers. We will \n",
    "# store the predictions in the following lists, \"classfication predictions_rf\" \n",
    "# and \"classfication predictions_xgb\".\n",
    "classification_predictions_rf = list()\n",
    "for idx, (train, test) in enumerate(stratified_kfold_generator.split(X, y)):\n",
    "    \n",
    "    print(idx)\n",
    "    \n",
    "    # Fit the classifiers to the training data. This erases the training from any\n",
    "    # previous iteration of the loop.\n",
    "    rf_classifier.fit(X[train], y[train].ravel())\n",
    "\n",
    "    # Have the newly trained classifiers predict the classes of the withheld testing data.\n",
    "    y_predicted_rf = rf_classifier.predict(X[test])\n",
    "\n",
    "    # Add the true class labels and the predicted class labels to the storage lists.\n",
    "    classification_predictions_rf.append((y[test].ravel(), y_predicted_rf.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garrettgraham/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_normalized_conf_matrix(classification_predictions):\n",
    "    cf = np.zeros((2,2))\n",
    "    for y_true, y_pred in classification_predictions:\n",
    "        cf += confusion_matrix(y_true, y_pred)\n",
    "    return cf/cf.sum()\n",
    "\n",
    "class_labels_list = [\"Normal\", \"Anomalous\"]\n",
    "\n",
    "# Loop through the cached predictions and calculate the confusion matrices for\n",
    "# both classifiers.\n",
    "cf_rf = calculate_normalized_conf_matrix(classification_predictions_rf)\n",
    "\n",
    "# Initialize figure and axes objects in order to plot the two confusion matrices.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "\n",
    "# Plot the random forest confusion matrix.\n",
    "seaborn.heatmap(\n",
    "    cf_rf, cmap=\"Reds\", annot=cf_rf, square=True, cbar=True,\n",
    "    xticklabels=class_labels_list, yticklabels=class_labels_list,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Add labels to the x-axis and the y-axis.\n",
    "ax.set_xlabel(\"Predicted\", fontsize=16)\n",
    "ax.set_ylabel(\"True\", fontsize=16)\n",
    "\n",
    "# Add a title to the figure.\n",
    "fig.suptitle(\"Confusion matrix for lightly optimized RF classifer\\nof ACCLIMA sensor stations\")\n",
    "\n",
    "plt.savefig(\"randomforest_confusion_matrix.png\", dpi=300)\n",
    "\n",
    "# Display the figure.\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8823122 , 0.00147708],\n",
       "       [0.00401193, 0.11219878]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_rf\n",
    "\n",
    "# # Previous cf_rf w/out Port Aransas filtered.\n",
    "# array([[0.88232525, 0.00146891],\n",
    "#        [0.00401041, 0.11219543]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "99.45%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\")\n",
    "print(str(np.round(100*(cf_rf[0,0]+cf_rf[1,1]), 2))+\"%\")\n",
    "\n",
    "# # Previous accuracy w/out Port Aransas filtered.\n",
    "# Accuracy:\n",
    "# 99.45%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.964242658050827"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-(cf_rf[1,0]/cf_rf[1,1])\n",
    "\n",
    "# # Previous percentage of correctly detected anomalies w/out Port Aransas filtered.\n",
    "# 0.9642551447274832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

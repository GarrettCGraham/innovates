{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names_list = [\"station_id\", 'date', 'name', 'value', 'range_flag', 'door_flag', 'frozen_flag', 'manual_flag']\n",
    "\n",
    "# # Ingest raw data for single soil station.\n",
    "# df = pd.read_csv(\n",
    "#     data_dir+\"acclima_soil_water_rleeper_FULL.csv\",\n",
    "#     header=\"infer\",\n",
    "#     index_col=False\n",
    "# )\n",
    "\n",
    "# df = df.drop(columns=[\"TAGS\"])\n",
    "# df.columns = column_names_list\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_station_ids = [\n",
    "    int(stat_id)\n",
    "    if stat_id!=\"UN\" else stat_id\n",
    "    for stat_id in df.station_id.values\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.station_id = new_station_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of all unique station IDs.\n",
    "station_id_array = df.station_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garrettgraham/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Isolate the current station's rows.\n",
    "df_station = df[df.station_id == station_id_array[0]]\n",
    "\n",
    "# Convert all values in \"date\" column to datetime objects.\n",
    "df_station[\"date\"] = pd.to_datetime(df_station[\"date\"], format='%Y-%m-%d %H:%M:%S',)\n",
    "\n",
    "# Pivot the station's data and target values into ML-compatible arrays. \n",
    "df_station_data =\\\n",
    "    df_station.pivot(index=\"date\", columns=\"name\", values=\"value\")\n",
    "\n",
    "# Create the class labels DF for the station.\n",
    "df_station_targets =\\\n",
    "    df_station.pivot(\n",
    "        index=\"date\", columns=\"name\", values=\"manual_flag\"\n",
    "    )\n",
    "\n",
    "# Create a precipitation and temperature DF.\n",
    "df_station_pt_data = df_station_data[[\"p_official\", \"t_official\"]]\n",
    "\n",
    "# Drop irrelevant columns from the DFs.\n",
    "df_station_data = df_station_data.drop(\n",
    "        columns=[\"p_official\", \"t_official\"]\n",
    "    )\n",
    "df_station_targets = df_station_targets.drop(\n",
    "        columns=[\"p_official\", \"t_official\"]\n",
    "    )\n",
    "\n",
    "# Get rid of rows of missing data and then drop those rows from the targets dataframe.\n",
    "df_station_data = df_station_data.dropna(how=\"all\")\n",
    "df_station_pt_data = df_station_pt_data.loc[df_station_data.index]\n",
    "df_station_targets = df_station_targets.loc[df_station_data.index]\n",
    "\n",
    "# Add the precip and temp data back onto the station data.\n",
    "df_station_data[\"p_official\"] = df_station_pt_data[\"p_official\"]\n",
    "df_station_data[\"t_official\"] = df_station_pt_data[\"t_official\"]\n",
    "\n",
    "# # Save the station's data and labels as a CSV\n",
    "df_station_data.to_csv(\"../data/stations/data_\"+str(station_id_array[0])+\".csv\")\n",
    "df_station_targets.to_csv(\"../data/stations/targets_\"+str(station_id_array[0])+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station_data[\"p_official\"] = df_station_pt_data[\"p_official\"]\n",
    "df_station_data[\"t_official\"] = df_station_pt_data[\"t_official\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset and cache all station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garrettgraham/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "pbar = ProgressBar()\n",
    "\n",
    "for station_id in pbar(station_id_array):\n",
    "    \n",
    "    # Isolate the current station's rows.\n",
    "    df_station = df[df.station_id == station_id]\n",
    "\n",
    "    # Convert all values in \"date\" column to datetime objects.\n",
    "    df_station[\"date\"] = pd.to_datetime(df_station[\"date\"], format='%Y-%m-%d %H:%M:%S',)\n",
    "\n",
    "    # Pivot the station's data and target values into ML-compatible arrays. \n",
    "    df_station_data =\\\n",
    "        df_station.pivot(index=\"date\", columns=\"name\", values=\"value\")\n",
    "\n",
    "    # Create the class labels DF for the station.\n",
    "    df_station_targets =\\\n",
    "        df_station.pivot(\n",
    "            index=\"date\", columns=\"name\", values=\"manual_flag\"\n",
    "        )\n",
    "\n",
    "    # Create a precipitation and temperature DF.\n",
    "    df_station_pt_data = df_station_data[[\"p_official\", \"t_official\"]]\n",
    "\n",
    "    # Drop irrelevant columns from the DFs.\n",
    "    df_station_data = df_station_data.drop(\n",
    "            columns=[\"p_official\", \"t_official\"]\n",
    "        )\n",
    "    df_station_targets = df_station_targets.drop(\n",
    "            columns=[\"p_official\", \"t_official\"]\n",
    "        )\n",
    "\n",
    "    # Get rid of rows of missing data and then drop those rows from the targets dataframe.\n",
    "    df_station_data = df_station_data.dropna(how=\"all\")\n",
    "    df_station_pt_data = df_station_pt_data.loc[df_station_data.index]\n",
    "    df_station_targets = df_station_targets.loc[df_station_data.index]\n",
    "\n",
    "    # Add the precip and temp data back onto the station data.\n",
    "    df_station_data[\"p_official\"] = df_station_pt_data[\"p_official\"]\n",
    "    df_station_data[\"t_official\"] = df_station_pt_data[\"t_official\"]\n",
    "\n",
    "    # # Save the station's data and labels as a CSV\n",
    "    df_station_data.to_csv(\"../data/stations/data_\"+str(station_id)+\".csv\")\n",
    "    df_station_targets.to_csv(\"../data/stations/targets_\"+str(station_id)+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that data and targets were properly cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3728\n",
      "3758\n",
      "4126\n",
      "4141\n",
      "4236\n",
      "4990\n",
      "4994\n",
      "13301\n",
      "23906\n",
      "26563\n",
      "53152\n",
      "53155\n",
      "53877\n",
      "53968\n",
      "54808\n",
      "54811\n",
      "54854\n",
      "63826\n",
      "63828\n",
      "63829\n",
      "63831\n",
      "63850\n",
      "63855\n",
      "63858\n",
      "63869\n",
      "64756\n",
      "92827\n",
      "93243\n",
      "94077\n",
      "94995\n",
      "UN\n"
     ]
    }
   ],
   "source": [
    "# Check whether anomalies were actually recorded for at least some stations.\n",
    "data_dir = \"../data/stations/\"\n",
    "for test_station_id in station_id_array:\n",
    "    test_data = pd.read_csv(data_dir+f\"data_{test_station_id}.csv\")\n",
    "    test_targets = pd.read_csv(data_dir+f\"targets_{test_station_id}.csv\")\n",
    "\n",
    "    tt = test_targets.drop(columns=[\"date\"])\n",
    "    anom_sum = (tt > 0).sum().sum()\n",
    "    if anom_sum > 0:\n",
    "        print(test_station_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's all looking pretty reasonable! Time to do some data exploration on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass SANITY CHECK\n",
    "\n",
    "This notebook is for me to figure out where the heck I'm going wrong with breaking down the multiclass stuff labeling.\n",
    "\n",
    "The problem was discovered when I tried to do a 1-to-1 comparison of the binary labels to the multiclass labels. If a multiclass label was present, it ought to have been matched by a '1' in the binary label set. This was not the case.\n",
    "\n",
    "I circled back and parsed apart the binary label treatment and am fairly confident that I didn't make an error there.\n",
    "\n",
    "Thus, the error must reside with how I preprocessed the multiclass labels. \n",
    "\n",
    "Hopefully, I can identify the source of the errors in this notebook. \n",
    "\n",
    "This notebook was derived from _20201229_multiclass.ipynb_.\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# FINISHED!\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from progressbar import ProgressBar\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of stations ID #s.\n",
    "data_dir = \"../data/stations/\"\n",
    "station_filenames_list = [\n",
    "    filename for filename in os.listdir(path=data_dir)\n",
    "    if filename!=\".DS_Store\"\n",
    "]\n",
    "\n",
    "# Load just the list of ACCLIMA station IDs.\n",
    "file_path = \"../data/acclima_stations_id_list.txt\"\n",
    "acclima_stations_list = pd.read_csv(file_path, header=None).iloc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load expanded array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34343059, 13)\n",
      "(34343059, 13)\n",
      "(34343058, 13)\n"
     ]
    }
   ],
   "source": [
    "# This operation takes a couple minutes, so only do it if you really need to reload the stuff.\n",
    "df_expanded = pd.read_pickle(\"../data/acclima_soil_water_rleeper_1214.pickle\")\n",
    "print(df_expanded.shape)\n",
    "\n",
    "# The previous line loads column names as values in the first row. Set them\n",
    "# as the actual column names and then delete the first row.\n",
    "df_expanded.columns = df_expanded.iloc[0].values\n",
    "print(df_expanded.shape)\n",
    "\n",
    "df_expanded = df_expanded.iloc[1:, :]\n",
    "print(df_expanded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks out thus far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset the expanded 2020-12-14 dataset to ACCLIMA only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes about 1 minute to run, so only run when necessary.\n",
    "# Filter the expanded dataset down to just the ACCLIMA stations so that\n",
    "# it's easier to wield in memory.\n",
    "df_acclima = df_expanded.isin({\"WBANNO\":acclima_stations_list})\n",
    "df_acclima = df_expanded.iloc[df_acclima.WBANNO.values]\n",
    "\n",
    "# Delete df_expanded to free up some dang memory.\n",
    "del(df_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the ACCLIMA DF's TAG columns to not be \"TAGS, NaN, NaN, NaN\".\n",
    "df_acclima.columns =\\\n",
    "    df_acclima.columns[:9].tolist() + [\"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14641744, 13)\n",
      "19701315\n"
     ]
    }
   ],
   "source": [
    "print(df_acclima.shape)\n",
    "\n",
    "print(34343059 - 14641744)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, checks out. Successfully subsetted to ACCLIMA sensor stations only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WBANNO</th>\n",
       "      <th>UTC_START</th>\n",
       "      <th>NAME</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>ACCLIMA</th>\n",
       "      <th>RANGE_FLAG</th>\n",
       "      <th>DOOR_FLAG</th>\n",
       "      <th>FROZEN_FLAG</th>\n",
       "      <th>MANUAL_FLAG</th>\n",
       "      <th>TAGS_00</th>\n",
       "      <th>TAGS_01</th>\n",
       "      <th>TAGS_02</th>\n",
       "      <th>TAGS_03</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740067</th>\n",
       "      <td>03054</td>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740068</th>\n",
       "      <td>03054</td>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>16.491</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740069</th>\n",
       "      <td>03054</td>\n",
       "      <td>2017-05-01 01:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740070</th>\n",
       "      <td>03054</td>\n",
       "      <td>2017-05-01 01:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>12.662</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740071</th>\n",
       "      <td>03054</td>\n",
       "      <td>2017-05-01 02:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       WBANNO            UTC_START        NAME   VALUE ACCLIMA RANGE_FLAG  \\\n",
       "740067  03054  2017-05-01 00:00:00  p_official       0                  0   \n",
       "740068  03054  2017-05-01 00:00:00  t_official  16.491                  0   \n",
       "740069  03054  2017-05-01 01:00:00  p_official       0                  0   \n",
       "740070  03054  2017-05-01 01:00:00  t_official  12.662                  0   \n",
       "740071  03054  2017-05-01 02:00:00  p_official       0                  0   \n",
       "\n",
       "       DOOR_FLAG FROZEN_FLAG MANUAL_FLAG TAGS_00 TAGS_01 TAGS_02 TAGS_03  \n",
       "740067         0           0           0            None    None    None  \n",
       "740068         0           0           0            None    None    None  \n",
       "740069         0           0           0            None    None    None  \n",
       "740070         0           0           0            None    None    None  \n",
       "740071         0           0           0            None    None    None  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acclima.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset down to a single station and create a data-prep regime that works here for reducing **from** multilabel **to** multiclass\n",
    "\n",
    "First, create a few constants and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS_SET = {\n",
    "    'Acclima-Zero', 'Acclima-Toohigh', 'Acclima-Too high', 'Acclima-NoPrcpResponse', \n",
    "    'Acclima-FrozenRecovery', 'Acclima-Noise', 'Acclima-Failure',\n",
    "    'Acclima-Spike', 'Acclima-DiurnalNoise', 'Acclima-Erratic', \n",
    "    'Acclima-Static'\n",
    "}\n",
    "\n",
    "def clean_tags_dataframe(df_targets):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes a target data frame and replaces the tags with their cleaned-up, space-less versions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy of the dataframe so we don't overwrite the original.\n",
    "    df_targets_cleaned = copy.deepcopy(df_targets)\n",
    "    \n",
    "    # Loop through all the cleaned versions of the tags and replace the original versions,\n",
    "    # which have extra whitespace pre-pended to them, with the cleaned versions.\n",
    "    for tag in TAGS_SET:\n",
    "        df_targets_cleaned.replace(\n",
    "            to_replace=\" \"+tag,\n",
    "            value=tag,\n",
    "            inplace=True,\n",
    "        )\n",
    "    \n",
    "    # Replace \"None\" tags with an empty string.\n",
    "    df_targets_cleaned.replace(\n",
    "        to_replace=[None],\n",
    "        value=[\"\"],\n",
    "        inplace=True,\n",
    "    )\n",
    "    \n",
    "    return df_targets_cleaned\n",
    "    \n",
    "    \n",
    "def rename_tags_in_df(df_targets):\n",
    "    \"\"\"\n",
    "    Replaces 'Acclima-Spike' with 'spike' and noise-related Acclima tags with 'noise'.\n",
    "    Returns a dataframe with renamed tags.\n",
    "    \"\"\"\n",
    "    df_targets_renamed = copy.deepcopy(df_targets)\n",
    "    \n",
    "    # Rename SPIKES.\n",
    "    df_targets_renamed.replace(\n",
    "        to_replace=\"Acclima-Spike\",\n",
    "        value=\"spike\",\n",
    "        inplace=True,\n",
    "    )\n",
    "    # Rename NOISE.\n",
    "    noise_tag_list = [\n",
    "        \"Acclima-Noise\",\n",
    "        \"Acclima-Diurnal Noise\", \n",
    "        \"Acclima-FrozenRecovery\", \n",
    "        \"Acclima-Erratic\",\n",
    "    ]\n",
    "    for noise_tag in noise_tag_list:\n",
    "        df_targets_renamed.replace(\n",
    "            to_replace=noise_tag,\n",
    "            value=\"noise\",\n",
    "            inplace=True,\n",
    "        )\n",
    "    return df_targets_renamed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare targets and then features for single station\n",
    "\n",
    "Need to select station that has multiple types of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(457391,)\n",
      "['' 'Acclima-Too high' 'Acclima-Zero' 'noise' 'spike']\n",
      "Series([], Name: TAGS_01, dtype: object)\n",
      "13435593    Acclima-Too high\n",
      "13435610    Acclima-Too high\n",
      "13435627    Acclima-Too high\n",
      "13435644    Acclima-Too high\n",
      "13435661    Acclima-Too high\n",
      "                  ...       \n",
      "13454155    Acclima-Too high\n",
      "13454172    Acclima-Too high\n",
      "13454189    Acclima-Too high\n",
      "13454206    Acclima-Too high\n",
      "13454223    Acclima-Too high\n",
      "Name: TAGS_01, Length: 257, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Select station to treat. I selected 14 b/c it's the first one in the station series with >=5 tags.\n",
    "station_list_idx = 14\n",
    "station_id_num = acclima_stations_list[station_list_idx]\n",
    "\n",
    "# Subset down to the single ACCLIMA station of interest.\n",
    "df_station = df_acclima[df_acclima.WBANNO.eq(station_id_num)]\n",
    "\n",
    "# Subset down to just the columns of interest.\n",
    "df_station =\\\n",
    "    df_station[\n",
    "        [\"UTC_START\", \"NAME\", \"VALUE\", \"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\"]\n",
    "    ]\n",
    "\n",
    "# Convert all datetimes to actual datetime datatypes.\n",
    "df_station.UTC_START = pd.to_datetime(df_station.UTC_START, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Get just the station's targets.\n",
    "df_station_targets = df_station[[\"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\"]]\n",
    "\n",
    "# Clean up and then rename the targets.\n",
    "df_station_targets =\\\n",
    "        rename_tags_in_df(\n",
    "            clean_tags_dataframe(\n",
    "                df_station[[\"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\"]]\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(df_station_targets.TAGS_00.shape)\n",
    "\n",
    "print(np.unique(df_station_targets.values))\n",
    "\n",
    "print(df_station_targets.TAGS_01[df_station_targets.TAGS_01.isna()])\n",
    "\n",
    "print(df_station_targets.TAGS_01[df_station_targets.TAGS_01 != \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop all tags except for '', 'spikes', and 'noise'.\n",
    "# array_is_normal = df_station_targets.eq('').values\n",
    "\n",
    "# vec_is_normal = np.logical_or(np.logical_or(np.logical_or(array_is_normal[:,0], array_is_normal[:,1]), array_is_normal[:,2]), array_is_normal[:, 3])\n",
    "\n",
    "# print(vec_is_normal.sum())\n",
    "\n",
    "# print(vec_is_normal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding approach won't work b/c nearly every row will have a 'normal' tag associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For each row in the targets, loop through and drop rows with more than one tag.\n",
    "I'm taking this approach b/c there's plenty of data and the rows that have multiple tags\n",
    "represent odd cases that clearly are not **only** an example of a spike or noise.\"\"\"\n",
    "\n",
    "# play = df_station_targets.iloc[:, 1:]\n",
    "\n",
    "# playmore = play[play.TAGS_01!=''].values\n",
    "# # playmore = play[play.TAGS_01==''].values\n",
    "\n",
    "# # \"\".join(playmore[0])\n",
    "\n",
    "# ## THE FOLLOWING LINE DOESN'T WORK AS INTENDED. Need to use np.array([\"\".join(row) for row in play]) instead!\n",
    "# arr_tags_concatenated = np.apply_along_axis(\"\".join, 1, playmore)\n",
    "# # arr_tags_concatenated\n",
    "\n",
    "# arr_concattags_lengths = np.array([l for l in map(len, arr_tags_concatenated)])\n",
    "# # arr_concattags_lengths\n",
    "\n",
    "# arr_tags_to_keep = ~(arr_concattags_lengths > 3)\n",
    "# # ~(arr_concattags_lengths > 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying a different approach now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' ' Acclima-Too high']\n"
     ]
    }
   ],
   "source": [
    "# play = df_station_targets.iloc[:, 1:].values\n",
    "play = np.char.array([ [\"\",\"\",\"\"], [\"\",\" Acclima-Too high\", \"\"], [\"\",\"\",\"\"]])\n",
    "print(np.unique(play))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' ' Acclima-Too high']\n"
     ]
    }
   ],
   "source": [
    "arr_tags_concatenated = np.array([\"\".join(row) for row in play])\n",
    "\n",
    "print(np.unique(arr_tags_concatenated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 17  0]\n"
     ]
    }
   ],
   "source": [
    "arr_concattags_lengths = np.array([l for l in map(len, arr_tags_concatenated)])\n",
    "print(arr_concattags_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False  True]\n"
     ]
    }
   ],
   "source": [
    "arr_tags_to_keep = ~(arr_concattags_lengths > 3)\n",
    "print(arr_tags_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['' '' '']\n",
      " ['' '' '']]\n"
     ]
    }
   ],
   "source": [
    "play_updated = play[arr_tags_to_keep]\n",
    "print(play_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stuff above works great! I'm going to generate that array for the full targets list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' 'Acclima-Too high']\n"
     ]
    }
   ],
   "source": [
    "play = df_station_targets.iloc[:, 1:].values\n",
    "print(np.unique(play))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' 'Acclima-Too high']\n"
     ]
    }
   ],
   "source": [
    "arr_tags_concatenated = np.array([\"\".join(row) for row in play])\n",
    "print(np.unique(arr_tags_concatenated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "4112\n"
     ]
    }
   ],
   "source": [
    "arr_concattags_lengths = np.array([l for l in map(len, arr_tags_concatenated)])\n",
    "print(arr_concattags_lengths)\n",
    "print(arr_concattags_lengths.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ...  True  True  True]\n",
      "257\n"
     ]
    }
   ],
   "source": [
    "arr_tags_to_keep = ~(arr_concattags_lengths > 3)\n",
    "print(arr_tags_to_keep)\n",
    "print((~arr_tags_to_keep).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457134\n",
      "457391\n",
      "457391\n"
     ]
    }
   ],
   "source": [
    "df_tags_reduced = df_station_targets[arr_tags_to_keep].iloc[:,0]\n",
    "print(df_tags_reduced.shape[0])\n",
    "print(df_station_targets.shape[0])\n",
    "print((~arr_tags_to_keep).sum() + df_tags_reduced.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' 'spike' 'Acclima-Zero' 'noise']\n"
     ]
    }
   ],
   "source": [
    "print(df_tags_reduced.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. It's working. It's quick. It's time to serialize this for use this to subset the targets down to just single-label rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(457134,)\n",
      "(454740,)\n"
     ]
    }
   ],
   "source": [
    "df_station_targets_reduced_final = df_tags_reduced[df_tags_reduced.isin([\"\", \"spike\", \"noise\"])]\n",
    "\n",
    "print(df_tags_reduced.shape)\n",
    "print(df_station_targets_reduced_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to reduce the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(457391, 3)\n",
      "(454740, 3)\n"
     ]
    }
   ],
   "source": [
    "# All station data w/out tag filtering.\n",
    "print(df_station.iloc[:, :3].shape)\n",
    "\n",
    "# Station data w/tag filtering.\n",
    "print(df_station.loc[df_station_targets_reduced_final.index, [\"UTC_START\", \"NAME\", \"VALUE\"]].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions for the tag-filtered feature set check out!\n",
    "\n",
    "Now I'll filter and double check that all the data matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(454740, 3)\n",
      "(454740,)\n",
      "['' 'spike' 'noise']\n"
     ]
    }
   ],
   "source": [
    "df_station_features = df_station.loc[df_station_targets_reduced_final.index, [\"UTC_START\", \"NAME\", \"VALUE\"]]\n",
    "\n",
    "print(df_station_features.shape)\n",
    "\n",
    "print(df_station_targets_reduced_final.shape)\n",
    "\n",
    "print(df_station_targets_reduced_final.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like it's working.  I'm going functionalize these routines that I've written, test the functions, and then iterate over all the stations, caching as I go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining unique labels:                                  ['' 'spike' 'Acclima-Zero' 'noise']\n",
      "Number of reduced targets:                                457134\n",
      "Original number of targets:                               457391\n",
      "Number of reduced targets plus number of dropped targets: 457391\n",
      "\n",
      "Final set of unique labels: ['' 'spike' 'noise']\n",
      "Number of final labels:     454740\n",
      "Number of final features:   454740\n"
     ]
    }
   ],
   "source": [
    "# Isolate the last three columns of targets. \n",
    "# If there's a row where they're non-empty, then that's indicative of a multilabel example. \n",
    "# Our goal here is to eliminate all multilabel examples.\n",
    "array_multilabel_targets = df_station_targets.iloc[:, 1:].values\n",
    "\n",
    "# Iterate through the rows of multilabel targets and concatenate all tags into a single string.\n",
    "# For rows that don't have any multilabel tag, the resulting entry will be an empty string of length 0.\n",
    "# For multilabel rows, there will be a string with non-zero length.\n",
    "arr_tags_concatenated = np.array([\"\".join(row) for row in array_multilabel_targets])\n",
    "\n",
    "# Iterate through the concatenated tags and calculate their lengths. \n",
    "# These lengths will be stored in the new array defined below.\n",
    "arr_concattags_lengths = np.array([l for l in map(len, arr_tags_concatenated)])\n",
    "\n",
    "# Find all zero-length elements of the array. \n",
    "# These entries are the rows in the original targets dataframe that we want to keep,\n",
    "# since they are the single-label (ie, non-multilabel) rows.\n",
    "arr_tags_to_keep = (arr_concattags_lengths == 0)\n",
    "\n",
    "# Reduce the targets dataframe to the first column.\n",
    "# This column represents all of the single-label targets.\n",
    "df_tags_reduced = df_station_targets[arr_tags_to_keep].iloc[:,0]\n",
    "\n",
    "# Print some sanity statistics.\n",
    "print(\"Remaining unique labels:                                 \", df_tags_reduced.unique())\n",
    "print(\"Number of reduced targets:                               \", df_tags_reduced.shape[0])\n",
    "print(\"Original number of targets:                              \", df_station_targets.shape[0])\n",
    "print(\"Number of reduced targets plus number of dropped targets:\", (~arr_tags_to_keep).sum() + df_tags_reduced.shape[0])\n",
    "\n",
    "# Get the final set of targets by filtering out anything that's not a spike, noise, or normal.\n",
    "df_station_targets_reduced_final = df_tags_reduced[df_tags_reduced.isin([\"\", \"spike\", \"noise\"])]\n",
    "\n",
    "# Get the final set of station features by using the indices of the remaining targets.\n",
    "df_station_features = df_station.loc[df_station_targets_reduced_final.index, [\"UTC_START\", \"NAME\", \"VALUE\"]]\n",
    "\n",
    "print()\n",
    "print(\"Final set of unique labels:\", df_station_targets_reduced_final.unique())\n",
    "print(\"Number of final labels:    \", df_station_features.shape[0])\n",
    "print(\"Number of final features:  \",df_station_targets_reduced_final.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this for a few other stations before moving onto actually functionalizing and implementing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################ 20 ############################\n",
      "Original set of targets:                                          ['']\n",
      "Original number of targets:                                       248753\n",
      "\n",
      "Remaining unique labels:                                          ['']\n",
      "Original number of targets:                                       248753\n",
      "Number of reduced targets:                                        248753\n",
      "Number of reduced targets plus number of dropped targets:         248753\n",
      "\n",
      "Final set of unique labels:                                       ['']\n",
      "Number of final labels:                                           248753\n",
      "Number of final features:                                         248753\n",
      "\n",
      "\n",
      "############################ 21 ############################\n",
      "Original set of targets:                                          ['' 'Acclima-Zero' 'noise']\n",
      "Original number of targets:                                       249528\n",
      "\n",
      "Remaining unique labels:                                          ['' 'Acclima-Zero']\n",
      "Original number of targets:                                       249528\n",
      "Number of reduced targets:                                        242281\n",
      "Number of reduced targets plus number of dropped targets:         249528\n",
      "\n",
      "Final set of unique labels:                                       ['']\n",
      "Number of final labels:                                           241509\n",
      "Number of final features:                                         241509\n",
      "\n",
      "\n",
      "############################ 22 ############################\n",
      "Original set of targets:                                          ['']\n",
      "Original number of targets:                                       151159\n",
      "\n",
      "Remaining unique labels:                                          ['']\n",
      "Original number of targets:                                       151159\n",
      "Number of reduced targets:                                        151159\n",
      "Number of reduced targets plus number of dropped targets:         151159\n",
      "\n",
      "Final set of unique labels:                                       ['']\n",
      "Number of final labels:                                           151159\n",
      "Number of final features:                                         151159\n",
      "\n",
      "\n",
      "############################ 23 ############################\n",
      "Original set of targets:                                          ['' 'Acclima-Failure' 'Acclima-Static' 'Acclima-Zero' 'noise' 'spike']\n",
      "Original number of targets:                                       454718\n",
      "\n",
      "Remaining unique labels:                                          ['' 'noise' 'spike']\n",
      "Original number of targets:                                       454718\n",
      "Number of reduced targets:                                        443837\n",
      "Number of reduced targets plus number of dropped targets:         454718\n",
      "\n",
      "Final set of unique labels:                                       ['' 'noise' 'spike']\n",
      "Number of final labels:                                           443837\n",
      "Number of final features:                                         443837\n",
      "\n",
      "\n",
      "############################ 24 ############################\n",
      "Original set of targets:                                          ['' 'Acclima-NoPrcpResponse' 'Acclima-Zero' 'noise' 'spike']\n",
      "Original number of targets:                                       345151\n",
      "\n",
      "Remaining unique labels:                                          ['' 'spike' 'Acclima-NoPrcpResponse' 'Acclima-Zero' 'noise']\n",
      "Original number of targets:                                       345151\n",
      "Number of reduced targets:                                        345151\n",
      "Number of reduced targets plus number of dropped targets:         345151\n",
      "\n",
      "Final set of unique labels:                                       ['' 'spike' 'noise']\n",
      "Number of final labels:                                           343999\n",
      "Number of final features:                                         343999\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select station to test.\n",
    "# station_list_idx = 14\n",
    "for station_list_idx in range(20, 25):\n",
    "    print(\"############################\", station_list_idx, \"############################\")\n",
    "\n",
    "    station_id_num = acclima_stations_list[station_list_idx]\n",
    "\n",
    "    # Subset down to the single ACCLIMA station of interest.\n",
    "    df_station = df_acclima[df_acclima.WBANNO.eq(station_id_num)]\n",
    "\n",
    "    # Subset down to just the columns of interest.\n",
    "    df_station =\\\n",
    "        df_station[\n",
    "            [\"UTC_START\", \"NAME\", \"VALUE\", \"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\"]\n",
    "        ]\n",
    "\n",
    "    # Convert all datetimes to actual datetime datatypes.\n",
    "    df_station.UTC_START = pd.to_datetime(df_station.UTC_START, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Get just the station's targets.\n",
    "    df_station_targets = df_station[[\"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\"]]\n",
    "\n",
    "    # Clean up and then rename the targets.\n",
    "    df_station_targets =\\\n",
    "            rename_tags_in_df(\n",
    "                clean_tags_dataframe(\n",
    "                    df_station[[\"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\"]]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Print sanity-check statistics.\n",
    "    print(\"Original set of targets:                                         \", np.unique(df_station_targets.values))\n",
    "    print(\"Original number of targets:                                      \", df_station_targets.shape[0])\n",
    "\n",
    "    ################################################################\n",
    "    ################################################################\n",
    "    # FILTER OUT MULTILABEL INSTANCES AND THEN RUN SANITY CHECKS.\n",
    "    ################################################################\n",
    "    ################################################################\n",
    "\n",
    "    # Isolate the last three columns of targets. \n",
    "    # If there's a row where they're non-empty, then that's indicative of a multilabel example. \n",
    "    # Our goal here is to eliminate all multilabel examples.\n",
    "    array_multilabel_targets = df_station_targets.iloc[:, 1:].values\n",
    "\n",
    "    # Iterate through the rows of multilabel targets and concatenate all tags into a single string.\n",
    "    # For rows that don't have any multilabel tag, the resulting entry will be an empty string of length 0.\n",
    "    # For multilabel rows, there will be a string with non-zero length.\n",
    "    arr_tags_concatenated = np.array([\"\".join(row) for row in array_multilabel_targets])\n",
    "\n",
    "    # Iterate through the concatenated tags and calculate their lengths. \n",
    "    # These lengths will be stored in the new array defined below.\n",
    "    arr_concattags_lengths = np.array([l for l in map(len, arr_tags_concatenated)])\n",
    "\n",
    "    # Find all zero-length elements of the array. \n",
    "    # These entries are the rows in the original targets dataframe that we want to keep,\n",
    "    # since they are the single-label (ie, non-multilabel) rows.\n",
    "    arr_tags_to_keep = (arr_concattags_lengths == 0)\n",
    "\n",
    "    # Reduce the targets dataframe to the first column.\n",
    "    # This column represents all of the single-label targets.\n",
    "    df_tags_reduced = df_station_targets[arr_tags_to_keep].iloc[:,0]\n",
    "\n",
    "    # Print some sanity statistics.\n",
    "    print()\n",
    "    print(\"Remaining unique labels:                                         \", df_tags_reduced.unique())\n",
    "    print(\"Original number of targets:                                      \", df_station_targets.shape[0])\n",
    "    print(\"Number of reduced targets:                                       \", df_tags_reduced.shape[0])\n",
    "    print(\"Number of reduced targets plus number of dropped targets:        \", (~arr_tags_to_keep).sum() + df_tags_reduced.shape[0])\n",
    "\n",
    "    # Get the final set of targets by filtering out anything that's not a spike, noise, or normal.\n",
    "    df_station_targets_reduced_final = df_tags_reduced[df_tags_reduced.isin([\"\", \"spike\", \"noise\"])]\n",
    "\n",
    "    # Get the final set of station features by using the indices of the remaining targets.\n",
    "    df_station_features = df_station.loc[df_station_targets_reduced_final.index, [\"UTC_START\", \"NAME\", \"VALUE\"]]\n",
    "\n",
    "    print()\n",
    "    print(\"Final set of unique labels:                                      \", df_station_targets_reduced_final.unique())\n",
    "    print(\"Number of final labels:                                          \", df_station_features.shape[0])\n",
    "    print(\"Number of final features:                                        \",df_station_targets_reduced_final.shape[0])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks like this is working well! I reviewed the sanity-check print-outs. I'm going to functionalize these methods now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_dataframe(station_id_num, df_acclima):\n",
    "    # Subset down to the single ACCLIMA station of interest.\n",
    "    df_station = df_acclima[df_acclima.WBANNO.eq(station_id_num)]\n",
    "    return df_station\n",
    "\n",
    "\n",
    "def reduce_station_df_and_convert_dates(df_station):\n",
    "    # Subset down to just the columns of interest.\n",
    "    df_station =\\\n",
    "        df_station[\n",
    "            [\"UTC_START\", \"NAME\", \"VALUE\", \"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\"]\n",
    "        ]\n",
    "    # Convert all datetimes to actual datetime datatypes.\n",
    "    df_station.UTC_START = pd.to_datetime(df_station.UTC_START, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    return df_station\n",
    "\n",
    "\n",
    "def get_station_targets(df_station):\n",
    "\n",
    "    # Get just the station's targets.\n",
    "    df_station_targets = df_station[[\"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\"]]\n",
    "\n",
    "    # Clean up and then rename the targets.\n",
    "    df_station_targets =\\\n",
    "            rename_tags_in_df(\n",
    "                clean_tags_dataframe(\n",
    "                    df_station[[\"TAGS_00\", \"TAGS_01\", \"TAGS_02\", \"TAGS_03\"]]\n",
    "                )\n",
    "            )\n",
    "    return df_station_targets\n",
    "\n",
    "\n",
    "def get_filtered_targets(df_station_targets):\n",
    "    # Isolate the last three columns of targets. \n",
    "    # If there's a row where they're non-empty, then that's indicative of a multilabel example. \n",
    "    # Our goal here is to eliminate all multilabel examples.\n",
    "    array_multilabel_targets = df_station_targets.iloc[:, 1:].values\n",
    "\n",
    "    # Iterate through the rows of multilabel targets and concatenate all tags into a single string.\n",
    "    # For rows that don't have any multilabel tag, the resulting entry will be an empty string of length 0.\n",
    "    # For multilabel rows, there will be a string with non-zero length.\n",
    "    arr_tags_concatenated = np.array([\"\".join(row) for row in array_multilabel_targets])\n",
    "\n",
    "    # Iterate through the concatenated tags and calculate their lengths. \n",
    "    # These lengths will be stored in the new array defined below.\n",
    "    arr_concattags_lengths = np.array([l for l in map(len, arr_tags_concatenated)])\n",
    "\n",
    "    # Find all zero-length elements of the array. \n",
    "    # These entries are the rows in the original targets dataframe that we want to keep,\n",
    "    # since they are the single-label (ie, non-multilabel) rows.\n",
    "    arr_tags_to_keep = (arr_concattags_lengths == 0)\n",
    "\n",
    "    # Reduce the targets dataframe to the first column.\n",
    "    # This column represents all of the single-label targets.\n",
    "    df_tags_reduced = df_station_targets[arr_tags_to_keep].iloc[:,0]\n",
    "\n",
    "    # Get the final set of targets by filtering out anything that's not a spike, noise, or normal.\n",
    "    df_station_targets_reduced_final = df_tags_reduced[df_tags_reduced.isin([\"\", \"spike\", \"noise\"])]\n",
    "    \n",
    "    return df_station_targets_reduced_final, df_tags_reduced\n",
    "    \n",
    "    \n",
    "def get_filtered_features(df_station, df_station_targets_reduced_final):\n",
    "    # Get the final set of station features by using the indices of the remaining targets.\n",
    "    df_station_features = df_station.loc[df_station_targets_reduced_final.index, [\"UTC_START\", \"NAME\", \"VALUE\"]]\n",
    "    return df_station_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test these functionalized methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################ 15 ############################\n",
      "Original set of targets:                                          ['' 'spike']\n",
      "Original number of targets:                                       367029\n",
      "\n",
      "Remaining unique labels:                                          ['' 'spike']\n",
      "Original number of targets:                                       367029\n",
      "Number of reduced targets:                                        367029\n",
      "Number of reduced targets plus number of dropped targets:         383523\n",
      "\n",
      "Final set of unique labels:                                       ['' 'spike']\n",
      "Number of final labels:                                           367029\n",
      "Number of final features:                                         367029\n",
      "\n",
      "\n",
      "############################ 16 ############################\n",
      "Original set of targets:                                          ['' 'Acclima-NoPrcpResponse' 'Acclima-Zero' 'spike']\n",
      "Original number of targets:                                       368442\n",
      "\n",
      "Remaining unique labels:                                          ['' 'Acclima-Zero' 'spike']\n",
      "Original number of targets:                                       368442\n",
      "Number of reduced targets:                                        368237\n",
      "Number of reduced targets plus number of dropped targets:         384731\n",
      "\n",
      "Final set of unique labels:                                       ['' 'spike']\n",
      "Number of final labels:                                           368215\n",
      "Number of final features:                                         368215\n",
      "\n",
      "\n",
      "############################ 17 ############################\n",
      "Original set of targets:                                          ['']\n",
      "Original number of targets:                                       181088\n",
      "\n",
      "Remaining unique labels:                                          ['']\n",
      "Original number of targets:                                       181088\n",
      "Number of reduced targets:                                        181088\n",
      "Number of reduced targets plus number of dropped targets:         197582\n",
      "\n",
      "Final set of unique labels:                                       ['']\n",
      "Number of final labels:                                           181088\n",
      "Number of final features:                                         181088\n",
      "\n",
      "\n",
      "############################ 18 ############################\n",
      "Original set of targets:                                          ['']\n",
      "Original number of targets:                                       199617\n",
      "\n",
      "Remaining unique labels:                                          ['']\n",
      "Original number of targets:                                       199617\n",
      "Number of reduced targets:                                        199617\n",
      "Number of reduced targets plus number of dropped targets:         216111\n",
      "\n",
      "Final set of unique labels:                                       ['']\n",
      "Number of final labels:                                           199617\n",
      "Number of final features:                                         199617\n",
      "\n",
      "\n",
      "############################ 19 ############################\n",
      "Original set of targets:                                          ['' 'Acclima-Failure' 'Acclima-Zero']\n",
      "Original number of targets:                                       241670\n",
      "\n",
      "Remaining unique labels:                                          ['']\n",
      "Original number of targets:                                       241670\n",
      "Number of reduced targets:                                        225176\n",
      "Number of reduced targets plus number of dropped targets:         241670\n",
      "\n",
      "Final set of unique labels:                                       ['']\n",
      "Number of final labels:                                           225176\n",
      "Number of final features:                                         225176\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select station to test.\n",
    "# station_list_idx = 14\n",
    "\n",
    "for station_list_idx in range(15, 20):\n",
    "    \n",
    "    station_id_num = acclima_stations_list[station_list_idx]\n",
    "    print(\"############################\", station_list_idx, \"############################\")\n",
    "\n",
    "    # Get just the station of interest.\n",
    "    df_station = get_station_dataframe(station_id_num, df_acclima)\n",
    "\n",
    "    # Cut the station data down to just the columns-of-interest.\n",
    "    # Convert the date-times in the UTC_START column to datetime objects.\n",
    "    df_station = reduce_station_df_and_convert_dates(df_station)\n",
    "\n",
    "    # Isolate the station targets for filtering.\n",
    "    df_station_targets = get_station_targets(df_station)\n",
    "\n",
    "    # Print sanity-check statistics.\n",
    "    print(\"Original set of targets:                                         \", np.unique(df_station_targets.values))\n",
    "    print(\"Original number of targets:                                      \", df_station_targets.shape[0])\n",
    "    \n",
    "    # Filter the targets down to single-label targets-of-interest (ie, just normal, \"spike\" and \"noise\").\n",
    "    df_station_targets_reduced_final, df_tags_reduced = get_filtered_targets(df_station_targets)\n",
    "\n",
    "    # Print some sanity statistics.\n",
    "    print()\n",
    "    print(\"Remaining unique labels:                                         \", df_tags_reduced.unique())\n",
    "    print(\"Original number of targets:                                      \", df_station_targets.shape[0])\n",
    "    print(\"Number of reduced targets:                                       \", df_tags_reduced.shape[0])\n",
    "    print(\"Number of reduced targets plus number of dropped targets:        \", (~arr_tags_to_keep).sum() + df_tags_reduced.shape[0])\n",
    "\n",
    "    # Get the final feature-set by filtering to feature-rows that have labels remaining after the labels were filtered.\n",
    "    df_station_features = get_filtered_features(df_station, df_station_targets_reduced_final)\n",
    "\n",
    "    print()\n",
    "    print(\"Final set of unique labels:                                      \", df_station_targets_reduced_final.unique())\n",
    "    print(\"Number of final labels:                                          \", df_station_features.shape[0])\n",
    "    print(\"Number of final features:                                        \",df_station_targets_reduced_final.shape[0])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functionalizing the methods produces the same results as when they were non-functionalized. Whoooo!\n",
    "\n",
    "Moving onto creating pivot tables of features now, with appropriate filtering of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select station to test.\n",
    "station_list_idx = 24\n",
    "station_id_num = acclima_stations_list[station_list_idx]\n",
    "\n",
    "# Get just the station of interest.\n",
    "df_station = get_station_dataframe(station_id_num, df_acclima)\n",
    "\n",
    "# Cut the station data down to just the columns-of-interest.\n",
    "# Convert the date-times in the UTC_START column to datetime objects.\n",
    "df_station = reduce_station_df_and_convert_dates(df_station)\n",
    "\n",
    "# Isolate the station targets for filtering.\n",
    "df_station_targets = get_station_targets(df_station)\n",
    "\n",
    "# Filter the targets down to single-label targets-of-interest (ie, just normal, \"spike\" and \"noise\").\n",
    "df_station_targets_reduced_final, df_tags_reduced = get_filtered_targets(df_station_targets)\n",
    "\n",
    "# Get the final feature-set by filtering to feature-rows that have labels remaining after the labels were filtered.\n",
    "df_station_features = get_filtered_features(df_station, df_station_targets_reduced_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UTC_START</th>\n",
       "      <th>NAME</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>TAGS_00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20526527</th>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>0.7</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20526528</th>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>6.55</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20526529</th>\n",
       "      <td>2017-05-01 01:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>3.6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20526530</th>\n",
       "      <td>2017-05-01 01:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>6.502</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20526531</th>\n",
       "      <td>2017-05-01 02:00:00</td>\n",
       "      <td>p_official</td>\n",
       "      <td>5.6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20871673</th>\n",
       "      <td>2020-07-31 23:00:00</td>\n",
       "      <td>sw3010</td>\n",
       "      <td>0.3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20871674</th>\n",
       "      <td>2020-07-31 23:00:00</td>\n",
       "      <td>sw3020</td>\n",
       "      <td>0.319</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20871675</th>\n",
       "      <td>2020-07-31 23:00:00</td>\n",
       "      <td>sw3050</td>\n",
       "      <td>0.292</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20871676</th>\n",
       "      <td>2020-07-31 23:00:00</td>\n",
       "      <td>sw3100</td>\n",
       "      <td>0.35</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20871677</th>\n",
       "      <td>2020-07-31 23:00:00</td>\n",
       "      <td>t_official</td>\n",
       "      <td>23.175</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>343999 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   UTC_START        NAME   VALUE TAGS_00\n",
       "20526527 2017-05-01 00:00:00  p_official     0.7        \n",
       "20526528 2017-05-01 00:00:00  t_official    6.55        \n",
       "20526529 2017-05-01 01:00:00  p_official     3.6        \n",
       "20526530 2017-05-01 01:00:00  t_official   6.502        \n",
       "20526531 2017-05-01 02:00:00  p_official     5.6        \n",
       "...                      ...         ...     ...     ...\n",
       "20871673 2020-07-31 23:00:00      sw3010     0.3        \n",
       "20871674 2020-07-31 23:00:00      sw3020   0.319        \n",
       "20871675 2020-07-31 23:00:00      sw3050   0.292        \n",
       "20871676 2020-07-31 23:00:00      sw3100    0.35        \n",
       "20871677 2020-07-31 23:00:00  t_official  23.175        \n",
       "\n",
       "[343999 rows x 4 columns]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_station_combined = pd.concat([df_station_features, df_station_targets_reduced_final],axis=1)\n",
    "df_station_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', 'spike', 'noise'], dtype=object)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_station_combined.TAGS_00.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identified a new potential problem:\n",
    "##### Even though I've eliminated multilable instances for single sensors at single timepoints, there may be single timepoints where **multiple sensors** have differing tags. I'll need to drop those intances from this analysis in order to get to the pure single-label, multiclass case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28491\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>NAME</th>\n",
       "      <th>sw1005</th>\n",
       "      <th>sw1010</th>\n",
       "      <th>sw1020</th>\n",
       "      <th>sw1050</th>\n",
       "      <th>sw1100</th>\n",
       "      <th>sw2005</th>\n",
       "      <th>sw2010</th>\n",
       "      <th>sw2020</th>\n",
       "      <th>sw2050</th>\n",
       "      <th>sw2100</th>\n",
       "      <th>sw3005</th>\n",
       "      <th>sw3010</th>\n",
       "      <th>sw3020</th>\n",
       "      <th>sw3050</th>\n",
       "      <th>sw3100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTC_START</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-01 00:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 01:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 02:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 03:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 04:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 19:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>noise</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 20:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>noise</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 21:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>noise</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 22:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>noise</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31 23:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>noise</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28491 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "NAME                sw1005 sw1010 sw1020 sw1050 sw1100 sw2005 sw2010 sw2020  \\\n",
       "UTC_START                                                                     \n",
       "2017-05-01 00:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 01:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 02:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 03:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2017-05-01 04:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "...                    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2020-07-31 19:00:00                         NaN  noise                        \n",
       "2020-07-31 20:00:00                         NaN  noise                        \n",
       "2020-07-31 21:00:00                         NaN  noise                        \n",
       "2020-07-31 22:00:00                         NaN  noise                        \n",
       "2020-07-31 23:00:00                         NaN  noise                        \n",
       "\n",
       "NAME                sw2050 sw2100 sw3005 sw3010 sw3020 sw3050 sw3100  \n",
       "UTC_START                                                             \n",
       "2017-05-01 00:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "2017-05-01 01:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "2017-05-01 02:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "2017-05-01 03:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "2017-05-01 04:00:00    NaN    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "...                    ...    ...    ...    ...    ...    ...    ...  \n",
       "2020-07-31 19:00:00                                                   \n",
       "2020-07-31 20:00:00                                                   \n",
       "2020-07-31 21:00:00                                                   \n",
       "2020-07-31 22:00:00                                                   \n",
       "2020-07-31 23:00:00                                                   \n",
       "\n",
       "[28491 rows x 15 columns]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pivoted_targets = df_station_combined.pivot(index=\"UTC_START\", columns=\"NAME\", values=\"TAGS_00\")\n",
    "df_pivoted_targets = df_pivoted_targets.drop([\"p_official\", \"t_official\"], axis=1)\n",
    "\n",
    "print(df_pivoted_targets.shape[0])\n",
    "\n",
    "df_pivoted_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28491\n",
      "28491\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>NAME</th>\n",
       "      <th>sw1005</th>\n",
       "      <th>sw1010</th>\n",
       "      <th>sw1020</th>\n",
       "      <th>sw1050</th>\n",
       "      <th>sw1100</th>\n",
       "      <th>sw2005</th>\n",
       "      <th>sw2010</th>\n",
       "      <th>sw2020</th>\n",
       "      <th>sw2050</th>\n",
       "      <th>sw2100</th>\n",
       "      <th>sw3005</th>\n",
       "      <th>sw3010</th>\n",
       "      <th>sw3020</th>\n",
       "      <th>sw3050</th>\n",
       "      <th>sw3100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTC_START</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-01 00:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 01:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 02:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 03:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01 04:00:00</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "NAME                sw1005 sw1010 sw1020 sw1050 sw1100 sw2005 sw2010 sw2020  \\\n",
       "UTC_START                                                                     \n",
       "2017-05-01 00:00:00                                                           \n",
       "2017-05-01 01:00:00                                                           \n",
       "2017-05-01 02:00:00                                                           \n",
       "2017-05-01 03:00:00                                                           \n",
       "2017-05-01 04:00:00                                                           \n",
       "\n",
       "NAME                sw2050 sw2100 sw3005 sw3010 sw3020 sw3050 sw3100  \n",
       "UTC_START                                                             \n",
       "2017-05-01 00:00:00                                                   \n",
       "2017-05-01 01:00:00                                                   \n",
       "2017-05-01 02:00:00                                                   \n",
       "2017-05-01 03:00:00                                                   \n",
       "2017-05-01 04:00:00                                                   "
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Figure out how many rows have a missing value. It seems like there may be a lot. Ooof.\n",
    "# print(\n",
    "#     (df_pivoted_targets.isna().values.sum(axis=1) > 0).sum()\n",
    "# )\n",
    "\n",
    "# Clean the tags up; ie, convert all NaN to ''.\n",
    "df_pivoted_targets_clean = clean_tags_dataframe(df_pivoted_targets)\n",
    "\n",
    "print(df_pivoted_targets.shape[0])\n",
    "print(df_pivoted_targets_clean.shape[0])\n",
    "\n",
    "df_pivoted_targets_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' '' '' ... 'noise' 'noise' 'noise']\n",
      "['' 'noise' 'spikespikespike' 'spikespikespikespikespike']\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all the values in each row and check for non-unique labels.\n",
    "# Ie, check for time points where there's normal/spike, etc.\n",
    "arr_tags_concatenated = np.array([\"\".join(row) for row in df_pivoted_targets_clean.values])\n",
    "\n",
    "print(arr_tags_concatenated)\n",
    "print(np.unique(arr_tags_concatenated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['', '', '', ..., '', '', ''],\n",
       "       ['', '', '', ..., '', '', ''],\n",
       "       ['', '', '', ..., '', '', ''],\n",
       "       ...,\n",
       "       ['', '', '', ..., '', '', ''],\n",
       "       ['', '', '', ..., '', '', ''],\n",
       "       ['', '', '', ..., '', '', '']], dtype=object)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play = df_pivoted_targets_clean.values\n",
    "play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[''], [''], [''], [''], [''], [''], [''], [''], [''], ['']]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use set operations to the ID unique tags in each row. Then, \n",
    "# set(np.char.array([ [\"\",\"\",\"\"], [\"\",\" Acclima-Too high\", \"\"], [\"a\",\"b\",\"c\"]])[2])\n",
    "unique_tags_by_row = [list(set(row)) for row in play]\n",
    "unique_tags_by_row[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "28491\n"
     ]
    }
   ],
   "source": [
    "# Check where there's more than one unique tag per row.\n",
    "# These ought to be the timepoints with more than one unique label.\n",
    "# These should be dropped from the analysis.\n",
    "print(\n",
    "    np.array(\n",
    "        [len(row) > 1 for row in unique_tags_by_row]\n",
    "    ).sum()\n",
    ")\n",
    "print(\n",
    "    len(unique_tags_by_row)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WELLLLL, ACTUALLY, I'm not concerned with multilabel rows that have both normal and then one tag. \n",
    "\n",
    "#### I'm concerned with multilabel rows that have \"spike\" and \"noise\".\n",
    "\n",
    "#### So, I'll concatenate the unique labels together. Anything over length 5 ( len(\"spike\")=5 and len(\"noise\")=5 ) will be a multilabel instance, since len(\"spikenoise\")=10.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28491\n"
     ]
    }
   ],
   "source": [
    "# Find all the multilabel row locations via some string method trickery.\n",
    "multilabel_row_locations = np.array(\n",
    "    [\n",
    "        len(\n",
    "            \"\".join(row)    # Join all the unique tags in each row together; ie, \n",
    "                            # [\"\", \"noise\"] --> \"noise\", while\n",
    "                            # [\"noise\", \"spike\"] --> \"noisespike\"\n",
    "        ) > 5               # Check for anything that has length > 5. This will only occur where\n",
    "                            # \"\".join(row) --> \"spikenoise\" or \"noisespike\".\n",
    "        for row in unique_tags_by_row\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(multilabel_row_locations.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now I need to drop the multilabel timepoints from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the pivoted targets DF of any multilabel row locations (ie, locations that are co-labeled \"spike\" and \"noise\").\n",
    "df_pivoted_targets_singlelabel =\\\n",
    "    df_pivoted_targets[~multilabel_row_locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28491\n",
      "28490\n"
     ]
    }
   ],
   "source": [
    "df_pivoted_features_singlelabel = df_station_features.pivot(\n",
    "    index=\"UTC_START\", columns=\"NAME\", values=\"VALUE\"            # Create a pivoted DF of the features.\n",
    ")\n",
    "\n",
    "print(df_pivoted_features_singlelabel.shape[0])\n",
    "\n",
    "# Use the filtered targets to dataframe's index to filter the pivoted features dataframe.\n",
    "# That way, we have only feature locations with single-label multiclass features.\n",
    "df_pivoted_features_singlelabel = df_station_features.pivot(\n",
    "    index=\"UTC_START\", columns=\"NAME\", values=\"VALUE\"            # Create a pivoted DF of the features.\n",
    ").loc[\n",
    "    df_pivoted_targets_singlelabel.index                         # Filter the pivoted features DF using the datetimes of the remaining targets.\n",
    "]\n",
    "\n",
    "print(df_pivoted_features_singlelabel.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that filtering by the single-label DF's index reduces the pivoted feature DF's date range. \n",
    "\n",
    "So, I need to reverse-filter the single-label DF using the remaining indices from the feature DF.\n",
    "\n",
    "Ugh. There's so many flipping tricky parts of this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28490\n"
     ]
    }
   ],
   "source": [
    "# Filter the pivoted single-label targets DF using the remaining feature DF datetime indices.\n",
    "df_pivoted_targets_singlelabel =\\\n",
    "    df_pivoted_targets_singlelabel.loc[\n",
    "        df_pivoted_features_singlelabel.index\n",
    "    ]\n",
    "\n",
    "print(df_pivoted_targets_singlelabel.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's done the trick.\n",
    "\n",
    "Now both the pivoted targets and the pivoted features have the same datetime indices.\n",
    "\n",
    "Now I need to go back and reformat the remaining targets so that they're a single-column series, rather than a multi-dimensional dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, float found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-386-b217d49e5eff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Since each row only has either {\"\"}, {\"\", \"spike\"} or {\"\", \"noise\"}, the result will be a single label per row.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m multiclass_targets_array = np.char.array(\n\u001b[0;32m---> 11\u001b[0;31m     [\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# Join together the unique single-labels;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                          \u001b[0;31m# ie, [\"\",\"spike\"] --> \"spike\" and [\"\", \"noise\"] --> \"noise\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-386-b217d49e5eff>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m multiclass_targets_array = np.char.array(\n\u001b[1;32m     11\u001b[0m     [\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# Join together the unique single-labels;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                                          \u001b[0;31m# ie, [\"\",\"spike\"] --> \"spike\" and [\"\", \"noise\"] --> \"noise\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_tags_by_row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, float found"
     ]
    }
   ],
   "source": [
    "# Get just the remaining single-label target values.\n",
    "# I'll use these to get down to one label entry per datetime row.\n",
    "array_pivoted_targets_singlelabel = clean_tags_dataframe(df_pivoted_targets_singlelabel).values\n",
    "\n",
    "# Use the list(set()) trick to filter down to the unique entries in each row of the targets array.\n",
    "unique_tags_by_row = [list(set(row)) for row in array_pivoted_targets_singlelabel]\n",
    "\n",
    "# Join these unique entries together to form a single entry per target row.\n",
    "# Since each row only has either {\"\"}, {\"\", \"spike\"} or {\"\", \"noise\"}, the result will be a single label per row.\n",
    "multiclass_targets_array = np.char.array(\n",
    "    [\n",
    "        \"\".join(row)                     # Join together the unique single-labels;\n",
    "                                         # ie, [\"\",\"spike\"] --> \"spike\" and [\"\", \"noise\"] --> \"noise\".\n",
    "        for row in unique_tags_by_row\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Recombine the newly-filtered multiclass targets with their original datetime index.\n",
    "df_targets_singlelabel = pd.DataFrame(index=df_pivoted_targets_singlelabel.index, values=mutliclass_targets_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28491\n",
      "28491\n",
      "343999\n",
      "28490\n"
     ]
    }
   ],
   "source": [
    "# multiclass_targets_array = np.char.array(\n",
    "#     [\n",
    "#         \"\".join(row)                     # Join together the unique single-labels;\n",
    "#                                          # ie, [\"\",\"spike\"] --> \"spike\" and [\"\", \"noise\"] --> \"noise\".\n",
    "#         for row in unique_tags_by_row\n",
    "#     ]\n",
    "# )\n",
    "# print(len(unique_tags_by_row))\n",
    "# print(multiclass_targets_array.shape[0])\n",
    "# print(df_station_features.shape[0])\n",
    "# print(df_pivoted_features_singlelabel.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_targets_series = pd.Series(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>NAME</th>\n",
       "      <th>p_official</th>\n",
       "      <th>sw1005</th>\n",
       "      <th>sw1010</th>\n",
       "      <th>sw1020</th>\n",
       "      <th>sw1050</th>\n",
       "      <th>sw1100</th>\n",
       "      <th>sw2005</th>\n",
       "      <th>sw2010</th>\n",
       "      <th>sw2020</th>\n",
       "      <th>sw2050</th>\n",
       "      <th>sw2100</th>\n",
       "      <th>sw3005</th>\n",
       "      <th>sw3010</th>\n",
       "      <th>sw3020</th>\n",
       "      <th>sw3050</th>\n",
       "      <th>sw3100</th>\n",
       "      <th>t_official</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTC_START</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-22 21:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-22 22:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-22 23:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-23 00:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-23 01:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-15 13:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.343</td>\n",
       "      <td>17.344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-15 14:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.342</td>\n",
       "      <td>19.362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-15 15:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.341</td>\n",
       "      <td>20.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-15 16:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.342</td>\n",
       "      <td>22.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-15 17:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.342</td>\n",
       "      <td>23.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18079 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "NAME                p_official sw1005 sw1010 sw1020 sw1050 sw1100 sw2005  \\\n",
       "UTC_START                                                                  \n",
       "2018-05-22 21:00:00          0  0.371      0  0.394  0.142      0  0.426   \n",
       "2018-05-22 22:00:00          0   0.37      0  0.397  0.142      0  0.425   \n",
       "2018-05-22 23:00:00          0  0.369      0  0.396  0.142      0  0.424   \n",
       "2018-05-23 00:00:00          0  0.369      0  0.395  0.142      0  0.423   \n",
       "2018-05-23 01:00:00          0  0.367      0  0.399  0.142      0  0.423   \n",
       "...                        ...    ...    ...    ...    ...    ...    ...   \n",
       "2020-06-15 13:00:00          0  0.132  0.175   0.19  0.224  0.308  0.201   \n",
       "2020-06-15 14:00:00          0  0.132  0.175   0.19  0.225  0.307  0.202   \n",
       "2020-06-15 15:00:00          0  0.133  0.175   0.19  0.225  0.306  0.203   \n",
       "2020-06-15 16:00:00          0  0.133  0.175   0.19  0.225  0.307  0.205   \n",
       "2020-06-15 17:00:00          0  0.133  0.175   0.19  0.225  0.307  0.206   \n",
       "\n",
       "NAME                sw2010 sw2020 sw2050 sw2100 sw3005 sw3010 sw3020 sw3050  \\\n",
       "UTC_START                                                                     \n",
       "2018-05-22 21:00:00  0.458  0.407  0.424  0.121  0.452  0.491  0.468      0   \n",
       "2018-05-22 22:00:00   0.45  0.406  0.424  0.121   0.45  0.491  0.468      0   \n",
       "2018-05-22 23:00:00  0.454  0.407  0.425  0.121   0.45  0.488  0.469      0   \n",
       "2018-05-23 00:00:00  0.448  0.408  0.424  0.121  0.449  0.487   0.47      0   \n",
       "2018-05-23 01:00:00   0.45  0.407  0.424  0.121  0.446  0.487  0.469      0   \n",
       "...                    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2020-06-15 13:00:00  0.225  0.212   0.33  0.117  0.247  0.267  0.254  0.273   \n",
       "2020-06-15 14:00:00  0.225   0.21   0.33  0.117  0.248  0.267  0.254  0.273   \n",
       "2020-06-15 15:00:00  0.225  0.209   0.33  0.117  0.249  0.266  0.254  0.273   \n",
       "2020-06-15 16:00:00  0.225  0.209  0.329  0.117   0.25  0.267  0.254  0.273   \n",
       "2020-06-15 17:00:00  0.233  0.209  0.329  0.117  0.252  0.267  0.254  0.272   \n",
       "\n",
       "NAME                sw3100 t_official  \n",
       "UTC_START                              \n",
       "2018-05-22 21:00:00      0     19.849  \n",
       "2018-05-22 22:00:00      0     19.868  \n",
       "2018-05-22 23:00:00      0     19.833  \n",
       "2018-05-23 00:00:00      0     19.046  \n",
       "2018-05-23 01:00:00      0     17.553  \n",
       "...                    ...        ...  \n",
       "2020-06-15 13:00:00  0.343     17.344  \n",
       "2020-06-15 14:00:00  0.342     19.362  \n",
       "2020-06-15 15:00:00  0.341     20.833  \n",
       "2020-06-15 16:00:00  0.342     22.185  \n",
       "2020-06-15 17:00:00  0.342      23.16  \n",
       "\n",
       "[18079 rows x 17 columns]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now I need to drop any and all feature rows that have NaN values.\n",
    "df_pivoted_features_singlelabel = df_pivoted_features_singlelabel.dropna(how=\"any\", axis=0)\n",
    "\n",
    "df_pivoted_features_singlelabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18079\n",
      "28491\n"
     ]
    }
   ],
   "source": [
    "print(df_pivoted_features_singlelabel.shape[0])\n",
    "print(multiclass_targets_array.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
